{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8187b80-aa12-4c9b-97b2-83f83691d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.processors.aggregate import Aggregator\n",
    "from cyclops.processors.cleaning import (\n",
    "    normalize_categories,\n",
    "    normalize_names,\n",
    "    normalize_values,\n",
    ")\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    AGE,\n",
    "    CARE_UNIT,\n",
    "    DIAGNOSIS_CODE,\n",
    "    DIAGNOSIS_TRAJECTORY,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_CATEGORY,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    EVENT_VALUE_UNIT,\n",
    "    HOSPITAL_ID,\n",
    "    SEX,\n",
    "    SUBJECT_ID,\n",
    "    TIMESTEP,\n",
    "    YEAR,\n",
    ")\n",
    "from cyclops.processors.constants import (\n",
    "    BINARY,\n",
    "    BY,\n",
    "    CATEGORICAL_INDICATOR,\n",
    "    FEATURE_INDICATOR_ATTR,\n",
    "    FEATURE_MAPPING_ATTR,\n",
    "    FEATURE_TYPE_ATTR,\n",
    "    FEATURE_TYPES,\n",
    "    FEATURES,\n",
    "    MEAN,\n",
    "    MIN_MAX,\n",
    "    MISSING_CATEGORY,\n",
    "    NUMERIC,\n",
    "    ORDINAL,\n",
    "    STANDARD,\n",
    "    STRING,\n",
    "    TARGETS,\n",
    ")\n",
    "from cyclops.query import mimic\n",
    "from cyclops.query import process as qp\n",
    "from cyclops.utils.file import load_dataframe, save_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf11348-e062-4d45-bbe8-b569a34e1f87",
   "metadata": {},
   "source": [
    "https://github.com/sqlalchemy/sqlalchemy/wiki/RangeQuery-and-WindowedRangeQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81b3cd-e787-4569-96a2-546ed9bc64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "AFTER_DATE = f\"{min(YEARS)}-01-01\"\n",
    "OUTCOME_DEATH = \"outcome_death\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163e09f-5ff1-4128-8503-3888a0f49712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions for batching.\"\"\"\n",
    "\n",
    "import psutil\n",
    "\n",
    "from cyclops.query.util import TableTypes\n",
    "\n",
    "\n",
    "def infer_feasible_batch_size(\n",
    "    query: TableTypes, use_mem_percent: float, test_size: int = 1000\n",
    ") -> int:\n",
    "    \"\"\"Infer a feasible batch size for a given query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query: cyclops.query.util.TableTypes\n",
    "        Query for which to get a feasible batch size.\n",
    "    use_mem_percent: float\n",
    "        A decimal percentage of the available memory to use for a batch.\n",
    "    test_size: int, default = 1000\n",
    "        The test size to query and evaluate the memory for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The batch size.\n",
    "\n",
    "    \"\"\"\n",
    "    available = psutil.virtual_memory().available\n",
    "    can_use = available * use_mem_percent\n",
    "    generator = run_query_in_batches(events_query, test_size)\n",
    "    data = next(generator)\n",
    "    did_use = data.memory_usage().sum()\n",
    "    batch_size = int(((can_use) / did_use) * test_size)\n",
    "    return batch_size\n",
    "\n",
    "\n",
    "def merge_batches_grouping_id(\n",
    "    batch_before: pd.DataFrame, batch_after: pd.DataFrame, id_col: str\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    \"\"\"Merge two consequtive batches sorted by sample IDs.\n",
    "\n",
    "    batch_before: pandas.DataFrame\n",
    "        The batch coming consequtively before batch_after. Sorted by sample IDs.\n",
    "    batch_after: pandas.DataFrame\n",
    "        The batch coming consequtively after batch_before. Sorted by sample IDs.\n",
    "    id_col: str\n",
    "        Name of the sample ID column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of (pandas.DataFrame, pandas.DataFrame or None), where the first\n",
    "        DataFrame consists of batch_before and any samples in batch_after\n",
    "        with the same ID as the last ID in batch_before. The second DataFrame\n",
    "        consists of the remaining samples, or None if there all the IDs matched.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if ID is continued from the previous file\n",
    "    if batch_after[id_col].iloc[0] == batch_before[id_col].iloc[-1]:\n",
    "        # Check if the entire file consists of the same ID\n",
    "        if batch_after[id_col].iloc[-1] == batch_after[id_col].iloc[0]:\n",
    "            return pd.concat([batch_before, batch_after]), None\n",
    "\n",
    "        change_ind = (\n",
    "            (batch_after[id_col].shift() != batch_after[id_col]).iloc[1:].idxmax()\n",
    "        )\n",
    "        save_batch = pd.concat([batch_before, batch_after.iloc[:change_ind]])\n",
    "        return save_batch, batch_after.iloc[change_ind:]\n",
    "\n",
    "    return batch_before, batch_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fda23-dfd5-4229-92d4-863516b0d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@table_params_to_type(Select)\n",
    "def run_query_in_batches(\n",
    "    self,\n",
    "    query: TableTypes,\n",
    "    batch_size: int,\n",
    ") -> None:\n",
    "    \"\"\"Generate query batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query: cyclops.query.util.TableTypes\n",
    "        Query to run.\n",
    "    batch_size: int\n",
    "        Batch size.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    pandas.DataFrame\n",
    "        A query batch.\n",
    "\n",
    "    \"\"\"\n",
    "    generator = pd.read_sql_query(query, self.engine, chunksize=batch_size)\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(generator)\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "\n",
    "@table_params_to_type(Select)\n",
    "def run_query_in_grouped_batches(\n",
    "    self,\n",
    "    query: TableTypes,\n",
    "    batch_size: int,\n",
    "    id_col: str,\n",
    ") -> None:\n",
    "    \"\"\"Generate query batches with complete sets of sample IDs.\n",
    "\n",
    "    Queries are sorted and grouped such that the rows for a given sample ID are kept\n",
    "    together in a single batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query: cyclops.query.util.TableTypes\n",
    "        Query to run.\n",
    "    batch_size: int\n",
    "        Approximate batch size before rearranging based on sample IDs.\n",
    "    id_col: str\n",
    "        Name of the sample ID column by which to batch.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    pandas.DataFrame\n",
    "        A query batch with complete sets of sample IDs.\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort in order to keep same IDs together, except perahps across the transitions\n",
    "    # of batches\n",
    "    # query = select(sort_values(query, id_col)) - REPLACE WITH qp.OrderBy\n",
    "    # qp.OrderBy\n",
    "\n",
    "    print(\"Happened1\")\n",
    "\n",
    "    generator = self.run_query_in_batches(query, batch_size)\n",
    "\n",
    "    print(\"Happened2\")\n",
    "\n",
    "    batch_before = next(generator)\n",
    "    print(\"Happened3\")\n",
    "    while True:\n",
    "        try:\n",
    "            batch_after = next(generator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        save_batch, batch_before = merge_batches_grouping_id(\n",
    "            batch_before, batch_after, id_col\n",
    "        )\n",
    "\n",
    "        # If batch_before is now None, all of batch_after was merged into batch_before\n",
    "        # and it is necessary to check the next batch for the same ID\n",
    "        if batch_before is None:\n",
    "            batch_before = save_batch\n",
    "        # If not all of the batch was merged, then we have all of the last ID and\n",
    "        # we can yield\n",
    "        else:\n",
    "            yield save_batch\n",
    "            del save_batch\n",
    "\n",
    "    # Yield the last batch\n",
    "    yield batch_before\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68c082-17d8-46bc-b9ce-4b19b0a97fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_interface = mimic.events(after_date=AFTER_DATE)\n",
    "events_interface.save_in_grouped_batches(\"./test_batches\", ENCOUNTER_ID, int(5e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b9719-7dd0-4bdd-b339-39b46960f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "value_counts = []\n",
    "for i in range(66):\n",
    "    df = load_dataframe(\"./test_batches_SAVE/\" + f\"batch_\" + \"{:04d}\".format(i))\n",
    "    lens.append(len(df))\n",
    "    value_counts.append(df[ENCOUNTER_ID].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1b2f8-f062-49f2-b131-1c038df9cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f34e86-7a66-4f11-a95c-8ddcf9e1471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_value_counts = pd.concat(value_counts)\n",
    "all_value_counts.groupby(all_value_counts.index).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88462c4-0efc-4bd5-aabb-29cc21efc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac00158-2ced-48e2-9146-775002d320e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataframe(\"./test_batches_SAVE/batch_0001.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c750b2-30ea-4016-8c1e-6c1f77fb40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_interface = mimic.events(after_date=AFTER_DATE, limit=10000)  # , limit=100000)\n",
    "# events_query = events_interface.query\n",
    "# events_query = select(events_query).limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cddc3f-950c-471e-b78f-076498e2468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = mimic.events(after_date=AFTER_DATE).query\n",
    "query = qp.OrderBy(ENCOUNTER_ID)(query)\n",
    "mimic.get_interface(query).run(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df555388-d2d6-47f8-b344-b54f6734cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "mimic.get_interface(select(query).offset(10)).run(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a01666-a8c7-4838-befb-dfab7ea0a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (\n",
    "    sess.query(Object)\n",
    "    .yield_per(100)\n",
    "    .options(lazyload(\"*\"), joinedload(Object.some_related))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7f10d-ec9f-428e-8fcc-a9756c960ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "\n",
    "query = mimic.events(after_date=AFTER_DATE).query\n",
    "\n",
    "e = mimic._db.engine\n",
    "sess = Session(e)\n",
    "q = sess.query(query).yield_per(100).enable_eagerloads(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f764d2-048b-4bee-b201-c830291c7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(q):\n",
    "    print(len(data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8966c-d8a7-48e4-af9c-de0889be3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "with e.connect() as conn:\n",
    "    result = conn.execution_options(yield_per=100).execute(query)\n",
    "\n",
    "    for partition in result.partitions():\n",
    "        ## partition is an iterable that will be at most 100 items\n",
    "        # for row in partition:\n",
    "        #    print(f\"{row}\")\n",
    "        print(len(partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f1c4e-3d42-4686-aeba-c7242ab0ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for data in events_interface.run_in_grouped_batches(5000, ENCOUNTER_ID):\n",
    "    unique = data[ENCOUNTER_ID].unique()\n",
    "    unique.sort()\n",
    "    print(len(data), unique)\n",
    "    s += len(data)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1d8a7-d21a-4698-9372-ecd5614afb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828ac1b-899e-49ee-a436-8b21fce3a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import and_, func, select\n",
    "\n",
    "from cyclops.query.mimic import get_interface\n",
    "from cyclops.query.util import get_column\n",
    "\n",
    "col = get_column(events_interface.query, ENCOUNTER_ID)\n",
    "\n",
    "# table = select(col)\n",
    "\n",
    "table = select(col, func.count(col).label(\"count\")).group_by(col)\n",
    "\n",
    "# col = get_column(table, ENCOUNTER_ID)\n",
    "# table = table.order_by(col)\n",
    "\n",
    "# table = qp.GroupByAggregate(ENCOUNTER_ID, {ENCOUNTER_ID: \"count\"})(table)\n",
    "df = get_interface(table).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdef0b-36ea-4065-8c31-644ddc75b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(ENCOUNTER_ID)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03fad9d-f956-4f01-bb5d-e2593b8a5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(ENCOUNTER_ID)\n",
    "df[\"cumsum\"] = df[\"count\"].cumsum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c5d77-346d-4885-8e3d-d316ba31909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26067901, 515179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d5d38-c061-4b08-91a0-2788bf344c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = int(5e7)\n",
    "\n",
    "max_count = df[\"count\"].max()\n",
    "\n",
    "if maximum < max_count:\n",
    "    raise ValueError(f\"Maximum must be at least {max_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe567227-33ca-48f6-893f-4d07d7a084a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_dividers(query, id_col, maximum):\n",
    "    # Compute the row count for each unique value\n",
    "    col = get_column(query, id_col)\n",
    "    table = select(col, func.count(col).label(\"count\")).group_by(col)\n",
    "    count_data = get_interface(table).run()\n",
    "\n",
    "    # count_data = self.run_query(table)\n",
    "\n",
    "    # Sort and create a cumulative sum of row counts\n",
    "    count_data = count_data.sort_values(id_col)\n",
    "    count_data[\"cumsum\"] = count_data[\"count\"].cumsum()\n",
    "\n",
    "    # Create query dividers\n",
    "    last_sum = 0\n",
    "    cur_sum = 0\n",
    "    dividers = []\n",
    "    for i, s in enumerate(count_data[\"cumsum\"].values):\n",
    "        if s - last_sum > maximum:\n",
    "            dividers.append(count_data[id_col].iloc[i - 1])\n",
    "            last_sum = count_data[\"cumsum\"].iloc[i - 1]\n",
    "\n",
    "    return dividers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042660f-fd77-4155-a2c6-a2beea87d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dividers(events_interface.query, ENCOUNTER_ID, int(5e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005af85f-5abd-444d-af93-fd78d0408f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum\n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fde48-bcb0-4403-84a0-e558509cff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_dividers(series: pd.Series, partitions: int):\n",
    "    ids = encounters[ENCOUNTER_ID]\n",
    "    ids = ids.unique()\n",
    "    ids.sort()\n",
    "\n",
    "    split_points = np.linspace(0, 1, num=partitions - 1) * len(ids)\n",
    "    split_points = split_points[:-1].astype(int)\n",
    "    dividers = [ids[i] for i in split_points]\n",
    "    return dividers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178d183-382f-4d40-932c-7ecef90f3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import and_, func, select\n",
    "\n",
    "\n",
    "def column_windows(session, column, windowsize):\n",
    "    \"\"\"Return a series of WHERE clauses against\n",
    "    a given column that break it into windows.\n",
    "\n",
    "    Result is an iterable of tuples, consisting of\n",
    "    ((start, end), whereclause), where (start, end) are the ids.\n",
    "\n",
    "    Requires a database that supports window functions,\n",
    "    i.e. Postgresql, SQL Server, Oracle.\n",
    "\n",
    "    Enhance this yourself !  Add a \"where\" argument\n",
    "    so that windows of just a subset of rows can\n",
    "    be computed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def int_for_range(start_id, end_id):\n",
    "        if end_id:\n",
    "            return and_(column >= start_id, column < end_id)\n",
    "        else:\n",
    "            return column >= start_id\n",
    "\n",
    "    intervals = get_dividers\n",
    "\n",
    "    while intervals:\n",
    "        start = intervals.pop(0)\n",
    "        if intervals:\n",
    "            end = intervals[0]\n",
    "        else:\n",
    "            end = None\n",
    "        yield int_for_range(start, end)\n",
    "\n",
    "\n",
    "def windowed_query(q, column, windowsize, engine):\n",
    "    \"\"\" \"Break a Query into windows on a given column.\"\"\"\n",
    "\n",
    "    for whereclause in column_windows(q.session, column, windowsize):\n",
    "        yield pd.read_sql_query(\n",
    "            select(q.filter(whereclause).order_by(column).subquery()), engine\n",
    "        )\n",
    "        # for row in q.filter(whereclause).order_by(column):\n",
    "        #    yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953a0cb-d938-4831-b1d1-77e0cc0e2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import and_, func, select\n",
    "\n",
    "\n",
    "def column_windows(session, column, windowsize):\n",
    "    \"\"\"Return a series of WHERE clauses against\n",
    "    a given column that break it into windows.\n",
    "\n",
    "    Result is an iterable of tuples, consisting of\n",
    "    ((start, end), whereclause), where (start, end) are the ids.\n",
    "\n",
    "    Requires a database that supports window functions,\n",
    "    i.e. Postgresql, SQL Server, Oracle.\n",
    "\n",
    "    Enhance this yourself !  Add a \"where\" argument\n",
    "    so that windows of just a subset of rows can\n",
    "    be computed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def int_for_range(start_id, end_id):\n",
    "        if end_id:\n",
    "            return and_(column >= start_id, column < end_id)\n",
    "        else:\n",
    "            return column >= start_id\n",
    "\n",
    "    # q = session.query(\n",
    "    #    column,\n",
    "    #    func.row_number().over(partition_by=column, order_by=column).label('rownum')\n",
    "    # ).from_self(column)\n",
    "\n",
    "    # print(\"H\")\n",
    "    # table = select(col, func.count(col).label(\"count\")).group_by(col)\n",
    "\n",
    "    # print(len(pd.read_sql_query(table.subquery(), engine)))\n",
    "\n",
    "    # print(\"H2\")\n",
    "\n",
    "    # return None\n",
    "\n",
    "    if windowsize > 1:\n",
    "        print(\"TEXT\")\n",
    "        print(sqlalchemy.text(\"rownum %% %d=1\" % windowsize))\n",
    "        q = q.filter(sqlalchemy.text(\"rownum %% %d=1\" % windowsize))\n",
    "\n",
    "    # print(\"\\n\")\n",
    "    # print(pd.read_sql_query(q.subquery(), engine))\n",
    "\n",
    "    # intervals = [id for id, in q]\n",
    "\n",
    "    # Remove duplicates\n",
    "    result = []\n",
    "    for item in intervals:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "\n",
    "    intervals = result\n",
    "\n",
    "    print(\"intervals\\n\", intervals)\n",
    "    print([intervals[i + 1] - intervals[i] for i in range(len(intervals) - 1)])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    while intervals:\n",
    "        start = intervals.pop(0)\n",
    "        if intervals:\n",
    "            end = intervals[0]\n",
    "        else:\n",
    "            end = None\n",
    "        yield int_for_range(start, end)\n",
    "\n",
    "\n",
    "def windowed_query(q, column, windowsize, engine):\n",
    "    \"\"\" \"Break a Query into windows on a given column.\"\"\"\n",
    "\n",
    "    for whereclause in column_windows(q.session, column, windowsize):\n",
    "        yield pd.read_sql_query(\n",
    "            select(q.filter(whereclause).order_by(column).subquery()), engine\n",
    "        )\n",
    "        # for row in q.filter(whereclause).order_by(column):\n",
    "        #    yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a776165-ff4c-482e-9a96-2eb123c21961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.query.mimic import _db\n",
    "\n",
    "session = _db.session\n",
    "engine = _db.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb9c1a-e5ba-4a12-98e0-736c11a4f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.query.util import get_column\n",
    "\n",
    "q = session.query(events_interface.query)\n",
    "column = get_column(events_interface.query, ENCOUNTER_ID)\n",
    "\n",
    "s = 0\n",
    "num_encounters = 0\n",
    "for data in windowed_query(q, column, 3, engine):\n",
    "    print(len(data), len(data[ENCOUNTER_ID].unique()))\n",
    "    s += len(data)\n",
    "    num_encounters += len(data[ENCOUNTER_ID].unique())\n",
    "\n",
    "print(\"Total length:\", s)\n",
    "print(\"Num encounters:\", num_encounters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d11ef-81ff-4fd0-b95b-886ca845e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lens = []\n",
    "for i in range(100):\n",
    "    events_interface.clear_data()\n",
    "    data = events_interface.run()\n",
    "    lens.append(len(data[ENCOUNTER_ID].unique()))\n",
    "\n",
    "lens = np.array(lens)\n",
    "lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0e246-8a72-4d78-a1f2-70609db2f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[ENCOUNTER_ID].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df52e7c-b9d8-4976-8f75-01be92216348",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[ENCOUNTER_ID].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac26404-4efb-4f82-ac81-6ba31dd50b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[ENCOUNTER_ID].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0c4a3-426b-4d7c-869e-957d7f1032ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(events_interface.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869dfa7b-d892-429d-8670-349d38bdd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in events_interface.run_in_batches(10000):\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96763ecb-d455-43bb-9ab8-c5ffb0bfe4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in events_interface.run_in_grouped_batches(10000, ENCOUNTER_ID):\n",
    "    print(\"LENGTH:\", len(df))\n",
    "    print(df[ENCOUNTER_ID])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adf0ff-7f96-48de-99c2-88be854cfd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_interface.save_in_grouped_batches(\"./test_batches\", 10000, ENCOUNTER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f7268-b886-410d-a673-a5ed9225de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_interface.save_in_grouped_batches(\"./test_batches\", 5e6, ENCOUNTER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52b0bc-52f9-40c3-8cdf-63981a9a0079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e270d9-3da9-49ec-984d-970cf8e7fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, List, Union\n",
    "\n",
    "from sqlalchemy.sql.schema import Column\n",
    "\n",
    "from cyclops.utils.batching import query_batch_conditions\n",
    "\n",
    "\n",
    "@table_params_to_type(Select)\n",
    "def windowed_query(\n",
    "    self, query, column, window_size\n",
    ") -> Generator[pd.DataFrame, None, None]:\n",
    "    \"\"\" \"Break a query into batches by segmenting a given column into value ranges.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Opportunity for multi-processing/parallelization here!\n",
    "    for condition in query_batch_conditions(self.session, column, window_size):\n",
    "        query = query.where(cond).subquery()\n",
    "        yield pd.read_sql_query(run_query, self.engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5dc2dd-5fa4-46f6-b547-2ea10328eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = mimic._db.session\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd288031-7567-4d0b-97f6-0b3832a2d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = mimic._db.engine\n",
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378688b2-6935-42c3-888d-429e34713a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "from cyclops.query.util import get_column\n",
    "\n",
    "query = events_interface.query\n",
    "\n",
    "s = 0\n",
    "for data in windowed_query(\n",
    "    session.query(query), session, engine, get_column(query, ENCOUNTER_ID), int(1e7)\n",
    "):\n",
    "    print(len(data), len(data[ENCOUNTER_ID].unique()))\n",
    "    s += len(data)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe446e-e55c-4cab-a762-c46129f47fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "select(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79873db-91c8-4901-b420-8c71f160938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9965b-b467-47b7-b5ce-20b7f21cdc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fa385-09c0-4b1c-8b7e-380418c9cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = events_interface.run()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b1e8d-8e17-4734-8b24-1f1cbf050478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ENCOUNTER_ID].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942d582-de41-4c17-a9d0-3ced05b11816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfe08e-acc1-4f77-a8f2-47332ba55a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ba80d-da74-40bb-bb2a-9310bdd4356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_interface = mimic.events(after_date=AFTER_DATE)\n",
    "events_query = events_interface.query\n",
    "events_query = qp.Drop([\"warning\", \"itemid\", \"storetime\"])(events_query)\n",
    "events_interface = mimic.get_interface(events_query)\n",
    "events = events_interface.run()  # limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb5a0b-f0fc-4d7d-971a-c657d427be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the deidentified dating\n",
    "events = pd.merge(\n",
    "    encounters[[ENCOUNTER_ID, \"anchor_year_difference\"]], events, on=ENCOUNTER_ID\n",
    ")\n",
    "\n",
    "\n",
    "def add_offset(row):\n",
    "    row[EVENT_TIMESTAMP] += pd.DateOffset(years=row[\"anchor_year_difference\"])\n",
    "    return row\n",
    "\n",
    "\n",
    "events = events.apply(add_offset, axis=1)\n",
    "events = events.drop(\"anchor_year_difference\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e4baf-d3d8-4863-97bc-af18fc035806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target as a timeseries event\n",
    "target_events = encounters[encounters[OUTCOME_DEATH] == True]\n",
    "target_events = target_events[[ENCOUNTER_ID, \"deathtime\"]]\n",
    "target_events = target_events.rename({\"deathtime\": EVENT_TIMESTAMP}, axis=1)\n",
    "target_events[EVENT_NAME] = OUTCOME_DEATH\n",
    "target_events[EVENT_CATEGORY] = TARGETS\n",
    "target_events[EVENT_VALUE] = 1\n",
    "target_events.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6aca3-2a88-4bab-8a84-973bcdc36419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include target\n",
    "events = pd.concat([events, target_events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a818d-7ed4-48a6-9530-b232eb91b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "events[EVENT_NAME] = normalize_names(events[EVENT_NAME])\n",
    "events[EVENT_CATEGORY] = normalize_categories(events[EVENT_CATEGORY])\n",
    "# events[EVENT_VALUE] = normalize_values(events[EVENT_VALUE])\n",
    "\n",
    "# Concatenate event name and category since some names are the same in\n",
    "# different categories, e.g., 'flow' for categories 'heartware' and 'ecmo'\n",
    "events[EVENT_NAME] = events[EVENT_CATEGORY] + \" - \" + events[EVENT_NAME]\n",
    "events.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc2a25-f959-48f4-ad3b-4b95f940a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(events, \"events.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9671e-84dc-4e60-9d64-ca67ff0e5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort by encounters\n",
    "break between (not in the middle of) encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87dcdb-2556-44b8-a5ed-d563f447a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_over = ENCOUNTER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337042e-354e-4aa8-9e3b-d64eeb2ee854",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(10e6)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cd4a3-f460-4107-9b74-d20a84083f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47274e35-550f-44a1-8e89-7e5f32463822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d950325-8667-4f99-9d60-dc689757852a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86a1fe-867f-49f5-8154-b0e78c98cd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04553254-a5d4-48fd-bf2b-88c4bdf9e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import Column, Integer, and_, create_engine, func, select\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from cyclops.query import mimic\n",
    "from cyclops.query.util import get_column\n",
    "\n",
    "\n",
    "def column_windows(session, column, windowsize):\n",
    "    \"\"\"Return a series of WHERE clauses against\n",
    "    a given column that break it into windows.\n",
    "\n",
    "    Result is an iterable of tuples, consisting of\n",
    "    ((start, end), whereclause), where (start, end) are the ids.\n",
    "\n",
    "    Requires a database that supports window functions,\n",
    "    i.e. Postgresql, SQL Server, Oracle.\n",
    "\n",
    "    Enhance this yourself !  Add a \"where\" argument\n",
    "    so that windows of just a subset of rows can\n",
    "    be computed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def int_for_range(start_id, end_id):\n",
    "        if end_id:\n",
    "            return and_(column >= start_id, column < end_id)\n",
    "        else:\n",
    "            return column >= start_id\n",
    "\n",
    "    q = session.query(\n",
    "        column, func.row_number().over(order_by=column).label(\"rownum\")\n",
    "    ).from_self(column)\n",
    "    if windowsize > 1:\n",
    "        q = q.filter(sqlalchemy.text(\"rownum %% %d=1\" % windowsize))\n",
    "\n",
    "    intervals = [id for id, in q]\n",
    "\n",
    "    while intervals:\n",
    "        start = intervals.pop(0)\n",
    "        if intervals:\n",
    "            end = intervals[0]\n",
    "        else:\n",
    "            end = None\n",
    "        yield int_for_range(start, end)\n",
    "\n",
    "\n",
    "def windowed_query(q, column, windowsize):\n",
    "    \"\"\" \"Break a Query into windows on a given column.\"\"\"\n",
    "\n",
    "    for whereclause in column_windows(q.session, column, windowsize):\n",
    "        for row in q.filter(whereclause).order_by(column):\n",
    "            yield row\n",
    "\n",
    "\n",
    "e = mimic._db.engine\n",
    "s = Session(e)\n",
    "\n",
    "query = mimic.events(after_date=AFTER_DATE, limit=10000).query\n",
    "q = s.query(query)\n",
    "\n",
    "encounter_ids = []\n",
    "for data in windowed_query(q, get_column(query, ENCOUNTER_ID), 1000):\n",
    "    encounter_ids.append(data[0])\n",
    "\n",
    "print(len(encounter_ids))\n",
    "np.unique(encounter_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede5535-2919-423a-9176-5edb6b93b87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-env",
   "language": "python",
   "name": "cyclops-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
