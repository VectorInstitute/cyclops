{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5117a0d-9d1f-495e-93d5-edb057cf4c0a",
   "metadata": {},
   "source": [
    "## Data pipeline for predicting risk of mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb2625-e38f-428a-bcfd-6299b405d0f4",
   "metadata": {},
   "source": [
    "## Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.feature_handler import FeatureHandler\n",
    "from cyclops.plotter import set_bars_color, setup_plot, plot_timeline\n",
    "from cyclops.processor import run_data_pipeline\n",
    "from cyclops.processors.aggregate import Aggregator\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    AGE,\n",
    "    DIAGNOSIS_CODE,\n",
    "    DISCHARGE_DISPOSITION,\n",
    "    DISCHARGE_TIMESTAMP,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    EVENT_CATEGORY,\n",
    "    HOSPITAL_ID,\n",
    "    LENGTH_OF_STAY_IN_ER,\n",
    "    RESTRICT_TIMESTAMP,\n",
    "    SEX,\n",
    "    TRIAGE_LEVEL,\n",
    "    WINDOW_START_TIMESTAMP,\n",
    "    TIMESTEP,\n",
    ")\n",
    "from cyclops.processors.constants import SMH\n",
    "from cyclops.processors.events import (\n",
    "    combine_events,\n",
    "    convert_to_events,\n",
    "    normalize_events,\n",
    ")\n",
    "from cyclops.processors.impute import Imputer\n",
    "from cyclops.processors.statics import compute_statics\n",
    "from cyclops.processors.util import (\n",
    "    fill_missing_timesteps,\n",
    "    gather_columns,\n",
    "    pivot_aggregated_events_to_features,\n",
    "    create_indicator_variables\n",
    ")\n",
    "from cyclops.query import gemini\n",
    "from cyclops.utils.file import save_dataframe, load_dataframe\n",
    "\n",
    "\n",
    "MORTALITY = \"mortality\"\n",
    "LOS = \"los\"\n",
    "BASE_DATA_PATH = \"/mnt/nfs/project/delirium/drift_exp/_extract\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8091448-5301-4631-a7d4-150667441e77",
   "metadata": {},
   "source": [
    "## Run query, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9efab5-8eb8-45fc-a53d-e5bfcc72ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "er_admin_table = gemini.get_table(gemini.ER_ADMIN)\n",
    "encounters = gemini.patient_encounters(\n",
    "    er_admin_table=er_admin_table,\n",
    "    years=[2018, 2019, 2020],\n",
    "    died=True,\n",
    "    died_binarize_col=\"mortality\",\n",
    ")\n",
    "encounters_labs = gemini.events(\n",
    "    patient_encounters_table=encounters.query, event_category=\"lab\"\n",
    ")\n",
    "encounters_vitals = gemini.events(\n",
    "    patient_encounters_table=encounters.query, event_category=\"vitals\"\n",
    ")\n",
    "imaging = gemini.imaging(years=[2018, 2019, 2020])\n",
    "transfusions = gemini.blood_transfusions(years=[2018, 2019, 2020])\n",
    "interventions = gemini.interventions()\n",
    "\n",
    "encounters.run()\n",
    "encounters_labs.run()\n",
    "encounters_vitals.run()\n",
    "imaging.run()\n",
    "transfusions.run()\n",
    "interventions.run()\n",
    "\n",
    "print(f\"{len(encounters.data)} rows extracted!\")\n",
    "print(f\"{len(encounters_labs.data)} rows extracted!\")\n",
    "print(f\"{len(encounters_vitals.data)} rows extracted!\")\n",
    "print(f\"{len(imaging.data)} rows extracted!\")\n",
    "print(f\"{len(transfusions.data)} rows extracted!\")\n",
    "print(f\"{len(interventions.data)} rows extracted!\")\n",
    "\n",
    "encounters_imaging = pd.merge(encounters.data, imaging.data, on=ENCOUNTER_ID, how=\"inner\")\n",
    "encounters_transfusions = pd.merge(encounters.data, transfusions.data, on=ENCOUNTER_ID, how=\"inner\")\n",
    "encounters_interventions = pd.merge(encounters.data, interventions.data, on=ENCOUNTER_ID, how=\"inner\")\n",
    "\n",
    "os.makedirs(BASE_DATA_PATH, exist_ok=True)\n",
    "encounters.save(os.path.join(BASE_DATA_PATH, \"admin_er\"))\n",
    "encounters_labs.save(os.path.join(BASE_DATA_PATH, \"labs\"))\n",
    "encounters_vitals.save(os.path.join(BASE_DATA_PATH, \"vitals\"))\n",
    "encounters_imaging.to_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "encounters_transfusions.to_parquet(os.path.join(BASE_DATA_PATH, \"transfusions.parquet\"))\n",
    "encounters_interventions.to_parquet(os.path.join(BASE_DATA_PATH, \"interventions.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde24be8-7759-4241-a24d-e63eaa9ce9ac",
   "metadata": {},
   "source": [
    "## Read saved query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a868e-11ab-432e-9d45-006f57d5a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"admin_er.parquet\"))\n",
    "labs_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"labs.parquet\"))\n",
    "# vitals_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"vitals.parquet\"))\n",
    "imaging_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "transfusions_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"transfusions.parquet\"))\n",
    "interventions_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"interventions.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0af1b6-1e8b-4554-944f-f13209d282cc",
   "metadata": {},
   "source": [
    "## Map imaging and transfusions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131da1-d7e8-4564-b1ff-d6e9e2251d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_data = imaging_data.rename(columns={\"imaging_test_description\": \"event_name\", \"performed_date_time\": \"event_timestamp\"})\n",
    "imaging_data[EVENT_CATEGORY] = \"imaging\"\n",
    "imaging_data[EVENT_VALUE] = 1\n",
    "\n",
    "transfusions_data = transfusions_data.rename(columns={\"issue_date_time\": \"event_timestamp\"})\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[\"rbc_mapped\"]\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[EVENT_NAME].apply(lambda x: \"rbc\" if x else \"non-rbc\")\n",
    "transfusions_data[EVENT_VALUE] = 1\n",
    "transfusions_data[EVENT_CATEGORY] = \"transfusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564db1ba-9f9d-4b39-a57a-90a357afab8f",
   "metadata": {},
   "source": [
    "##  Process interventions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34a21c-49ec-48a9-bfac-0cae8dce5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions_data[EVENT_NAME] = \"\"\n",
    "interventions_data[EVENT_VALUE] = 1\n",
    "interventions_data[EVENT_CATEGORY] = \"interventions\"\n",
    "\n",
    "for name in [\"endoscopy_mapped\", \"gi_endoscopy_mapped\", \"bronch_endoscopy_mapped\", \"dialysis_mapped\", \"inv_mech_vent_mapped\", \"surgery_mapped\"]:\n",
    "    cond = interventions_data[name] == 1\n",
    "    interventions_data[EVENT_NAME][cond] = name.replace(\"_mapped\", \"\")\n",
    "interventions_data[EVENT_NAME] = interventions_data[EVENT_NAME].apply(lambda x: \"unmapped_intervention\" if x == \"\" else x)\n",
    "interventions_data = interventions_data[interventions_data[\"intervention_episode_start_date\"].notna()]\n",
    "\n",
    "interventions_data = interventions_data.rename(columns={\"intervention_episode_start_date\": EVENT_TIMESTAMP})\n",
    "interventions_data[EVENT_TIMESTAMP] = interventions_data[EVENT_TIMESTAMP].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e205-6491-442d-92a2-abc9a5c2a15d",
   "metadata": {},
   "source": [
    "## Filter out encounters that had less than 24 hours LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc8075-57e8-49ac-b6f8-124d5b3591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data[LOS] = encounters_data[DISCHARGE_TIMESTAMP] - encounters_data[ADMIT_TIMESTAMP] + pd.to_timedelta(encounters_data[LENGTH_OF_STAY_IN_ER], unit=\"h\")\n",
    "encounters_data_atleast_los_24_hrs = encounters_data.loc[encounters_data[LOS] >= pd.to_timedelta(24, unit=\"h\")]\n",
    "# encounters_data_atleast_los_24_hrs                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72566d9f-db66-47f2-8820-fdd8904aafe1",
   "metadata": {},
   "source": [
    "## Get encounters that ended in mortality outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f04573-71c9-4f25-9a31-21df233e8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality = encounters_data_atleast_los_24_hrs.loc[encounters_data_atleast_los_24_hrs[MORTALITY] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae3659-4304-4697-808d-75c66d2fe6f3",
   "metadata": {},
   "source": [
    "## Get encounters that didn't end up in mortality outcome (limit to a subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d191738-2eb9-4cda-83cd-fe17a7fc72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_not_mortality = encounters_data_atleast_los_24_hrs.loc[encounters_data_atleast_los_24_hrs[MORTALITY] == False]\n",
    "\n",
    "num_encounters_not_mortality = len(encounters_mortality)\n",
    "encounters_not_mortality_subset = encounters_not_mortality[0:num_encounters_not_mortality]\n",
    "# encounters_not_mortality_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464ae6f-2fe4-419e-8afb-57c8ab974136",
   "metadata": {},
   "source": [
    "## Combine both subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d25c8-e4a1-4529-a9de-5b02bd37c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_train_val_test = pd.concat([encounters_mortality, encounters_not_mortality_subset], ignore_index=True)\n",
    "# encounters_train_val_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496ca76-0058-4223-8925-3c357ee153c2",
   "metadata": {},
   "source": [
    "## Offset discharge timestamp by K hours and create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c71da3-c4ec-4305-86a9-608dc6f556d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0  # in hours\n",
    "encounters_mortality = encounters_mortality.copy()\n",
    "encounters_mortality[\"death_timestamp_offset\"] = encounters_mortality[\n",
    "    DISCHARGE_TIMESTAMP\n",
    "] + pd.Timedelta(-offset, unit=\"h\")\n",
    "\n",
    "# encounters_mortality[[DISCHARGE_TIMESTAMP, \"death_timestamp_offset\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6498e-f2a6-4383-89bd-6c9b5b4ee144",
   "metadata": {},
   "source": [
    "## Convert \"death_timestamp_offset\" to event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90081c16-ac41-4b60-ab48-7ff747630e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_risk_events = convert_to_events(\n",
    "    encounters_mortality, event_name=f\"mortality_risk_{offset}\", event_category=\"general\", timestamp_col=\"death_timestamp_offset\"\n",
    ")\n",
    "mortality_risk_events = pd.merge(\n",
    "    mortality_risk_events, encounters_mortality, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "mortality_risk_events = mortality_risk_events[\n",
    "    [ENCOUNTER_ID, EVENT_NAME, EVENT_TIMESTAMP, ADMIT_TIMESTAMP, EVENT_VALUE, EVENT_CATEGORY]\n",
    "]\n",
    "mortality_risk_events[EVENT_VALUE] = 1\n",
    "# mortality_risk_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2653f-b1de-4e63-8531-b92677534041",
   "metadata": {},
   "source": [
    "## Get admission/discharge events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de93b44-b260-44d5-869f-80752cc913c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_events = convert_to_events(encounters_train_val_test, event_name=\"admission\", event_category=\"general\", timestamp_col=\"admit_timestamp\")\n",
    "disch_events = convert_to_events(encounters_train_val_test, event_name=\"discharge\", event_category=\"general\", timestamp_col=\"discharge_timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238709bd-47f6-4cf6-8562-09717824d57f",
   "metadata": {},
   "source": [
    "## Filter labs, vitals, imaging, transfusions, interventions to be in train_val_test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d7deb-f5bf-4a7a-a773-5768a68f7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_train_val_test = labs_data.loc[\n",
    "    labs_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "imaging_train_val_test = imaging_data.loc[\n",
    "    imaging_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "transfusions_train_val_test = transfusions_data.loc[\n",
    "    transfusions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "interventions_train_val_test = interventions_data.loc[\n",
    "    interventions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e4baa-d706-4b72-88e4-7416db744d7e",
   "metadata": {},
   "source": [
    "## Normalize all event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e249a41-d012-4f25-9241-66c15fad7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_events = normalize_events(imaging_train_val_test)\n",
    "transfusion_events = normalize_events(transfusions_train_val_test)\n",
    "lab_events = normalize_events(labs_train_val_test)\n",
    "mortality_risk_events = normalize_events(mortality_risk_events)\n",
    "intervention_events = normalize_events(interventions_train_val_test)\n",
    "admit_events = normalize_events(admit_events)\n",
    "disch_events = normalize_events(disch_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950cea0-8178-4303-865d-ceb47ac2b30d",
   "metadata": {},
   "source": [
    "## Combine different event data, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080940-2a0e-45ab-987a-ef7f200a7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_events = combine_events(\n",
    "    [\n",
    "        intervention_events,\n",
    "        imaging_events,\n",
    "        transfusion_events,\n",
    "        lab_events,\n",
    "        mortality_risk_events,\n",
    "        admit_events,\n",
    "        disch_events\n",
    "    ]\n",
    ")\n",
    "save_dataframe(combined_events, os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692cd1a-e9ec-456d-9df2-3a9cfef6eeab",
   "metadata": {},
   "source": [
    "## Load combined events, aggregate them, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9306e6-90a0-4adb-8691-972424912f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e10d6-b372-45e6-b0b3-8aa45593ef38",
   "metadata": {},
   "source": [
    "## Aggregate combined events, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e02af-1c6e-4cec-8b84-52e2cf24c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator(bucket_size=24, window=144)\n",
    "aggregated_events = aggregator(combined_events)\n",
    "save_dataframe(aggregated_events, os.path.join(BASE_DATA_PATH, \"aggregated_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc55f-d3f2-41c6-a367-c8a817b424d8",
   "metadata": {},
   "source": [
    "## Load aggregated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab947b7d-b489-4ce6-be32-546a8d0ae6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"aggregated_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de68a9-434b-44a1-bbb2-3b9cb1759f91",
   "metadata": {},
   "source": [
    "## Pivot table to get column that can be used to create labels. 1 corresponds to timestep where death happens (shifted by offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84f2b5-09a9-49ef-967f-42496665d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_mortality = aggregated_events.loc[aggregated_events[EVENT_NAME] == \"death\"]\n",
    "pivoted_mortality = pivot_aggregated_events_to_features(aggregated_mortality, np.mean)\n",
    "timesteps_mortality = pivoted_mortality.loc[\n",
    "    pivoted_mortality[\"death\"] == 1\n",
    "].index.get_level_values(1)\n",
    "\n",
    "timesteps_mortality_counts = timesteps_mortality.value_counts()\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 5), tight_layout=True)\n",
    "ts_vals = plt.bar(\n",
    "    list(np.unique(timesteps_mortality)), list(timesteps_mortality_counts), alpha=0.5\n",
    ")\n",
    "set_bars_color(ts_vals, \"r\")\n",
    "axs.set_xticks(\n",
    "    range(len(ts_vals)),\n",
    "    list(np.unique(timesteps_mortality)),\n",
    "    rotation=\"vertical\",\n",
    "    fontsize=20,\n",
    ")\n",
    "setup_plot(\n",
    "    axs,\n",
    "    \"Mortality encounter distribution over timesteps\",\n",
    "    \"timestep\",\n",
    "    \"Num. encounters that resulted in mortality in that timestep\",\n",
    "    [\"Count\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4793-a22b-4611-96b3-65e2bd0d3d3c",
   "metadata": {},
   "source": [
    "## Pivot aggregated events to get column-wise temporal features and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a50a78-9e33-4f71-b45c-a2ca21ad31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = pivot_aggregated_events_to_features(aggregated_events, np.mean)\n",
    "save_dataframe(temporal_features, os.path.join(BASE_DATA_PATH, \"temporal_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277e20e-c9c7-427c-b3a4-e6120b342581",
   "metadata": {},
   "source": [
    "## Add to feature handler, with indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf2259-d1e8-4d72-acb5-d4c1ebd4cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler = FeatureHandler()\n",
    "\n",
    "temporal_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"temporal_features\"))\n",
    "# indicators = create_indicator_variables(temporal_features)\n",
    "feature_handler.add_features(temporal_features)\n",
    "# feature_handler.add_features(indicators)\n",
    "# feature_handler.drop_features([\"death_indicator\"])\n",
    "feature_handler.features[\"temporal\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725048d6-7442-48a3-ae76-e3d29280eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler.features[\"temporal\"][\"unmapped_intervention\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e89d8-a325-45f5-a38e-0f0ef6abe443",
   "metadata": {},
   "source": [
    "## Compute static features, save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c4497-03d3-4304-86d1-54b4eb5b5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality = gather_columns(\n",
    "    encounters_mortality,\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        AGE,\n",
    "        SEX,\n",
    "        HOSPITAL_ID,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        DISCHARGE_TIMESTAMP,\n",
    "        TRIAGE_LEVEL,\n",
    "    ],\n",
    ")\n",
    "static_features = compute_statics(encounters_mortality)\n",
    "save_dataframe(static_features, os.path.join(BASE_DATA_PATH, \"static_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b6950-850c-4a83-a568-45088411911a",
   "metadata": {},
   "source": [
    "##  Load static features, add to feature handler, save all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4759c9-bfd8-42ec-adeb-c518489caae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"static_features\"))\n",
    "feature_handler.add_features(\n",
    "    static_features, reference_cols=[HOSPITAL_ID, ADMIT_TIMESTAMP, DISCHARGE_TIMESTAMP]\n",
    ")\n",
    "feature_handler.save(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028c06-7771-46db-a291-f7621471ef5e",
   "metadata": {},
   "source": [
    "## Create new feature handler, load saved features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836f236-1f0b-46e8-90b8-8e811adcbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler1 = FeatureHandler()\n",
    "feature_handler1.load(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da874af-fc43-4373-86b5-e3296893c27b",
   "metadata": {},
   "source": [
    "## Impute temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a57ca-2e72-4f17-8452-3c3e5f4d6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_simple(dataframe: pd.DataFrame, time_index: str=TIMESTEP) -> pd.DataFrame:\n",
    "    \"\"\"Impute features using 'Simple' method.\n",
    "    \n",
    "    Concatenate the forward filled value, the mask of the measurement,\n",
    "    and the time of the last measurement.\n",
    "\n",
    "    Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu,\n",
    "    \"Recurrent Neural Networks for Multivariate Time Series with Missing Values,\"\n",
    "    Scientific Reports, vol. 8, no. 1, p. 6085, Apr 2018.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pandas.DataFrame\n",
    "        Temporal features dataframe.\n",
    "    time_index: str, optional\n",
    "        The name of the time-series index.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "     Dataframe after applying imputation.\n",
    "     \n",
    "    \"\"\"\n",
    "    # Mask missingness.\n",
    "    masked_df=pd.isna(dataframe)\n",
    "    masked_df=masked_df.apply(pd.to_numeric)\n",
    "\n",
    "    # Compute time since last measurement.\n",
    "    index_of_time=list(dataframe.index.names).index(time_index)\n",
    "    time_in=[item[index_of_time] for item in dataframe.index.tolist()]\n",
    "    time_df=dataframe.copy()\n",
    "    for col in time_df.columns.tolist():\n",
    "        time_df[col]=time_in\n",
    "    time_df[masked_df]=np.nan\n",
    "\n",
    "    # Concatenate the dataframes.\n",
    "    df_prime=pd.concat([dataframe, masked_df, time_df], axis=1, keys=['measurement','mask', 'time'])\n",
    "    df_prime.columns=df_prime.columns.rename(\"impute_simple\", level=0)\n",
    "\n",
    "    # Fill each dataframe using either ffill or mean.\n",
    "    df_prime=df_prime.fillna(method='ffill').unstack().fillna(0)\n",
    "\n",
    "    # Swap the levels so that the simple imputation feature is the lowest value.\n",
    "    col_level_names=list(df_prime.columns.names)\n",
    "    col_level_names.append(col_level_names.pop(0))\n",
    "\n",
    "    df_prime=df_prime.reorder_levels(col_level_names, axis=1)\n",
    "    df_prime.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    return  df_prime\n",
    "\n",
    "\n",
    "def create_train_test_split(encounters: pd.DataFrame, fractions: Optional[List] = [0.8, 0.2], split_column: Optional[str] = None, split_values: List = None) -> tuple:\n",
    "    \"\"\"Split encounters into train/test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encounters: pandas.DataFrame\n",
    "        Dataframe with encounter IDs.\n",
    "    fractions: list, optional\n",
    "        Fraction of samples to use for train, test sets.\n",
    "    split_column: str, optional\n",
    "        If 'split_column' is specified, then that column is used to split.\n",
    "    split_values: list, optional\n",
    "        Along with 'split_column', a list of lists can be specified for filtering.\n",
    "        e.g. [[2008], [2009, 2010]] for train/test split based on year.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train IDs, test IDs)\n",
    "    \n",
    "    \"\"\"\n",
    "    if split_column:\n",
    "        if split_column not in encounters.columns:\n",
    "            raise ValueError(\"Specified 'split column' not found in input dataframe\")\n",
    "        if not split_values:\n",
    "            raise ValueError(\"Specify train/test split values for the 'split column'.!\")\n",
    "        train_encounters = encounters[ENCOUNTER_ID].loc[encounters[split_column].isin(split_values[0])]\n",
    "        test_encounters = encounters[ENCOUNTER_ID].loc[encounters[split_column].isin(split_values[1])]\n",
    "        return train_encounters, test_encounters\n",
    "    \n",
    "    encounter_ids = list(encounters[ENCOUNTER_ID].unique())\n",
    "    random.shuffle(encounter_ids)\n",
    "    num_train = int(fractions[0] * len(encounter_ids))\n",
    "    \n",
    "    return encounter_ids[0: num_train], encounter_ids[num_train:]\n",
    "\n",
    "\n",
    "\n",
    "# By Year\n",
    "encounters_mortality[\"year\"] = encounters_mortality[\"admit_timestamp\"].dt.year\n",
    "train_ids, val_test_ids = create_train_test_split(encounters_mortality, split_column=\"year\", split_values=[[2018, 2019], [2020]])\n",
    "\n",
    "# By HOSPITAL_ID\n",
    "# train_ids, val_test_ids = create_train_test_split(encounters_mortality, split_column=HOSPITAL_ID, split_values=[[\"THPM\", \"SMH\", \"THPC\", \"SBK\", \"MSH\"], [\"UHNTW\", \"UHNTG\"]])\n",
    "\n",
    "# Random\n",
    "# train_ids, val_test_ids = create_train_test_split(encounters_mortality, split_column=HOSPITAL_ID, split_values=[[\"THPM\", \"SMH\", \"THPC\"], [\"UHNTW\", \"UHNTG\", \"MSH\"]])\n",
    "\n",
    "val_ids, test_ids = create_train_test_split(encounters_mortality.loc[encounters_mortality[ENCOUNTER_ID].isin(val_test_ids)], [0.5, 0.5])\n",
    "print(f\"Train set: {len(train_ids)}, Val set: {len(val_ids)}, Test set: {len(test_ids)}\")\n",
    "encounters_mortality[HOSPITAL_ID].value_counts(), encounters_mortality[\"year\"].value_counts()\n",
    "\n",
    "static = feature_handler1.features[\"static\"]\n",
    "temporal = feature_handler1.features[\"temporal\"]\n",
    "\n",
    "mortality_risk_24_targets = temporal[\"death\"].copy()\n",
    "temporal = temporal.drop(columns=\"death\")\n",
    "\n",
    "X = temporal[np.in1d(temporal.index.get_level_values(0), static.index.get_level_values(0))]\n",
    "y = mortality_risk_24_targets[np.in1d(mortality_risk_24_targets.index.get_level_values(0), X.index.get_level_values(0))]\n",
    "\n",
    "y_train, y_val, y_test = [y[np.in1d(y.index.get_level_values(0), ids)] for ids in [train_ids, val_ids, test_ids]]\n",
    "X_train, X_val, X_test = [X[np.in1d(X.index.get_level_values(0), ids)] for ids in [train_ids, val_ids, test_ids]]\n",
    "\n",
    "len(X), len(X_train), len(X_val), len(X_test), len(y), len(y_train), len(y_val), len(y_test)\n",
    "\n",
    "\n",
    "# X_train_inputs = impute_simple(X_train)\n",
    "# X_train_inputs = format_dataset(X_train, level=\"features\",imputation_method=\"simple\")\n",
    "# X_val = X[np.in1d(X.index.get_level_values(0), y_val.index.get_level_values(0))]\n",
    "# X_val_inputs = format_dataset(X_val, level=\"features\",imputation_method=\"simple\")\n",
    "# X_test = X[np.in1d(X.index.get_level_values(0), y_test.index.get_level_values(0))]\n",
    "# X_test_inputs = format_dataset(X_test, level=\"features\",imputation_method=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb97e-e050-43fb-9b60-26f204260706",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_id = random.choice(temporal.index.get_level_values(0))\n",
    "X_train_inputs.loc[encounter_id][\"white blood cell count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93e697-f310-4fe5-a651-537187359fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_id = random.choice(combined_events[\"encounter_id\"].unique())\n",
    "# encounter_id = 14235864\n",
    "\n",
    "# Some strange cases:\n",
    "# 13759829\n",
    "\n",
    "# Some interesting cases:\n",
    "# 13542750\n",
    "# 15624655\n",
    "# 15351776\n",
    "# 15611082\n",
    "# 15794127\n",
    "\n",
    "print(encounter_id)\n",
    "combined_events_encounter = combined_events.loc[combined_events[\"encounter_id\"] == encounter_id]\n",
    "fig = plot_timeline(combined_events_encounter, return_fig=True)\n",
    "\n",
    "fig = fig.update_layout(\n",
    "    width=2000,\n",
    "    height=800\n",
    "    \n",
    ")\n",
    "fig.show()\n",
    "# combined_events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops",
   "language": "python",
   "name": "cyclops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
