{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9494d9-4240-4581-a5f2-df132bb63051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wanglabconsts import (\n",
    "from mimicmortalityconsts import (\n",
    "    AGGREGATED_DIR,\n",
    "    AGGREGATED_FILE,\n",
    "    CLEANED_DIR,\n",
    "    CONST_NAME,\n",
    "    ENCOUNTERS_FILE,\n",
    "    OUTCOME_DEATH,\n",
    "    OUTCOME_DEATH_PROCESSED,\n",
    "    PREDICT_OFFSET,\n",
    "    QUERIED_DIR,\n",
    "    SPLIT_FRACTIONS,\n",
    "    TAB_FEATURES_FILE,\n",
    "    TAB_TARGETS,\n",
    "    TAB_VEC_COMB,\n",
    "    TAB_VECTORIZED_FILE,\n",
    "    TEMP_TARGETS,\n",
    "    TEMP_VECTORIZED_FILE,\n",
    "    TIMESTEP_SIZE,\n",
    "    VECTORIZED_DIR,\n",
    "    WINDOW_DURATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87186a96-e5c5-4dce-ace1-48328b6c3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input(f\"WARNING: LOADING CONSTANTS FROM {CONST_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8187b80-aa12-4c9b-97b2-83f83691d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.processors.aggregate import Aggregator, tabular_as_aggregated\n",
    "from cyclops.processors.cleaning import (\n",
    "    normalize_categories,\n",
    "    normalize_names,\n",
    "    normalize_values,\n",
    ")\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    AGE,\n",
    "    CARE_UNIT,\n",
    "    DIAGNOSIS_CODE,\n",
    "    DIAGNOSIS_TRAJECTORY,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_CATEGORY,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    EVENT_VALUE_UNIT,\n",
    "    HOSPITAL_ID,\n",
    "    SEX,\n",
    "    SUBJECT_ID,\n",
    "    TIMESTEP,\n",
    "    YEAR,\n",
    ")\n",
    "from cyclops.processors.constants import (\n",
    "    ALL,\n",
    "    FEATURES,\n",
    "    MEAN,\n",
    "    MISSING_CATEGORY,\n",
    "    NUMERIC,\n",
    "    ORDINAL,\n",
    "    STANDARD,\n",
    "    TARGETS,\n",
    ")\n",
    "from cyclops.processors.feature.feature import TabularFeatures, TemporalFeatures\n",
    "from cyclops.processors.feature.normalization import VectorizedNormalizer\n",
    "from cyclops.processors.feature.vectorize import (\n",
    "    Vectorized,\n",
    "    intersect_vectorized,\n",
    "    split_vectorized,\n",
    "    vec_index_exp,\n",
    ")\n",
    "from cyclops.processors.impute import (\n",
    "    np_ffill,\n",
    "    np_ffill_bfill,\n",
    "    np_fill_null_num,\n",
    "    np_fill_null_zero,\n",
    ")\n",
    "from cyclops.query import mimic\n",
    "from cyclops.query import process as qp\n",
    "from cyclops.utils.common import print_dict\n",
    "from cyclops.utils.file import (\n",
    "    join,\n",
    "    load_array,\n",
    "    load_dataframe,\n",
    "    load_pickle,\n",
    "    process_dir_save_path,\n",
    "    save_dataframe,\n",
    "    save_pickle,\n",
    "    yield_dataframes,\n",
    "    yield_pickled_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f9507-91bd-487f-a73d-0faccf54c7a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8aec10-040a-4a71-8433-40f9d6855dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_features = load_pickle(TAB_FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b3a5b-703a-4969-9b8e-3e7dbd439f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = tab_features.features_by_type(NUMERIC)\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90115b1-3e5e-412c-ae3a-51e1f7d5607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = tab_features.features_by_type(ORDINAL)\n",
    "ordinal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda135bc-f0f6-4dd1-bb4e-91d96a24f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_vectorized = tab_features.vectorize(to_binary_indicators=ordinal_features)\n",
    "tab_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187007a-7fc8-4c33-b60a-44cb05562c16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Temporal-specific processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e89fbb-fffe-462c-a1fd-f9f9ce79dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "all_top_events = []\n",
    "for i, events in enumerate(yield_dataframes(CLEANED_DIR, log=False)):\n",
    "    # Keep only the most popular events where the values are not null\n",
    "    top_events = (\n",
    "        events[EVENT_NAME][~events[EVENT_VALUE].isna()]\n",
    "        .value_counts()[:TOP_N_EVENTS]\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    all_top_events.append(top_events)\n",
    "\n",
    "    del events\n",
    "\n",
    "# Take only the events common to every file\n",
    "top_events = reduce(np.intersect1d, tuple(all_top_events))\n",
    "\n",
    "# Force include the target\n",
    "top_events = np.unique(np.append(top_events, OUTCOME_DEATH_PROCESSED))\n",
    "\n",
    "top_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44595c02-6526-4e6e-930d-53d2e4a3481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de07f3-12a3-492d-aa2b-5260480ff962",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator(\n",
    "    aggfuncs={EVENT_VALUE: MEAN},\n",
    "    timestamp_col=EVENT_TIMESTAMP,\n",
    "    time_by=ENCOUNTER_ID,\n",
    "    agg_by=[ENCOUNTER_ID, EVENT_NAME],\n",
    "    timestep_size=TIMESTEP_SIZE,\n",
    "    window_duration=WINDOW_DURATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa902f2-6213-4e7d-8ae6-b9c926aa232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data generator\n",
    "skip_n = 0\n",
    "generator = yield_dataframes(CLEANED_DIR, skip_n=skip_n, log=False)\n",
    "\n",
    "for save_count, events in enumerate(generator):\n",
    "    # Take only the top events\n",
    "    events = events[events[EVENT_NAME].isin(top_events)]\n",
    "\n",
    "    # Aggregate\n",
    "    events = events.reset_index(drop=True)\n",
    "    tmp_features = TemporalFeatures(\n",
    "        events,\n",
    "        features=EVENT_VALUE,\n",
    "        by=[ENCOUNTER_ID, EVENT_NAME],\n",
    "        timestamp_col=EVENT_TIMESTAMP,\n",
    "        aggregator=aggregator,\n",
    "    )\n",
    "\n",
    "    aggregated = tmp_features.aggregate()\n",
    "\n",
    "    save_dataframe(\n",
    "        aggregated, join(AGGREGATED_DIR, \"batch_\" + f\"{save_count + skip_n:04d}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04dfbd-09bf-4f95-be5c-365316989cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data generator\n",
    "skip_n = 0\n",
    "generator = yield_dataframes(AGGREGATED_DIR, skip_n=skip_n, log=False)\n",
    "\n",
    "for save_count, aggregated in enumerate(generator):\n",
    "    vec = aggregator.vectorize(aggregated)\n",
    "    save_pickle(vec, join(VECTORIZED_DIR, \"batch_\" + f\"{save_count + skip_n:04d}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136fa3e-5df9-4ae0-8306-9e7a045c6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = list([vec for vec in yield_pickled_files(VECTORIZED_DIR)])\n",
    "encounter_axis = vecs[0].get_axis(ENCOUNTER_ID)\n",
    "res = np.concatenate([vec.data for vec in vecs], axis=encounter_axis)\n",
    "indexes = vecs[0].indexes\n",
    "indexes[encounter_axis] = np.concatenate([vec.indexes[encounter_axis] for vec in vecs])\n",
    "temp_vectorized = Vectorized(res, indexes, vecs[0].axis_names)\n",
    "del res\n",
    "save_pickle(temp_vectorized, TEMP_VECTORIZED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc24eed-f576-4057-9464-9a91b906ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa840bb6-043c-466d-8ff1-0418a3ccd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized.axis_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09885d43-5f94-4419-8f32-cebd9c9d32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process targets - this is not real imputation, just using the imputation functions\n",
    "# to do preprocessing\n",
    "for target in TEMP_TARGETS:\n",
    "    event_ind = temp_vectorized.get_index_map(EVENT_NAME)[target]\n",
    "    index_exp = vec_index_exp[:, :, event_ind]\n",
    "\n",
    "    # Forward fill target values, e.g., [nan, nan, 1., nan, nan] -> [nan, nan, 1, 1, 1]\n",
    "    temp_vectorized.impute_over_axis(TIMESTEP, np_ffill, index_exp=index_exp)\n",
    "\n",
    "    # Fill remaining values with 0, e.g., [nan, nan, 1, 1, 1] -> [0, 0, 1, 1, 1]\n",
    "    # or [nan, nan, nan, nan, nan] -> [0, 0, 0, 0, 0]\n",
    "    temp_vectorized.impute_over_axis(TIMESTEP, np_fill_null_zero, index_exp=index_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103337f-dbd8-4f04-96a7-b6e721cd3642",
   "metadata": {},
   "source": [
    "# Combined processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b89e1c-a1d7-4dd1-b37c-6e78698e3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized = load_pickle(TEMP_VECTORIZED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7197544-6b93-4856-8959-c1a8228557db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tab_features.get_data(to_binary_indicators=ordinal_features).reset_index()\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6e85b-802d-42d5-98d2-c2800ec6088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the encounters with temporal events\n",
    "tab = tab[np.in1d(tab[ENCOUNTER_ID].values, temp_vectorized.get_index(ENCOUNTER_ID))]\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227ef62-7506-4ad6-b813-857cdc4dfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_aggregated = tabular_as_aggregated(\n",
    "    tab=tab,\n",
    "    index=ENCOUNTER_ID,\n",
    "    var_name=EVENT_NAME,\n",
    "    value_name=EVENT_VALUE,\n",
    "    strategy=ALL,\n",
    "    num_timesteps=aggregator.window_duration // aggregator.timestep_size,\n",
    ")\n",
    "tab_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c547a8b-5ab5-4a85-8eb3-59aaa87f96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_aggregated_vec = aggregator.vectorize(tab_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b71923-7188-46f9-b6ba-98f69ee95f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_axis = temp_vectorized.get_axis(EVENT_NAME)\n",
    "res = np.concatenate([temp_vectorized.data, tab_aggregated_vec.data], axis=event_axis)\n",
    "indexes = [ind.copy() for ind in temp_vectorized.indexes]\n",
    "indexes[event_axis] = np.concatenate(\n",
    "    [temp_vectorized.indexes[event_axis], tab_aggregated_vec.indexes[event_axis]]\n",
    ")\n",
    "comb_vectorized = Vectorized(res, indexes, temp_vectorized.axis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64390c6-1d7b-478c-9ca5-17f363b3734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687002eb-4a93-44be-b78a-1f3c99df4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, save = comb_vectorized.split_out(EVENT_NAME, temp_vectorized.get_index(EVENT_NAME))\n",
    "assert np.array_equal(save.data, temp_vectorized.data, equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743a8e-b194-4b5f-84f5-f94403390fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, save = comb_vectorized.split_out(\n",
    "    EVENT_NAME, tab_aggregated_vec.get_index(EVENT_NAME)\n",
    ")\n",
    "assert np.array_equal(save.data, tab_aggregated_vec.data, equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfd482-0b8f-4c30-8d56-7e6b89c4f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include the tabular targets\n",
    "comb_vectorized, _ = comb_vectorized.split_out(EVENT_NAME, TAB_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e90186-de61-40ed-8018-a1c89f7efdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f266a-007a-4e50-b525-7c47fb475d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_vectorized.get_index(EVENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd21b0c-4296-4d92-8e0e-9306011b0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(tab_aggregated_vec.data).sum() / tab_aggregated_vec.data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aee9c3-8304-41e5-9a35-8885732fdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(temp_vectorized.data).sum() / temp_vectorized.data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecbad6-29c9-471b-a42c-28fa177e6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(comb_vectorized.data).sum() / comb_vectorized.data.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0a06e-d146-45af-89cd-a90964a6b2df",
   "metadata": {},
   "source": [
    "# Prepare splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0eaa25-0014-4212-b2ae-794cd70e915d",
   "metadata": {},
   "source": [
    "Take only the encounters available in all of the datasets and align the datasets over encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8670c0b-3e53-45f3-9aa1-428a14d13373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_vectorized.shape, temp_vectorized.shape, comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817d8ec-886b-494d-9d32-3e076f774d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_vectorized, temp_vectorized, comb_vectorized = intersect_vectorized(\n",
    "    [tab_vectorized, temp_vectorized, comb_vectorized], axes=ENCOUNTER_ID\n",
    ")\n",
    "tab_vectorized.shape, temp_vectorized.shape, comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c5aba-847e-4f58-9bce-69222f538f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize only numeric features (e.g., not binary indicators)\n",
    "# Note: Normalization is not occuring, we are only preparing the object\n",
    "normalizer_map = {feat: STANDARD for feat in numeric_features}\n",
    "\n",
    "tab_vectorized.add_normalizer(\n",
    "    FEATURES,\n",
    "    normalizer_map=normalizer_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bb4b0-8948-42d6-a477-b0792a9b7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all events\n",
    "# Note: Normalization is not occuring, we are only preparing the object\n",
    "temp_vectorized.add_normalizer(\n",
    "    EVENT_NAME,\n",
    "    normalization_method=STANDARD,\n",
    ")\n",
    "\n",
    "comb_vectorized.add_normalizer(\n",
    "    EVENT_NAME,\n",
    "    normalization_method=STANDARD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6663ae-1ed5-445e-a1d6-9021154fe1a7",
   "metadata": {},
   "source": [
    "## Dataset splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839995aa-40cb-449c-a057-ad7719998ecf",
   "metadata": {},
   "source": [
    "Split into training, validation, and testing datasets such that the tabular and temporal encounters remain aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ec3d2-a221-41ce-93ae-9f5a3018eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_splits, temp_splits, comb_splits = split_vectorized(\n",
    "    [tab_vectorized, temp_vectorized, comb_vectorized],\n",
    "    SPLIT_FRACTIONS,\n",
    "    axes=ENCOUNTER_ID,\n",
    ")\n",
    "tab_train, tab_val, tab_test = tab_splits\n",
    "temp_train, temp_val, temp_test = temp_splits\n",
    "comb_train, comb_val, comb_test = comb_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6a36d-499d-447e-93eb-3ac002c986c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train.shape, tab_val.shape, tab_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd710c-b21d-4dc4-b8e4-550235570768",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train.shape, temp_val.shape, temp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea0fb5-526a-4f42-af8d-d4ee9c9e60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_train.shape, comb_val.shape, comb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ec0ad-a35a-49df-8979-a32779c3ac13",
   "metadata": {},
   "source": [
    "## Split features/targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f20f3-fe51-41f5-be9f-c74d32519787",
   "metadata": {},
   "source": [
    "Split out the targets in the temporal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6b88f-8f3b-46d2-9d75-3fe8eec1745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train_X, tab_train_y = tab_train.split_out(FEATURES, TAB_TARGETS)\n",
    "tab_train_X.shape, tab_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a428e-fcab-4a49-b9c9-77fe5b56da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_val_X, tab_val_y = tab_val.split_out(FEATURES, TAB_TARGETS)\n",
    "tab_val_X.shape, tab_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adfca2-4347-475e-ad9d-aeb15a9f3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test_X, tab_test_y = tab_test.split_out(FEATURES, TAB_TARGETS)\n",
    "tab_test_X.shape, tab_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a251574-2635-42db-b42e-3d8b77143b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_X, temp_train_y = temp_train.split_out(EVENT_NAME, TEMP_TARGETS)\n",
    "temp_train_X.shape, temp_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54795d9d-c2a4-4dd4-9ff9-d3927a0339a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_val_X, temp_val_y = temp_val.split_out(EVENT_NAME, TEMP_TARGETS)\n",
    "temp_val_X.shape, temp_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857300c-e98e-4af3-89eb-4665e887c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_X, temp_test_y = temp_test.split_out(EVENT_NAME, TEMP_TARGETS)\n",
    "temp_test_X.shape, temp_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24732d0c-cd51-44ed-b84a-ecb7d11588d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_train_X, comb_train_y = comb_train.split_out(EVENT_NAME, TEMP_TARGETS)\n",
    "comb_train_X.shape, comb_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb708d63-2fff-4f76-814a-376eb6f06969",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_val_X, comb_val_y = comb_val.split_out(EVENT_NAME, TEMP_TARGETS)\n",
    "comb_val_X.shape, comb_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86adf65-7931-49ce-ab02-eae544758a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test_X, comb_test_y = comb_test.split_out(EVENT_NAME, TEMP_TARGETS)\n",
    "comb_test_X.shape, comb_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901b017-237b-4cb9-85a7-04172d7dc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(temp_vec):\n",
    "    # Forward fill then backward fill to get rid of each of the timestep nulls\n",
    "    temp_vec.impute_over_axis(TIMESTEP, np_ffill_bfill)\n",
    "\n",
    "    # Fill those all-null timesteps with feature mean\n",
    "    # (since forward and backward filling still leaves them all null)\n",
    "    axis = temp_vec.get_axis(EVENT_NAME)\n",
    "\n",
    "    for i in range(temp_vec.data.shape[axis]):\n",
    "        index_exp = vec_index_exp[:, :, i]\n",
    "        data_slice = temp_vec.data[index_exp]\n",
    "        mean = np.nanmean(data_slice)\n",
    "        func = lambda x: np_fill_null_num(x, mean)\n",
    "        temp_vec.impute_over_axis(TIMESTEP, func, index_exp=index_exp)\n",
    "\n",
    "    return temp_vec\n",
    "\n",
    "\n",
    "temp_train_X = impute(temp_train_X)\n",
    "temp_val_X = impute(temp_val_X)\n",
    "temp_test_X = impute(temp_test_X)\n",
    "\n",
    "comb_train_X = impute(comb_train_X)\n",
    "comb_val_X = impute(comb_val_X)\n",
    "comb_test_X = impute(comb_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45650c0-847e-4b13-974f-49f9b89cc5f3",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47896217-2e2c-449b-9ac9-b3ce08028f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = (\n",
    "    tab_train_X,\n",
    "    tab_val_X,\n",
    "    tab_test_X,\n",
    "    temp_train_X,\n",
    "    temp_val_X,\n",
    "    temp_test_X,\n",
    "    comb_train_X,\n",
    "    comb_val_X,\n",
    "    comb_test_X,\n",
    ")\n",
    "\n",
    "for split in splits:\n",
    "    split.fit_normalizer()\n",
    "    split.normalize()\n",
    "\n",
    "(\n",
    "    tab_train_X,\n",
    "    tab_val_X,\n",
    "    tab_test_X,\n",
    "    temp_train_X,\n",
    "    temp_val_X,\n",
    "    temp_test_X,\n",
    "    comb_train_X,\n",
    "    comb_val_X,\n",
    "    comb_test_X,\n",
    ") = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2c5b5-9979-4678-bead-ad42b547abfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88e2b1-373e-46c6-97ca-aabd2873fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "vectorized = [\n",
    "    (tab_train_X, \"tab_train_X\"),\n",
    "    (tab_train_y, \"tab_train_y\"),\n",
    "    (tab_val_X, \"tab_val_X\"),\n",
    "    (tab_val_y, \"tab_val_y\"),\n",
    "    (tab_test_X, \"tab_test_X\"),\n",
    "    (tab_test_y, \"tab_test_y\"),\n",
    "    (temp_train_X, \"temp_train_X\"),\n",
    "    (temp_train_y, \"temp_train_y\"),\n",
    "    (temp_val_X, \"temp_val_X\"),\n",
    "    (temp_val_y, \"temp_val_y\"),\n",
    "    (temp_test_X, \"temp_test_X\"),\n",
    "    (temp_test_y, \"temp_test_y\"),\n",
    "    (comb_train_X, \"comb_train_X\"),\n",
    "    (comb_train_y, \"comb_train_y\"),\n",
    "    (comb_val_X, \"comb_val_X\"),\n",
    "    (comb_val_y, \"comb_val_y\"),\n",
    "    (comb_test_X, \"comb_test_X\"),\n",
    "    (comb_test_y, \"comb_test_y\"),\n",
    "]\n",
    "for vec, name in vectorized:\n",
    "    save_pickle(vec, TAB_VEC_COMB + name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a64e9-e654-44ad-9351-2f41b3fa6c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops_env",
   "language": "python",
   "name": "cyclops_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
