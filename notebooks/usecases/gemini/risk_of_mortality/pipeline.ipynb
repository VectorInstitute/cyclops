{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5117a0d-9d1f-495e-93d5-edb057cf4c0a",
   "metadata": {},
   "source": [
    "## Data pipeline for predicting risk of mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb2625-e38f-428a-bcfd-6299b405d0f4",
   "metadata": {},
   "source": [
    "## Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.feature_handler import FeatureHandler\n",
    "from cyclops.plotter import plot_timeline, set_bars_color, setup_plot\n",
    "from cyclops.processor import run_data_pipeline\n",
    "from cyclops.processors.aggregate import Aggregator\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    AGE,\n",
    "    DIAGNOSIS_CODE,\n",
    "    DISCHARGE_DISPOSITION,\n",
    "    DISCHARGE_TIMESTAMP,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_CATEGORY,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    HOSPITAL_ID,\n",
    "    LENGTH_OF_STAY_IN_ER,\n",
    "    RESTRICT_TIMESTAMP,\n",
    "    SEX,\n",
    "    TIMESTEP,\n",
    "    TRIAGE_LEVEL,\n",
    "    WINDOW_START_TIMESTAMP,\n",
    ")\n",
    "from cyclops.processors.constants import SMH\n",
    "from cyclops.processors.events import (\n",
    "    combine_events,\n",
    "    convert_to_events,\n",
    "    normalize_events,\n",
    ")\n",
    "from cyclops.processors.impute import Imputer\n",
    "from cyclops.processors.statics import compute_statics\n",
    "from cyclops.processors.util import (\n",
    "    create_indicator_variables,\n",
    "    fill_missing_timesteps,\n",
    "    gather_columns,\n",
    "    pivot_aggregated_events_to_features,\n",
    ")\n",
    "from cyclops.query import gemini\n",
    "from cyclops.utils.file import load_dataframe, save_dataframe\n",
    "\n",
    "MORTALITY = \"mortality\"\n",
    "LOS = \"los\"\n",
    "BASE_DATA_PATH = \"/mnt/nfs/project/delirium/drift_exp/_extract_v2\"\n",
    "AGGREGATION_WINDOW = 144\n",
    "AGGREGATION_BUCKET_SIZE = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8091448-5301-4631-a7d4-150667441e77",
   "metadata": {},
   "source": [
    "## Run query, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9efab5-8eb8-45fc-a53d-e5bfcc72ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(BASE_DATA_PATH, exist_ok=True)\n",
    "# er_admin_table = gemini.get_table(gemini.ER_ADMIN)\n",
    "# encounters = gemini.patient_encounters(\n",
    "#     er_admin_table=er_admin_table,\n",
    "#     years=[2018, 2019, 2020],\n",
    "#     died=True,\n",
    "#     died_binarize_col=\"mortality\",\n",
    "# )\n",
    "# encounters_labs = gemini.events(\n",
    "#     patient_encounters_table=encounters.query, event_category=\"lab\"\n",
    "# )\n",
    "# encounters_vitals = gemini.events(\n",
    "#     patient_encounters_table=encounters.query, event_category=\"vitals\"\n",
    "# )\n",
    "# imaging = gemini.imaging(years=[2018, 2019, 2020])\n",
    "# transfusions = gemini.blood_transfusions(years=[2018, 2019, 2020])\n",
    "# interventions = gemini.interventions(years=[2018, 2019, 2020])\n",
    "\n",
    "# encounters.run()\n",
    "# print(f\"{len(encounters.data)} rows extracted!\")\n",
    "\n",
    "# encounters_labs.run()\n",
    "# print(f\"{len(encounters_labs.data)} rows extracted!\")\n",
    "# encounters_labs.save(os.path.join(BASE_DATA_PATH, \"labs\"))\n",
    "# encounters_labs.clear_data()\n",
    "\n",
    "# encounters_vitals.run()\n",
    "# print(f\"{len(encounters_vitals.data)} rows extracted!\")\n",
    "# encounters_vitals.save(os.path.join(BASE_DATA_PATH, \"vitals\"))\n",
    "# encounters_vitals.clear_data()\n",
    "\n",
    "# imaging.run()\n",
    "# print(f\"{len(imaging.data)} rows extracted!\")\n",
    "# transfusions.run()\n",
    "# print(f\"{len(transfusions.data)} rows extracted!\")\n",
    "# interventions.run()\n",
    "# print(f\"{len(interventions.data)} rows extracted!\")\n",
    "\n",
    "# encounters_imaging = pd.merge(\n",
    "#     encounters.data, imaging.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    "# )\n",
    "# encounters_transfusions = pd.merge(\n",
    "#     encounters.data, transfusions.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    "# )\n",
    "# encounters_interventions = pd.merge(\n",
    "#     encounters.data, interventions.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    "# )\n",
    "\n",
    "# encounters.save(os.path.join(BASE_DATA_PATH, \"admin_er\"))\n",
    "# encounters_imaging.to_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "# encounters_transfusions.to_parquet(os.path.join(BASE_DATA_PATH, \"transfusions.parquet\"))\n",
    "# encounters_interventions.to_parquet(\n",
    "#     os.path.join(BASE_DATA_PATH, \"interventions.parquet\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde24be8-7759-4241-a24d-e63eaa9ce9ac",
   "metadata": {},
   "source": [
    "## Read saved query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a868e-11ab-432e-9d45-006f57d5a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"admin_er.parquet\"))\n",
    "labs_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"labs.parquet\"))\n",
    "# vitals_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"vitals.parquet\"))\n",
    "imaging_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "transfusions_data = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, \"transfusions.parquet\")\n",
    ")\n",
    "interventions_data = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, \"interventions.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0af1b6-1e8b-4554-944f-f13209d282cc",
   "metadata": {},
   "source": [
    "## Map imaging and transfusions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131da1-d7e8-4564-b1ff-d6e9e2251d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_data = imaging_data.rename(\n",
    "    columns={\n",
    "        \"imaging_test_description\": EVENT_NAME,\n",
    "        \"performed_date_time\": EVENT_TIMESTAMP,\n",
    "    }\n",
    ")\n",
    "imaging_data[EVENT_CATEGORY] = \"imaging\"\n",
    "imaging_data[EVENT_VALUE] = 1\n",
    "\n",
    "transfusions_data = transfusions_data.rename(\n",
    "    columns={\"issue_date_time\": EVENT_TIMESTAMP}\n",
    ")\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[\"rbc_mapped\"]\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[EVENT_NAME].apply(\n",
    "    lambda x: \"rbc\" if x else \"non-rbc\"\n",
    ")\n",
    "transfusions_data[EVENT_VALUE] = 1\n",
    "transfusions_data[EVENT_CATEGORY] = \"transfusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564db1ba-9f9d-4b39-a57a-90a357afab8f",
   "metadata": {},
   "source": [
    "##  Process interventions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34a21c-49ec-48a9-bfac-0cae8dce5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions_data[EVENT_VALUE] = 1\n",
    "interventions_data[EVENT_CATEGORY] = \"interventions\"\n",
    "\n",
    "binary_mapped_cols = [\n",
    "    \"endoscopy_mapped\",\n",
    "    \"gi_endoscopy_mapped\",\n",
    "    \"bronch_endoscopy_mapped\",\n",
    "    \"dialysis_mapped\",\n",
    "    \"inv_mech_vent_mapped\",\n",
    "    \"surgery_mapped\",\n",
    "]\n",
    "interventions_data[\"intervention_episode_start_time\"].loc[\n",
    "    interventions_data[\"intervention_episode_start_time\"].isna()\n",
    "] = \"12:00:00\"\n",
    "interventions_data[EVENT_TIMESTAMP] = pd.to_datetime(\n",
    "    interventions_data[\"intervention_episode_start_date\"].astype(str)\n",
    "    + \" \"\n",
    "    + interventions_data[\"intervention_episode_start_time\"].astype(str)\n",
    ")\n",
    "interventions_data[EVENT_TIMESTAMP] = interventions_data[EVENT_TIMESTAMP].astype(\n",
    "    \"datetime64[ns]\"\n",
    ")\n",
    "interventions_data[\"unmapped_intervention\"] = ~(\n",
    "    interventions_data[\"endoscopy_mapped\"]\n",
    "    | interventions_data[\"gi_endoscopy_mapped\"]\n",
    "    | interventions_data[\"bronch_endoscopy_mapped\"]\n",
    "    | interventions_data[\"dialysis_mapped\"]\n",
    "    | interventions_data[\"inv_mech_vent_mapped\"]\n",
    "    | interventions_data[\"surgery_mapped\"]\n",
    ")\n",
    "interventions_data[EVENT_NAME] = interventions_data[\n",
    "    binary_mapped_cols + [\"unmapped_intervention\"]\n",
    "].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e205-6491-442d-92a2-abc9a5c2a15d",
   "metadata": {},
   "source": [
    "## Filter out encounters that had less than 24 hours LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc8075-57e8-49ac-b6f8-124d5b3591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data[LOS] = (\n",
    "    encounters_data[DISCHARGE_TIMESTAMP]\n",
    "    - encounters_data[ADMIT_TIMESTAMP]\n",
    "    + pd.to_timedelta(encounters_data[LENGTH_OF_STAY_IN_ER], unit=\"h\")\n",
    ")\n",
    "encounters_data_atleast_los_24_hrs = encounters_data.loc[\n",
    "    encounters_data[LOS] >= pd.to_timedelta(24, unit=\"h\")\n",
    "]\n",
    "# encounters_data_atleast_los_24_hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72566d9f-db66-47f2-8820-fdd8904aafe1",
   "metadata": {},
   "source": [
    "## Get encounters that ended in mortality outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f04573-71c9-4f25-9a31-21df233e8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality = encounters_data_atleast_los_24_hrs.loc[\n",
    "    encounters_data_atleast_los_24_hrs[MORTALITY] == True\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae3659-4304-4697-808d-75c66d2fe6f3",
   "metadata": {},
   "source": [
    "## Get encounters that didn't end up in mortality outcome (limit to a subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d191738-2eb9-4cda-83cd-fe17a7fc72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_not_mortality = encounters_data_atleast_los_24_hrs.loc[\n",
    "    encounters_data_atleast_los_24_hrs[MORTALITY] == False\n",
    "]\n",
    "\n",
    "num_encounters_not_mortality = len(encounters_mortality)\n",
    "encounters_not_mortality_subset = encounters_not_mortality[\n",
    "    0:num_encounters_not_mortality\n",
    "]\n",
    "# encounters_not_mortality_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464ae6f-2fe4-419e-8afb-57c8ab974136",
   "metadata": {},
   "source": [
    "## Combine both subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d25c8-e4a1-4529-a9de-5b02bd37c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_train_val_test = pd.concat(\n",
    "    [encounters_mortality, encounters_not_mortality_subset], ignore_index=True\n",
    ")\n",
    "# encounters_train_val_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496ca76-0058-4223-8925-3c357ee153c2",
   "metadata": {},
   "source": [
    "## Offset discharge timestamp by K hours and create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c71da3-c4ec-4305-86a9-608dc6f556d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 168  # in hours\n",
    "encounters_mortality = encounters_mortality.copy()\n",
    "encounters_mortality[\"death_timestamp_offset\"] = encounters_mortality[\n",
    "    DISCHARGE_TIMESTAMP\n",
    "] + pd.Timedelta(-offset, unit=\"h\")\n",
    "# encounters_mortality[[DISCHARGE_TIMESTAMP, \"death_timestamp_offset\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569cdd0-1d93-4aba-a6fa-082282d95eb6",
   "metadata": {},
   "source": [
    "## Get encounters which result in death within timeframe (i.e. window + offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8036a-a108-49a9-9a0e-9cf7b818e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality_within_risk_timeframe = encounters_mortality.loc[\n",
    "    encounters_mortality[LOS] <= pd.to_timedelta(AGGREGATION_WINDOW + offset, unit=\"h\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6498e-f2a6-4383-89bd-6c9b5b4ee144",
   "metadata": {},
   "source": [
    "## Convert \"death_timestamp_offset\" to event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90081c16-ac41-4b60-ab48-7ff747630e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_risk_events = convert_to_events(\n",
    "    encounters_mortality_within_risk_timeframe,\n",
    "    event_name=f\"mortality_risk_{offset}\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=\"death_timestamp_offset\",\n",
    ")\n",
    "mortality_risk_events = pd.merge(\n",
    "    mortality_risk_events, encounters_mortality, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "mortality_risk_events = mortality_risk_events[\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        EVENT_NAME,\n",
    "        EVENT_TIMESTAMP,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        EVENT_VALUE,\n",
    "        EVENT_CATEGORY,\n",
    "    ]\n",
    "]\n",
    "mortality_risk_events[EVENT_VALUE] = 1\n",
    "# mortality_risk_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2653f-b1de-4e63-8531-b92677534041",
   "metadata": {},
   "source": [
    "## Get admission/discharge events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de93b44-b260-44d5-869f-80752cc913c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_events = convert_to_events(\n",
    "    encounters_train_val_test,\n",
    "    event_name=\"admission\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=\"admit_timestamp\",\n",
    ")\n",
    "disch_events = convert_to_events(\n",
    "    encounters_train_val_test,\n",
    "    event_name=\"discharge\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=\"discharge_timestamp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238709bd-47f6-4cf6-8562-09717824d57f",
   "metadata": {},
   "source": [
    "## Filter labs, vitals, imaging, transfusions, interventions to be in train_val_test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d7deb-f5bf-4a7a-a773-5768a68f7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_train_val_test = labs_data.loc[\n",
    "    labs_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "imaging_train_val_test = imaging_data.loc[\n",
    "    imaging_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "transfusions_train_val_test = transfusions_data.loc[\n",
    "    transfusions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "interventions_train_val_test = interventions_data.loc[\n",
    "    interventions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e4baa-d706-4b72-88e4-7416db744d7e",
   "metadata": {},
   "source": [
    "## Normalize all event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e249a41-d012-4f25-9241-66c15fad7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_events = normalize_events(imaging_train_val_test)\n",
    "transfusion_events = normalize_events(transfusions_train_val_test)\n",
    "lab_events = normalize_events(labs_train_val_test)\n",
    "mortality_risk_events = normalize_events(mortality_risk_events)\n",
    "intervention_events = normalize_events(interventions_train_val_test)\n",
    "admit_events = normalize_events(admit_events)\n",
    "disch_events = normalize_events(disch_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950cea0-8178-4303-865d-ceb47ac2b30d",
   "metadata": {},
   "source": [
    "## Combine different event data, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080940-2a0e-45ab-987a-ef7f200a7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_events = combine_events(\n",
    "    [\n",
    "        intervention_events,\n",
    "        imaging_events,\n",
    "        transfusion_events,\n",
    "        lab_events,\n",
    "        mortality_risk_events,\n",
    "        admit_events,\n",
    "        disch_events,\n",
    "    ]\n",
    ")\n",
    "save_dataframe(combined_events, os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692cd1a-e9ec-456d-9df2-3a9cfef6eeab",
   "metadata": {},
   "source": [
    "## Load combined events, aggregate them, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9306e6-90a0-4adb-8691-972424912f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e10d6-b372-45e6-b0b3-8aa45593ef38",
   "metadata": {},
   "source": [
    "## Aggregate combined events, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e02af-1c6e-4cec-8b84-52e2cf24c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator(bucket_size=AGGREGATION_BUCKET_SIZE, window=AGGREGATION_WINDOW)\n",
    "aggregated_events = aggregator(combined_events)\n",
    "save_dataframe(aggregated_events, os.path.join(BASE_DATA_PATH, \"aggregated_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc55f-d3f2-41c6-a367-c8a817b424d8",
   "metadata": {},
   "source": [
    "## Load aggregated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab947b7d-b489-4ce6-be32-546a8d0ae6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"aggregated_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7170d3-18b9-4e45-b2af-62f1eb50e829",
   "metadata": {},
   "source": [
    "## Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c35e0-ef34-4951-8311-3b01602ae19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "num_timesteps = int(AGGREGATION_WINDOW / AGGREGATION_BUCKET_SIZE)\n",
    "encounter_ids = list(encounters_train_val_test[ENCOUNTER_ID])\n",
    "encounter_ids_idx = np.arange(len(encounter_ids))\n",
    "num_encounters = len(encounter_ids)\n",
    "labels = np.zeros((num_encounters, num_timesteps))\n",
    "\n",
    "labels[\n",
    "    [\n",
    "        encounter_ids.index(enc_id)\n",
    "        for enc_id in list(encounters_mortality_within_risk_timeframe[ENCOUNTER_ID])\n",
    "    ]\n",
    "] = 1\n",
    "print(len(encounters_mortality_within_risk_timeframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de68a9-434b-44a1-bbb2-3b9cb1759f91",
   "metadata": {},
   "source": [
    "## Pivot table to get column that can be used to create labels. 1 corresponds to timestep where death happens (shifted by offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84f2b5-09a9-49ef-967f-42496665d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_mortality_risk = aggregated_events.loc[\n",
    "    aggregated_events[EVENT_NAME] == f\"mortality_risk_{offset}\"\n",
    "]\n",
    "missed = set(encounters_mortality_within_risk_timeframe[ENCOUNTER_ID]) - set(\n",
    "    aggregated_mortality_risk[ENCOUNTER_ID]\n",
    ")\n",
    "missed\n",
    "\n",
    "# timesteps_mortality_risk = aggregated_mortality_risk[\"timestep\"]\n",
    "# timesteps_mortality_counts = timesteps_mortality_risk.value_counts()\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 5), tight_layout=True)\n",
    "# ts_vals = plt.bar(\n",
    "#     list(np.unique(timesteps_mortality_risk)), list(timesteps_mortality_counts), alpha=0.5\n",
    "# )\n",
    "# set_bars_color(ts_vals, \"r\")\n",
    "# axs.set_xticks(\n",
    "#     range(len(ts_vals)),\n",
    "#     list(np.unique(timesteps_mortality_risk)),\n",
    "#     rotation=\"vertical\",\n",
    "#     fontsize=20,\n",
    "# )\n",
    "# setup_plot(\n",
    "#     axs,\n",
    "#     \"Mortality encounter distribution over timesteps\",\n",
    "#     \"timestep\",\n",
    "#     \"Num. encounters that resulted in mortality in that timestep\",\n",
    "#     [\"Count\"],\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae17f56-f9c7-4bb9-82d4-586f1bce09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2267 + 3115, len(mortality_risk_events), 2267 + 3030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4793-a22b-4611-96b3-65e2bd0d3d3c",
   "metadata": {},
   "source": [
    "## Pivot aggregated events to get column-wise temporal features and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a50a78-9e33-4f71-b45c-a2ca21ad31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = pivot_aggregated_events_to_features(aggregated_events, np.mean)\n",
    "save_dataframe(temporal_features, os.path.join(BASE_DATA_PATH, \"temporal_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277e20e-c9c7-427c-b3a4-e6120b342581",
   "metadata": {},
   "source": [
    "## Add to feature handler, with indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf2259-d1e8-4d72-acb5-d4c1ebd4cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler = FeatureHandler()\n",
    "\n",
    "temporal_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"temporal_features\"))\n",
    "# indicators = create_indicator_variables(temporal_features)\n",
    "feature_handler.add_features(temporal_features)\n",
    "# feature_handler.add_features(indicators)\n",
    "# feature_handler.drop_features([\"death_indicator\"])\n",
    "feature_handler.features[\"temporal\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725048d6-7442-48a3-ae76-e3d29280eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler.features[\"temporal\"][\"unmapped_intervention\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e89d8-a325-45f5-a38e-0f0ef6abe443",
   "metadata": {},
   "source": [
    "## Compute static features, save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c4497-03d3-4304-86d1-54b4eb5b5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality = gather_columns(\n",
    "    encounters_mortality,\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        AGE,\n",
    "        SEX,\n",
    "        HOSPITAL_ID,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        DISCHARGE_TIMESTAMP,\n",
    "        TRIAGE_LEVEL,\n",
    "    ],\n",
    ")\n",
    "static_features = compute_statics(encounters_mortality)\n",
    "save_dataframe(static_features, os.path.join(BASE_DATA_PATH, \"static_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b6950-850c-4a83-a568-45088411911a",
   "metadata": {},
   "source": [
    "##  Load static features, add to feature handler, save all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4759c9-bfd8-42ec-adeb-c518489caae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"static_features\"))\n",
    "feature_handler.add_features(\n",
    "    static_features, reference_cols=[HOSPITAL_ID, ADMIT_TIMESTAMP, DISCHARGE_TIMESTAMP]\n",
    ")\n",
    "feature_handler.save(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028c06-7771-46db-a291-f7621471ef5e",
   "metadata": {},
   "source": [
    "## Create new feature handler, load saved features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836f236-1f0b-46e8-90b8-8e811adcbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler1 = FeatureHandler()\n",
    "feature_handler1.load(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da874af-fc43-4373-86b5-e3296893c27b",
   "metadata": {},
   "source": [
    "## Impute temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a57ca-2e72-4f17-8452-3c3e5f4d6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_simple(dataframe: pd.DataFrame, time_index: str = TIMESTEP) -> pd.DataFrame:\n",
    "    \"\"\"Impute features using 'Simple' method.\n",
    "\n",
    "    Concatenate the forward filled value, the mask of the measurement,\n",
    "    and the time of the last measurement.\n",
    "\n",
    "    Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu,\n",
    "    \"Recurrent Neural Networks for Multivariate Time Series with Missing Values,\"\n",
    "    Scientific Reports, vol. 8, no. 1, p. 6085, Apr 2018.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pandas.DataFrame\n",
    "        Temporal features dataframe.\n",
    "    time_index: str, optional\n",
    "        The name of the time-series index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "     Dataframe after applying imputation.\n",
    "\n",
    "    \"\"\"\n",
    "    # Mask missingness.\n",
    "    masked_df = pd.isna(dataframe)\n",
    "    masked_df = masked_df.apply(pd.to_numeric)\n",
    "\n",
    "    # Compute time since last measurement.\n",
    "    index_of_time = list(dataframe.index.names).index(time_index)\n",
    "    time_in = [item[index_of_time] for item in dataframe.index.tolist()]\n",
    "    time_df = dataframe.copy()\n",
    "    for col in time_df.columns.tolist():\n",
    "        time_df[col] = time_in\n",
    "    time_df[masked_df] = np.nan\n",
    "\n",
    "    # Concatenate the dataframes.\n",
    "    df_prime = pd.concat(\n",
    "        [dataframe, masked_df, time_df], axis=1, keys=[\"measurement\", \"mask\", \"time\"]\n",
    "    )\n",
    "    df_prime.columns = df_prime.columns.rename(\"impute_simple\", level=0)\n",
    "\n",
    "    # Fill each dataframe using either ffill or mean.\n",
    "    df_prime = df_prime.fillna(method=\"ffill\").unstack().fillna(0)\n",
    "\n",
    "    # Swap the levels so that the simple imputation feature is the lowest value.\n",
    "    col_level_names = list(df_prime.columns.names)\n",
    "    col_level_names.append(col_level_names.pop(0))\n",
    "\n",
    "    df_prime = df_prime.reorder_levels(col_level_names, axis=1)\n",
    "    df_prime.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    return df_prime\n",
    "\n",
    "\n",
    "def create_train_test_split(\n",
    "    encounters: pd.DataFrame,\n",
    "    fractions: Optional[List] = [0.8, 0.2],\n",
    "    split_column: Optional[str] = None,\n",
    "    split_values: List = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Split encounters into train/test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encounters: pandas.DataFrame\n",
    "        Dataframe with encounter IDs.\n",
    "    fractions: list, optional\n",
    "        Fraction of samples to use for train, test sets.\n",
    "    split_column: str, optional\n",
    "        If 'split_column' is specified, then that column is used to split.\n",
    "    split_values: list, optional\n",
    "        Along with 'split_column', a list of lists can be specified for filtering.\n",
    "        e.g. [[2008], [2009, 2010]] for train/test split based on year.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train IDs, test IDs)\n",
    "\n",
    "    \"\"\"\n",
    "    if split_column:\n",
    "        if split_column not in encounters.columns:\n",
    "            raise ValueError(\"Specified 'split column' not found in input dataframe\")\n",
    "        if not split_values:\n",
    "            raise ValueError(\"Specify train/test split values for the 'split column'.!\")\n",
    "        train_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[0])\n",
    "        ]\n",
    "        test_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[1])\n",
    "        ]\n",
    "        return train_encounters, test_encounters\n",
    "\n",
    "    encounter_ids = list(encounters[ENCOUNTER_ID].unique())\n",
    "    random.shuffle(encounter_ids)\n",
    "    num_train = int(fractions[0] * len(encounter_ids))\n",
    "\n",
    "    return encounter_ids[0:num_train], encounter_ids[num_train:]\n",
    "\n",
    "\n",
    "# By Year\n",
    "encounters_mortality[\"year\"] = encounters_mortality[\"admit_timestamp\"].dt.year\n",
    "train_ids, val_test_ids = create_train_test_split(\n",
    "    encounters_mortality, split_column=\"year\", split_values=[[2018, 2019], [2020]]\n",
    ")\n",
    "\n",
    "# By HOSPITAL_ID\n",
    "# train_ids, val_test_ids = create_train_test_split(encounters_mortality, split_column=HOSPITAL_ID, split_values=[[\"THPM\", \"SMH\", \"THPC\", \"SBK\", \"MSH\"], [\"UHNTW\", \"UHNTG\"]])\n",
    "\n",
    "# Random\n",
    "# train_ids, val_test_ids = create_train_test_split(encounters_mortality, split_column=HOSPITAL_ID, split_values=[[\"THPM\", \"SMH\", \"THPC\"], [\"UHNTW\", \"UHNTG\", \"MSH\"]])\n",
    "\n",
    "val_ids, test_ids = create_train_test_split(\n",
    "    encounters_mortality.loc[encounters_mortality[ENCOUNTER_ID].isin(val_test_ids)],\n",
    "    [0.5, 0.5],\n",
    ")\n",
    "print(\n",
    "    f\"Train set: {len(train_ids)}, Val set: {len(val_ids)}, Test set: {len(test_ids)}\"\n",
    ")\n",
    "encounters_mortality[HOSPITAL_ID].value_counts(), encounters_mortality[\n",
    "    \"year\"\n",
    "].value_counts()\n",
    "\n",
    "static = feature_handler1.features[\"static\"]\n",
    "temporal = feature_handler1.features[\"temporal\"]\n",
    "\n",
    "mortality_risk_24_targets = temporal[\"death\"].copy()\n",
    "temporal = temporal.drop(columns=\"death\")\n",
    "\n",
    "X = temporal[\n",
    "    np.in1d(temporal.index.get_level_values(0), static.index.get_level_values(0))\n",
    "]\n",
    "y = mortality_risk_24_targets[\n",
    "    np.in1d(\n",
    "        mortality_risk_24_targets.index.get_level_values(0), X.index.get_level_values(0)\n",
    "    )\n",
    "]\n",
    "\n",
    "y_train, y_val, y_test = [\n",
    "    y[np.in1d(y.index.get_level_values(0), ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "X_train, X_val, X_test = [\n",
    "    X[np.in1d(X.index.get_level_values(0), ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "\n",
    "len(X), len(X_train), len(X_val), len(X_test), len(y), len(y_train), len(y_val), len(\n",
    "    y_test\n",
    ")\n",
    "\n",
    "# X_train_inputs = impute_simple(X_train)\n",
    "# X_train_inputs = format_dataset(X_train, level=\"features\",imputation_method=\"simple\")\n",
    "# X_val = X[np.in1d(X.index.get_level_values(0), y_val.index.get_level_values(0))]\n",
    "# X_val_inputs = format_dataset(X_val, level=\"features\",imputation_method=\"simple\")\n",
    "# X_test = X[np.in1d(X.index.get_level_values(0), y_test.index.get_level_values(0))]\n",
    "# X_test_inputs = format_dataset(X_test, level=\"features\",imputation_method=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb97e-e050-43fb-9b60-26f204260706",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_id = random.choice(temporal.index.get_level_values(0))\n",
    "X_train_inputs.loc[encounter_id][\"white blood cell count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93e697-f310-4fe5-a651-537187359fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_id = random.choice(aggregated_mortality_risk[\"encounter_id\"].unique())\n",
    "encounter_id = 15576154\n",
    "\n",
    "# {12296817, 13651026, 14920341, 15576154}\n",
    "\n",
    "print(encounter_id)\n",
    "combined_events_encounter = combined_events.loc[\n",
    "    combined_events[\"encounter_id\"] == encounter_id\n",
    "]\n",
    "fig = plot_timeline(combined_events_encounter, return_fig=True)\n",
    "\n",
    "fig = fig.update_layout(width=2000, height=800)\n",
    "\n",
    "aggregator.meta[\"timestep_end_timestamp\"] = (\n",
    "    aggregator.meta[\"timestep_start_timestamp\"]\n",
    "    + pd.to_timedelta(AGGREGATION_BUCKET_SIZE, unit=\"h\")\n",
    ").rename(columns={\"timestep_start_timestamp\": \"timestep_end_timestamp\"})\n",
    "aggregator.meta[\"timestep_end_timestamp\"]\n",
    "ts_ends = aggregator.meta[\"timestep_end_timestamp\"].loc[encounter_id][\n",
    "    \"timestep_end_timestamp\"\n",
    "]\n",
    "for ts_end in ts_ends:\n",
    "    fig.add_vline(ts_end)\n",
    "fig.show()\n",
    "\n",
    "# combined_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8bbc1-d821-48e1-81d0-f501364f9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_events.loc[combined_events[ENCOUNTER_ID] == 15576154][EVENT_TIMESTAMP]\n",
    "# encounters_mortality.loc[encounters_mortality[ENCOUNTER_ID] == 15576154]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops",
   "language": "python",
   "name": "cyclops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
