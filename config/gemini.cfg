[Data Pipeline]
# Gemini database parameters, required if running extraction from database
# Use flag 'r' on command line to indicate 'read from database'
# By default user and password are read from environment 
# variables USER and PGPASSWORD respectively.
port = 5432
host = 'db.gemini-hpc.ca'
database = 'delirium_v3_0_0'

# If reading data from previously saved csv file
input = '/mnt/nfs/project/delirium/data/all_before_2018.csv'

# if write flag ('w') is specified, save extracted data to this folder with generated filename
# alternatively can specify output_full_path to indicate full path and filename
output_folder = '/mnt/nfs/project/delirium/data'

# dataset description - features/target
features = [sex,age,mort_hosp,readmission_7,readmission_28,palliative,los_er,icd10_A00_B99,icd10_C00_D49,icd10_D50_D89,icd10_E00_E89,icd10_F01_F99,icd10_G00_G99,icd10_H00_H59,icd10_H60_H95,icd10_I00_I99,icd10_J00_J99,icd10_K00_K95,icd10_L00_L99,icd10_M00_M99,icd10_N00_N99,icd10_O00_O99,icd10_Q00_Q99,icd10_R00_R99,icd10_S00_T88,icd10_U07_U08,icd10_Z00_Z99,icd10_nan]
target = los

# dataset stats file path (during training save train features mean/std, load on inference)
stats_path = '/mnt/nfs/project/delirium/data/all_before_2018_stats.pkl'

# indicates column for train/val/test slicing. Currently tested ('year' and 'hospital_id')
# refer to config.py for more data extraction options (such as extracting only for a specific time period)
split_column = 'year'
test_split = 2018
val_split = 2017

# Total number of records to read from the database (0 - to read all)'
pop_size = 10000

[Train][Predict]
# By default training on gemini data starts on the data loaded from 'input' file above.
model = "mlp"
model_path = "./model.pt" #used for saving when training and for reading saved model on inference
dataset = "gemini"
batch_size = 64
num_workers = 0
num_epochs = 10
lr=3e-4

# classification threshold for prediction
threshold=0.5
# output of prediction is saved to this file (includes original data as well)
result_output = "../result.csv"

[Analysis]
# type of analysis to perform ('dataset' for dataset drift, 'performance' for classification model comparison)
type = 'dataset'

# Two ways to read data for dataset drift analysis:
# 1) if 'slice' column is specified then we read from 'input' file, and slice by data_ref, data_eval values
# 2) if 'slice' is not specified, read two input files 'referece', 'test'

# Model performance report requires 'referece' and 'test' input files containing prediction_col
reference = '../gemini_val_result.csv'
test = '../gemini_test_result.csv'

# column mapping for evidenlty analysis
numerical_features = [age, los_er]
categorical_features = [hospital_id,sex,mort_hosp,readmission_7,readmission_28,palliative,icd10_A00_B99,icd10_C00_D49,icd10_D50_D89,icd10_E00_E89,icd10_F01_F99,icd10_G00_G99,icd10_H00_H59,icd10_H60_H95,icd10_I00_I99,icd10_J00_J99,icd10_K00_K95,icd10_L00_L99,icd10_M00_M99,icd10_N00_N99,icd10_O00_O99,icd10_Q00_Q99,icd10_R00_R99,icd10_S00_T88,icd10_Z00_Z99]
prediction_col = 'prediction'
# also uses 'target' parameter defined above

# Path for auto-generated reports, specify 'report_full_path' for custom report names
report_path = '../'

# Generate HMTL report, otherwise it's JSON
# html
