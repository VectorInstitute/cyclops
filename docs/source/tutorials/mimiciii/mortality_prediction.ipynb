{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-hospital Mortality Prediction\n",
    "\n",
    "This notebook showcases in-hospital mortality prediction due to heart failure on a subset of the MIMIC-III dataset using CyclOps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datasets import Dataset\n",
    "from datasets.features import ClassLabel\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from cyclops.data.slicer import SliceSpec\n",
    "from cyclops.evaluate.fairness import FairnessConfig  # noqa: E402\n",
    "from cyclops.evaluate.metrics import MetricCollection, create_metric\n",
    "from cyclops.models.catalog import create_model\n",
    "from cyclops.process.feature.feature import TabularFeatures\n",
    "from cyclops.report import ModelCardReport\n",
    "from cyclops.report.plot.classification import ClassificationPlotter\n",
    "from cyclops.tasks.mortality_prediction import MortalityPredictionTask\n",
    "from cyclops.utils.file import join, load_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CyclOps offers a package for documentation of the model through a model card. The `ModelCardReport` class is used to populate and generate the model card as an HTML file. The model card has the following sections:\n",
    "- Model Details: This section contains descriptive metadata about the model such as the owners, version, license, etc.\n",
    "- Model Parameters: This section contains the technical details of the model such as the model architecture, training parameters, etc.\n",
    "- Considerations: This section contains descriptions of the considerations involved in developing and using the model such as the intended use, limitations, etc.\n",
    "- Quantitative Analysis: This section contains the performance metrics of the model for different sets of the data and subpopulations.\n",
    "- Explainaibility Analysis: This section contains the explainability metrics of the model.\n",
    "- Fairness Analysis: This section contains the fairness metrics of the model.\n",
    "\n",
    "We will use this to document the model development process as we go along and generate the model card at the end.\n",
    "\n",
    "```{note}\n",
    "The model card tool is a work in progress and is subject to change.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ModelCardReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "RANDOM_SEED = 85\n",
    "NAN_THRESHOLD = 0.75\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Before starting, make sure to install the Kaggle API by running `pip install kaggle`. To use the Kaggle API, you need to sign up for a Kaggle account at https://www.kaggle.com. Then go to the 'Account' tab of your user profile (`https://www.kaggle.com/<username>/account`) and select 'Create API Token'. This will trigger the download of kaggle.json, a file containing your API credentials. Place this file in the location `~/.kaggle/kaggle.json` on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files(\n",
    "    \"saurabhshahane/in-hospital-mortality-prediction\", path=DATA_DIR, unzip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_dataframe(join(DATA_DIR, \"data01.csv\"), file_format=\"csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection and Preprocessing\n",
    "\n",
    "#### Drop NaNs based on the `NAN_THRESHOLD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "fig = go.Figure(data=[go.Bar(x=null_counts.index, y=null_counts.values)])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Number of Null Values per Column\",\n",
    "    xaxis_title=\"Columns\",\n",
    "    yaxis_title=\"Number of Null Values\",\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresh_nan = int(NAN_THRESHOLD * len(df))\n",
    "df = df.dropna(axis=1, thresh=thresh_nan)\n",
    "df = df.dropna(axis=0, subset=[\"outcome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female: gender = 1\n",
    "# Male: gender = 0\n",
    "df = df.rename(columns={\"gendera\": \"gender\"})\n",
    "df[\"gender\"] = df[\"gender\"].replace({1: 0, 2: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.pie(df, names=\"gender\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Gender Distribution\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"age\")\n",
    "fig.update_layout(\n",
    "    title=\"Age Distribution\",\n",
    "    xaxis_title=\"Age\",\n",
    "    yaxis_title=\"Count\",\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"outcome\"] = df[\"outcome\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.pie(df, names=\"outcome\")\n",
    "fig.update_traces(textinfo=\"percent+label\")\n",
    "fig.update_layout(title_text=\"Outcome Distribution\")\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"Outcome: %{label}<br>Count: %{value}<br>Percent: %{percent}\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_counts = df[\"outcome\"].value_counts()\n",
    "class_ratio = class_counts[0] / class_counts[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the features in the dataset, we select 20 of them which was reported by [Li et al.](https://pubmed.ncbi.nlm.nih.gov/34301649/)  to be the most important features in this classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_list = [\n",
    "    \"Anion gap\",\n",
    "    \"Lactic acid\",\n",
    "    \"Blood calcium\",\n",
    "    \"Lymphocyte\",\n",
    "    \"Leucocyte\",\n",
    "    \"heart rate\",\n",
    "    \"Blood sodium\",\n",
    "    \"Urine output\",\n",
    "    \"Platelets\",\n",
    "    \"Urea nitrogen\",\n",
    "    \"age\",\n",
    "    \"MCH\",\n",
    "    \"RBC\",\n",
    "    \"Creatine kinase\",\n",
    "    \"PCO2\",\n",
    "    \"Blood potassium\",\n",
    "    \"Diastolic blood pressure\",\n",
    "    \"Respiratory rate\",\n",
    "    \"Renal failure\",\n",
    "    \"NT-proBNP\",\n",
    "]\n",
    "features_list = sorted(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying feature types\n",
    "\n",
    "Cyclops `TabularFeatures` class helps to identify feature types, an essential step before preprocessing the data. Understanding feature types (numerical/categorical/binary) allows us to apply appropriate preprocessing steps for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tab_features = TabularFeatures(\n",
    "    data=df.reset_index(),\n",
    "    features=features_list,\n",
    "    by=\"ID\",\n",
    "    targets=\"outcome\",\n",
    ")\n",
    "tab_features.types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data preprocessors\n",
    "\n",
    "We create a data preprocessor using sklearn's ColumnTransformer. This helps in applying different preprocessing steps to different columns in the dataframe. For instance, binary features might be processed differently from numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "binary_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_features = sorted((tab_features.features_by_type(\"numeric\")))\n",
    "numeric_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in numeric_features\n",
    "]\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_features = sorted(tab_features.features_by_type(\"binary\"))\n",
    "binary_features.remove(\"outcome\")\n",
    "binary_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in binary_features\n",
    "]\n",
    "binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_indices),\n",
    "        (\"bin\", binary_transformer, binary_indices),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's document the dataset in the model card. This can be done using the `log_dataset` method, which takes the following arguments:\n",
    "- description: A description of the dataset.\n",
    "- citation: The citation for the dataset.\n",
    "- link: A link to a resource for the dataset.\n",
    "- license_id: The SPDX license identifier for the dataset.\n",
    "- version: The version of the dataset.\n",
    "- features: A list of features in the dataset.\n",
    "- split: The split of the dataset (train, test, validation, etc.).\n",
    "- sensitive_features: A list of sensitive features used to train/evaluate the model.\n",
    "- sensitive_feature_justification: A justification for the sensitive features used to train/evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_dataset(\n",
    "    description=\"MIMIC-III is a large, freely-available database comprising of \\\n",
    "        deidentified health-related data associated with over forty thousand \\\n",
    "        patients who stayed in the ICU of the Beth Israel Deaconess Medical Center \\\n",
    "        between 2001 and 2012. The data includes vital sign measurements, medications, \\\n",
    "        laboratory measurements, imaging reports, and more.\",\n",
    "    citation=\"Zhou, Jingmin et al. (2021), Prediction model of in-hospital mortality \\\n",
    "        in intensive care unit patients with heart failure: machine learning-based, \\\n",
    "        retrospective analysis of the MIMIC-III database, Dryad, Dataset, \\\n",
    "        https://doi.org/10.5061/dryad.0p2ngf1zd\",\n",
    "    link=\"\"\"\n",
    "    https://www.kaggle.com/datasets/saurabhshahane/in-hospital-mortality-prediction\n",
    "    \"\"\",\n",
    "    license_id=\"CC0-1.0\",\n",
    "    version=\"Version 1\",\n",
    "    features=features_list,\n",
    "    sensitive_attributes=[\"gender\", \"age\"],\n",
    "    sensitive_feature_justification=\"Demographic information like age and gender \\\n",
    "        often have a strong correlation with health outcomes. For example, older \\\n",
    "        patients are more likely to have a higher risk of mortality.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Hugging Face Dataset\n",
    "\n",
    "We convert our processed Pandas dataframe into a Hugging Face dataset, a powerful and easy-to-use data format which is also compatible with CyclOps models and evaluator modules. The dataset is then split to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.cleanup_cache_files()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"outcome\", ClassLabel(num_classes=2))\n",
    "dataset = dataset.train_test_split(\n",
    "    train_size=TRAIN_SIZE, stratify_by_column=\"outcome\", seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "CyclOps model registry allows for straightforward creation and selection of models. This registry maintains a list of pre-configured models, which can be instantiated with a single line of code. Here we use a SGD classifier to fit a logisitic regression model. The model configurations can be passed to `create_model` based on the sllearn parameters for SGDClassifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"sgd_classifier\"\n",
    "model = create_model(model_name, random_state=123, verbose=0, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Creation\n",
    "\n",
    "We use Cyclops tasks to define our model's task (in this case, MortalityPrediction), train the model, make predictions, and evaluate performance. Cyclops task classes encapsulate the entire ML pipeline into a single, cohesive structure, making the process smooth and easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mortality_task = MortalityPredictionTask(\n",
    "    {model_name: model}, task_features=features_list, task_target=\"outcome\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mortality_task.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `best_model_params` is passed to the `train` method, the best model will be selected after the hyperparameter search. The parameters in `best_model_params` indicate the values to create the parameters grid.\n",
    "\n",
    "Note that the data preprocessor needs to be passed to the tasks methods if the Hugging Face dataset is not already preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_params = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"metric\": \"roc_auc\",\n",
    "    \"method\": \"grid\",\n",
    "}\n",
    "\n",
    "mortality_task.train(\n",
    "    dataset[\"train\"],\n",
    "    model_name=model_name,\n",
    "    transforms=preprocessor,\n",
    "    best_model_params=best_model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add model parameters to the model card using the `log_model_parameters` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_model_parameters(\n",
    "    params=mortality_task.list_models_params()[model_name],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_task.list_models_params()[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction output can be either the whole Hugging Face dataset with the prediction columns added to it or the single column containing the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = mortality_task.predict(\n",
    "    dataset[\"test\"],\n",
    "    model_name=model_name,\n",
    "    transforms=preprocessor,\n",
    "    proba=False,\n",
    "    only_predictions=True,\n",
    ")\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is done using various evaluation metrics that provide different perspectives on the model's predictive abilities i.e. standard performance metrics and fairness metrics.\n",
    "\n",
    "The standard performance metrics can be created using the `MetricCollection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_names = [\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"f1_score\",\n",
    "    \"auroc\",\n",
    "    \"roc_curve\",\n",
    "    \"precision_recall_curve\",\n",
    "]\n",
    "metrics = [create_metric(metric_name, task=\"binary\") for metric_name in metric_names]\n",
    "metric_collection = MetricCollection(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to overall metrics, it might be interesting to see how the model performs on certain subpopulations. We can define these subpopulations using `SliceSpec` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_list = [\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"min_value\": 30,\n",
    "            \"max_value\": 50,\n",
    "            \"min_inclusive\": True,\n",
    "            \"max_inclusive\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"min_value\": 50,\n",
    "            \"max_value\": 80,\n",
    "            \"min_inclusive\": True,\n",
    "            \"max_inclusive\": False,\n",
    "        }\n",
    "    },\n",
    "    {\"gender\": {\"value\": 1}},\n",
    "    {\"gender\": {\"value\": 0}},\n",
    "]\n",
    "slice_spec = SliceSpec(spec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `MetricCollection` can also be defined for the fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "specificity = create_metric(\n",
    "    metric_name=\"specificity\",\n",
    "    task=\"binary\",\n",
    ")\n",
    "sensitivity = create_metric(\n",
    "    metric_name=\"sensitivity\",\n",
    "    task=\"binary\",\n",
    ")\n",
    "\n",
    "fpr = 1 - specificity\n",
    "fnr = 1 - sensitivity\n",
    "\n",
    "ber = (fpr + fnr) / 2\n",
    "\n",
    "fairness_metric_collection = MetricCollection(\n",
    "    {\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"BER\": ber,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FairnessConfig helps in setting up and evaluating the fairness of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fairness_config = FairnessConfig(\n",
    "    metrics=fairness_metric_collection,\n",
    "    dataset=None,  # dataset is passed from the evaluator\n",
    "    target_columns=None,  # target columns are passed from the evaluator\n",
    "    groups=[\"gender\", \"age\"],\n",
    "    group_bins={\"age\": [50, 70]},\n",
    "    group_base_values={\"age\": 20, \"gender\": 0},\n",
    "    thresholds=[0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate methods outputs the evaluation results and the Hugging Face dataset with the predictions added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results, dataset_with_preds = mortality_task.evaluate(\n",
    "    dataset[\"test\"],\n",
    "    metric_collection,\n",
    "    model_names=model_name,\n",
    "    transforms=preprocessor,\n",
    "    prediction_column_prefix=\"preds\",\n",
    "    slice_spec=slice_spec,\n",
    "    batch_size=64,\n",
    "    fairness_config=fairness_config,\n",
    "    override_fairness_metrics=False,\n",
    ")\n",
    "dataset_with_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[model_name].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[model_name][\"overall\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[\"fairness\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"fairness\"][\"gender:0&age:(-inf - 50.0]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a performance metric to the model card using the `log_performance_metric` method, which expects a dictionary where the keys are in the following format: `slice_name/metric_name`. For instance, `overall/accuracy`. \n",
    "\n",
    "We first need to process the evaluation results to get the metrics in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the results to follow the model card schema\n",
    "results_flat = {}\n",
    "for name, model_results in results.items():\n",
    "    results_flat[name] = {}\n",
    "    for slice_name, slice_results in model_results.items():\n",
    "        for metric_name, metric_value in slice_results.items():\n",
    "            # remove the curve data\n",
    "            if metric_name not in [\"BinaryROCCurve\", \"BinaryPrecisionRecallCurve\"]:\n",
    "                results_flat[name][f\"{slice_name}/{metric_name}\"] = metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_performance_metrics(results_flat[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the `ClassificationPlotter` to plot the performance metrics and the add the figure to the model card using the `log_plotly_figure` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = ClassificationPlotter(task_type=\"binary\", class_names=[\"0\", \"1\"])\n",
    "plotter.set_template(\"plotly_white\")\n",
    "plotter.set_colorway(\n",
    "    [\n",
    "        \"#006ba6\",\n",
    "        \"#ffbc42\",\n",
    "        \"#0496ff\",\n",
    "        \"#d81159\",\n",
    "        \"#8f2d56\",\n",
    "        \"#75dddd\",\n",
    "        \"#508991\",\n",
    "        \"#172a3a\",\n",
    "        \"#004346\",\n",
    "        \"#09bc8a\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_plotly_figure(\n",
    "    fig=fig,\n",
    "    alt=\"ROC Curve for Female Patients\",\n",
    "    section_name=\"quantitative analysis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the ROC curves for all the slices\n",
    "roc_curves = {\n",
    "    slice_name: slice_results[\"BinaryROCCurve\"]\n",
    "    for slice_name, slice_results in results[model_name].items()\n",
    "}\n",
    "aurocs = {\n",
    "    slice_name: slice_results[\"BinaryAUROC\"]\n",
    "    for slice_name, slice_results in results[model_name].items()\n",
    "}\n",
    "roc_plot = plotter.roc_curve_comparison(roc_curves, aurocs=aurocs)\n",
    "report.log_plotly_figure(\n",
    "    fig=roc_plot,\n",
    "    alt=\"ROC Curve for Female Patients\",\n",
    "    section_name=\"quantitative analysis\",\n",
    ")\n",
    "roc_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the overall classification metric values.\n",
    "overall_performance = {\n",
    "    metric_name: metric_value\n",
    "    for metric_name, metric_value in results[model_name][\"overall\"].items()\n",
    "    if metric_name not in [\"BinaryROCCurve\", \"BinaryPrecisionRecallCurve\"]\n",
    "}\n",
    "overall_performance_plot = plotter.metrics_value(\n",
    "    overall_performance, title=\"Overall Performance\"\n",
    ")\n",
    "report.log_plotly_figure(\n",
    "    fig=overall_performance_plot,\n",
    "    alt=\"Overall Performance\",\n",
    "    section_name=\"quantitative analysis\",\n",
    ")\n",
    "overall_performance_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metric values for all the slices.\n",
    "slice_metrics = {\n",
    "    slice_name: {\n",
    "        metric_name: metric_value\n",
    "        for metric_name, metric_value in slice_results.items()\n",
    "        if metric_name not in [\"BinaryROCCurve\", \"BinaryPrecisionRecallCurve\"]\n",
    "    }\n",
    "    for slice_name, slice_results in results[model_name].items()\n",
    "}\n",
    "slice_metrics_plot = plotter.metrics_comparison_bar(slice_metrics)\n",
    "report.log_plotly_figure(\n",
    "    fig=slice_metrics_plot,\n",
    "    alt=\"Slice Metric Comparison\",\n",
    "    section_name=\"quantitative analysis\",\n",
    ")\n",
    "slice_metrics_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the fairness metrics\n",
    "fairness_results = copy.deepcopy(results[\"fairness\"])\n",
    "fairness_metrics = {}\n",
    "# remove the group size from the fairness results and add it to the slice name\n",
    "for slice_name, slice_results in fairness_results.items():\n",
    "    group_size = slice_results.pop(\"Group Size\")\n",
    "    fairness_metrics[f\"{slice_name} (Size={group_size})\"] = slice_results\n",
    "\n",
    "fairness_plot = plotter.metrics_comparison_bar(fairness_metrics)\n",
    "report.log_plotly_figure(\n",
    "    fig=fairness_plot,\n",
    "    alt=\"Slice Metric Comparison\",\n",
    "    section_name=\"quantitative analysis\",\n",
    ")\n",
    "fairness_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Generation\n",
    "\n",
    "Before generating the model card, let us document some of the details of the model and some considerations involved in developing and using the model.\n",
    "\n",
    "\n",
    "Let's start with populating the model details section, which includes the following fields by default:\n",
    "- description: A high-level description of the model and its usage for a general audience.\n",
    "- version: The version of the model.\n",
    "- owners: The individuals or organizations that own the model.\n",
    "- license: The license under which the model is made available.\n",
    "- citation: The citation for the model.\n",
    "- references: Links to resources that are relevant to the model.\n",
    "- path: The path to where the model is stored.\n",
    "- regulatory_requirements: The regulatory requirements that are relevant to the model.\n",
    "\n",
    "We can add additional fields to the model details section by passing a dictionary to the `log_from_dict` method and specifying the section name as `model_details`. You can also use the `log_descriptor` method to add a new field object with a `description` attribute to any section of the model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_from_dict(\n",
    "    data={\n",
    "        \"description\": \"The model was trained on a subset of the MIMIC-III dataset \\\n",
    "            to predict in-hospital mortality for patients admitted to the ICU with \\\n",
    "            heart failure. In-hospital mortality is defined as vital status at \\\n",
    "            hospital discharge.\",\n",
    "    },\n",
    "    section_name=\"model_details\",\n",
    ")\n",
    "report.log_version(\n",
    "    version_str=\"0.0.1\",\n",
    "    date=\"2023-06-05\",\n",
    "    description=\"Initial Release\",\n",
    ")\n",
    "report.log_owner(name=\"CyclOps Team\", contact=\"vectorinstitute.github.io/cyclops/\")\n",
    "report.log_license(identifier=\"Apache-2.0\")\n",
    "report.log_reference(\n",
    "    link=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\"  # noqa: E501\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's populate the considerations section, which includes the following fields by default:\n",
    "- users: The intended users of the model.\n",
    "- use_cases: The use cases for the model. These could be primary, downstream or out-of-scope use cases.\n",
    "- fairness_assessment: A description of the benefits and harms of the model for different groups as well as the steps taken to mitigate the harms.\n",
    "- ethical_considerations: The risks associated with using the model and the steps taken to mitigate them. This can be populated using the  `log_risk` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_from_dict(\n",
    "    data={\n",
    "        \"users\": [\n",
    "            {\"description\": \"Hospitals\"},\n",
    "            {\"description\": \"Clinicians\"},\n",
    "        ]\n",
    "    },\n",
    "    section_name=\"considerations\",\n",
    ")\n",
    "report.log_user(description=\"ML Engineers\")\n",
    "report.log_use_case(\n",
    "    description=\"Predicting mortality of patients admitted to the ICU with heart \\\n",
    "        failure.\",\n",
    "    kind=\"primary\",\n",
    ")\n",
    "report.log_use_case(\n",
    "    description=\"Predicting mortality patients admitted to other units of a hospital \\\n",
    "        with different conditions.\",\n",
    "    kind=\"out-of-scope\",\n",
    ")\n",
    "report.log_fairness_assessment(\n",
    "    affected_group=\"gendera, age\",\n",
    "    benefit=\"Improved health outcomes for patients.\",\n",
    "    harm=\"Biased predictions for patients in certain groups (e.g. older patients) \\\n",
    "        may lead to worse health outcomes.\",\n",
    "    mitigation_strategy=\"We will monitor the performance of the model on these groups \\\n",
    "        and retrain the model if the performance drops below a certain threshold.\",\n",
    ")\n",
    "report.log_risk(\n",
    "    risk=\"The model may be used to make decisions that affect the health of patients.\",\n",
    "    mitigation_strategy=\"The model should be continuously monitored for performance \\\n",
    "        and retrained if the performance drops below a certain threshold.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model card is populated, you can generate the report using the `export` method. The report is generated in the form of an HTML file. A JSON file containing the model card data will also be generated along with the HTML file. By default, the files will be saved in a folder named `cyclops_reports` in the current working directory. You can change the path by passing a `output_dir` argument when instantiating the `ModelCardReport` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.log_image(\n",
    "    \"/home/malinoori/cyclops/use_cases/plots/roc_curve_multilabel.png\",\n",
    "    alt=\"test\",\n",
    "    section_name=\"quantitative_analysis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops",
   "language": "python",
   "name": "cyclops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
