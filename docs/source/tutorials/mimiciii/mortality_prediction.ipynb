{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-hospital Mortality Prediction\n",
    "\n",
    "This notebook showcases in-hospital mortality prediction due to heart failure on a subset of the MIMIC-III dataset using Cyclops."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datasets import Dataset\n",
    "from datasets.features import ClassLabel\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from cyclops.data.slicer import SliceSpec\n",
    "from cyclops.evaluate.fairness import FairnessConfig  # noqa: E402\n",
    "from cyclops.evaluate.metrics import MetricCollection, create_metric\n",
    "from cyclops.models.catalog import create_model\n",
    "from cyclops.process.feature.feature import TabularFeatures\n",
    "from cyclops.tasks.mortality_prediction import MortalityPredictionTask\n",
    "from cyclops.utils.file import join, load_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "RANDOM_SEED = 85\n",
    "NAN_THRESHOLD = 0.75\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Before starting, make sure to install the Kaggle API by running `pip install kaggle`. To use the Kaggle API, you need to sign up for a Kaggle account at https://www.kaggle.com. Then go to the 'Account' tab of your user profile (https://www.kaggle.com/<username>/account) and select 'Create API Token'. This will trigger the download of kaggle.json, a file containing your API credentials. Place this file in the location ~/.kaggle/kaggle.json on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files(\n",
    "    \"saurabhshahane/in-hospital-mortality-prediction\", path=DATA_DIR, unzip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_dataframe(join(DATA_DIR, \"data01.csv\"), file_format=\"csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection and Preprocessing\n",
    "\n",
    "#### Drop NaNs based on the `NAN_THRESHOLD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "fig = go.Figure(data=[go.Bar(x=null_counts.index, y=null_counts.values)])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Number of Null Values per Column\",\n",
    "    xaxis_title=\"Columns\",\n",
    "    yaxis_title=\"Number of Null Values\",\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresh_nan = int(NAN_THRESHOLD * len(df))\n",
    "df = df.dropna(axis=1, thresh=thresh_nan)\n",
    "df = df.dropna(axis=0, subset=[\"outcome\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"gendera\"] = df[\"gendera\"].replace({1: 0, 2: 1})\n",
    "fig = px.pie(df, names=\"gendera\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Gender Distribution\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"age\")\n",
    "fig.update_layout(\n",
    "    title=\"Age Distribution\",\n",
    "    xaxis_title=\"Age\",\n",
    "    yaxis_title=\"Count\",\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"outcome\"] = df[\"outcome\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.pie(df, names=\"outcome\")\n",
    "fig.update_traces(textinfo=\"percent+label\")\n",
    "fig.update_layout(title_text=\"Outcome Distribution\")\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"Outcome: %{label}<br>Count: %{value}<br>Percent: %{percent}\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_counts = df[\"outcome\"].value_counts()\n",
    "class_ratio = class_counts[0] / class_counts[1]\n",
    "class_ratio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the features in the dataset, we select 20 of them which was reported by [Li et al.](https://pubmed.ncbi.nlm.nih.gov/34301649/)  to be the most important features in this classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_list = [\n",
    "    \"Anion gap\",\n",
    "    \"Lactic acid\",\n",
    "    \"Blood calcium\",\n",
    "    \"Lymphocyte\",\n",
    "    \"Leucocyte\",\n",
    "    \"heart rate\",\n",
    "    \"Blood sodium\",\n",
    "    \"Urine output\",\n",
    "    \"Platelets\",\n",
    "    \"Urea nitrogen\",\n",
    "    \"age\",\n",
    "    \"MCH\",\n",
    "    \"RBC\",\n",
    "    \"Creatine kinase\",\n",
    "    \"PCO2\",\n",
    "    \"Blood potassium\",\n",
    "    \"Diastolic blood pressure\",\n",
    "    \"Respiratory rate\",\n",
    "    \"Renal failure\",\n",
    "    \"NT-proBNP\",\n",
    "]\n",
    "features_list = sorted(features_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying feature types\n",
    "\n",
    "Cyclops `TabularFeatures` class helps to identify feature types, an essential step before preprocessing the data. Understanding feature types (numerical/categorical/binary) allows us to apply appropriate preprocessing steps for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tab_features = TabularFeatures(\n",
    "    data=df.reset_index(),\n",
    "    features=features_list,\n",
    "    by=\"ID\",\n",
    "    targets=\"outcome\",\n",
    ")\n",
    "tab_features.types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data preprocessors\n",
    "\n",
    "We create a data preprocessor using sklearn's ColumnTransformer. This helps in applying different preprocessing steps to different columns in the dataframe. For instance, binary features might be processed differently from numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "binary_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_features = sorted((tab_features.features_by_type(\"numeric\")))\n",
    "numeric_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in numeric_features\n",
    "]\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_features = sorted(tab_features.features_by_type(\"binary\"))\n",
    "binary_features.remove(\"outcome\")\n",
    "binary_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in binary_features\n",
    "]\n",
    "binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_indices),\n",
    "        (\"bin\", binary_transformer, binary_indices),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Hugging Face Dataset\n",
    "\n",
    "We convert our processed Pandas dataframe into a Hugging Face dataset, a powerful and easy-to-use data format which is also compatible with Cyclops models and evaluator modules. The dataset is then split to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.cleanup_cache_files()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"outcome\", ClassLabel(num_classes=2))\n",
    "dataset = dataset.train_test_split(\n",
    "    train_size=TRAIN_SIZE, stratify_by_column=\"outcome\", seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Cyclops model registry allows for straightforward creation and selection of models. This registry maintains a list of pre-configured models, which can be instantiated with a single line of code. Here we use a SGD classifier to fit a logisitic regression model. The model configurations can be passed to `create_model` based on the sllearn parameters for SGDClassifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"sgd_classifier\"\n",
    "model = create_model(model_name, random_state=123, verbose=0, class_weight=\"balanced\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Creation\n",
    "\n",
    "We use Cyclops tasks to define our model's task (in this case, MortalityPrediction), train the model, make predictions, and evaluate performance. Cyclops task classes encapsulate the entire ML pipeline into a single, cohesive structure, making the process smooth and easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mortality_task = MortalityPredictionTask(\n",
    "    {model_name: model}, task_features=features_list, task_target=\"outcome\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mortality_task.list_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `best_model_params` is passed to the `train` method, the best model will be selected after the hyperparameter search. The parameters in `best_model_params` indicate the values to create the parameters grid.\n",
    "\n",
    "Note that the data preprocessor needs to be passed to the tasks methods if the Hugging Face dataset is not already preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_params = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"metric\": \"roc_auc\",\n",
    "    \"method\": \"grid\",\n",
    "}\n",
    "\n",
    "mortality_task.train(\n",
    "    dataset[\"train\"],\n",
    "    model_name=model_name,\n",
    "    transforms=preprocessor,\n",
    "    best_model_params=best_model_params,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction output can be either the whole Hugging Face dataset with the prediction columns added to it or the single column containing the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = mortality_task.predict(\n",
    "    dataset[\"test\"],\n",
    "    model_name=model_name,\n",
    "    transforms=preprocessor,\n",
    "    proba=False,\n",
    "    only_predictions=True,\n",
    ")\n",
    "len(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is done using various evaluation metrics that provide different perspectives on the model's predictive abilities i.e. standard performance metrics and fairness metrics.\n",
    "\n",
    "The standard performance metrics can be created using the `MetricCollection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auroc\", \"roc_curve\"]\n",
    "metrics = [create_metric(metric_name, task=\"binary\") for metric_name in metric_names]\n",
    "metric_collection = MetricCollection(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to overall metrics, it might be interesting to see how the model performs on certain subpopulations. We can define these subpopulations using `SliceSpec` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_list = [\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"min_value\": 30,\n",
    "            \"max_value\": 50,\n",
    "            \"min_inclusive\": True,\n",
    "            \"max_inclusive\": False,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"min_value\": 50,\n",
    "            \"max_value\": 80,\n",
    "            \"min_inclusive\": True,\n",
    "            \"max_inclusive\": False,\n",
    "        }\n",
    "    },\n",
    "    {\"gendera\": {\"value\": 1}},\n",
    "    {\"gendera\": {\"value\": 0}},\n",
    "    {\n",
    "        \"Anion gap\": {\n",
    "            \"min_value\": 14.73,\n",
    "            \"min_inclusive\": False,\n",
    "        }\n",
    "    },\n",
    "]\n",
    "slice_spec = SliceSpec(spec_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `MetricCollection` can also be defined for the fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "specificity = create_metric(\n",
    "    metric_name=\"specificity\",\n",
    "    task=\"binary\",\n",
    ")\n",
    "sensitivity = create_metric(\n",
    "    metric_name=\"sensitivity\",\n",
    "    task=\"binary\",\n",
    ")\n",
    "\n",
    "fpr = 1 - specificity\n",
    "fnr = 1 - sensitivity\n",
    "\n",
    "ber = (fpr + fnr) / 2\n",
    "\n",
    "fairness_metric_collection = MetricCollection(\n",
    "    {\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"FPR\": fpr,\n",
    "        \"FNR\": fnr,\n",
    "        \"BER\": ber,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FairnessConfig helps in setting up and evaluating the fairness of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fairness_config = FairnessConfig(\n",
    "    metrics=fairness_metric_collection,\n",
    "    dataset=None,  # dataset is passed from the evaluator\n",
    "    target_columns=None,  # target columns are passed from the evaluator\n",
    "    groups=[\"gendera\", \"age\"],\n",
    "    group_bins={\"age\": [60, 70, 80]},\n",
    "    group_base_values={\"age\": 20, \"gendera\": 0},\n",
    "    thresholds=[0.5],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate methods outputs the evaluation results and the Hugging Face dataset with the predictions added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results, dataset_with_preds = mortality_task.evaluate(\n",
    "    dataset[\"test\"],\n",
    "    metric_collection,\n",
    "    model_names=model_name,\n",
    "    transforms=preprocessor,\n",
    "    prediction_column_prefix=\"preds\",\n",
    "    slice_spec=slice_spec,\n",
    "    batch_size=64,\n",
    "    fairness_config=fairness_config,\n",
    "    override_fairness_metrics=False,\n",
    ")\n",
    "dataset_with_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[model_name].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[model_name][\"overall\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = results[model_name][\"age:[50 - 80)\"][\"BinaryROCCurve\"]\n",
    "aurocs = results[model_name][\"age:[50 - 80)\"][\"BinaryAUROC\"]\n",
    "\n",
    "\n",
    "trace0 = go.Scatter(x=fpr, y=tpr, mode=\"lines\", name=f\"ROC curve (area = {aurocs:.2f})\")\n",
    "trace1 = go.Scatter(\n",
    "    x=[0, 1], y=[0, 1], mode=\"lines\", name=\"Random\", line=dict(dash=\"dash\")\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace0, trace1])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC Curve. age:[50 - 80)\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\",\n",
    "    showlegend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = results[model_name][\"gendera:1\"][\"BinaryROCCurve\"]\n",
    "aurocs = results[model_name][\"gendera:1\"][\"BinaryAUROC\"]\n",
    "\n",
    "\n",
    "trace0 = go.Scatter(x=fpr, y=tpr, mode=\"lines\", name=f\"ROC curve (area = {aurocs:.2f})\")\n",
    "trace1 = go.Scatter(\n",
    "    x=[0, 1], y=[0, 1], mode=\"lines\", name=\"Random\", line=dict(dash=\"dash\")\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace0, trace1])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC Curve. gender: female\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\",\n",
    "    showlegend=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[\"fairness\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops",
   "language": "python",
   "name": "cyclops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
