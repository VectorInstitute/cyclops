{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ignored-hostel",
   "metadata": {},
   "source": [
    "### This notebook loads extracted data, looks through the different fields, trains some baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-memorial",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-necessity",
   "metadata": {},
   "source": [
    "## Load data, print keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/mnt/nfs/project/delirium/all_hourly_data.h5\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "with pd.HDFStore(DATA_PATH, \"r\") as hdf:\n",
    "    hdf_keys= list(hdf.keys())\n",
    "    print(hdf_keys)\n",
    "    \n",
    "    dfs={}\n",
    "    print('Reading Patient Data ...')\n",
    "    dfs['statics'] = pd.read_hdf(DATA_PATH, key='patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-plane",
   "metadata": {},
   "source": [
    "## Patient Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs['statics'].count())\n",
    "dfs['statics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-provincial",
   "metadata": {},
   "source": [
    "## Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSPITAL_ID = {\n",
    "    \"THPM\": 0,\n",
    "    \"SBK\": 1,\n",
    "    \"UHNTG\": 2,\n",
    "    \"SMH\": 3,\n",
    "    \"UHNTW\": 4,\n",
    "    \"THPC\": 5,\n",
    "    \"PMH\": 6,\n",
    "    \"MSH\": 7,\n",
    "}\n",
    "\n",
    "labs = pd.DataFrame()\n",
    "for site in HOSPITAL_ID:\n",
    "    labs = pd.read_hdf(DATA_PATH, key=f'vitals_labs_{site}')\n",
    "    print(site, labs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-service",
   "metadata": {},
   "source": [
    "## Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-change",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inner-yorkshire",
   "metadata": {},
   "source": [
    "## Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[f'outcomes_{SITE}'].count())\n",
    "dfs[f'outcomes_{SITE}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-editing",
   "metadata": {},
   "source": [
    "## Train linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    RocCurveDisplay,\n",
    "    confusion_matrix\n",
    ")\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "HOSPITAL_ID = {\n",
    "    0: \"THPM\",\n",
    "    1: \"SBK\",\n",
    "    2: \"UHNTG\",\n",
    "    3: \"SMH\",\n",
    "    4: \"UHNTW\",\n",
    "    5: \"THPC\",\n",
    "    6: \"PMH\",\n",
    "    7: \"MSH\",\n",
    "}\n",
    "features = [\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"icd10_A00_B99\",\n",
    "    \"icd10_C00_D49\",\n",
    "    \"icd10_D50_D89\",\n",
    "    \"icd10_E00_E89\",\n",
    "    \"icd10_F01_F99\",\n",
    "    \"icd10_G00_G99\",\n",
    "    \"icd10_H00_H59\",\n",
    "    \"icd10_H60_H95\",\n",
    "    \"icd10_I00_I99\",\n",
    "    \"icd10_J00_J99\",\n",
    "    \"icd10_K00_K95\",\n",
    "    \"icd10_L00_L99\",\n",
    "    \"icd10_M00_M99\",\n",
    "    \"icd10_N00_N99\",\n",
    "    \"icd10_O00_O99\",\n",
    "    \"icd10_Q00_Q99\",\n",
    "    \"icd10_R00_R99\",\n",
    "    \"icd10_S00_T88\",\n",
    "    \"icd10_U07_U08\",\n",
    "    \"icd10_Z00_Z99\",\n",
    "]\n",
    "num_features = len(features)\n",
    "hidden_layer_sz = (num_features, num_features * 2, num_features)\n",
    "normalize_features = [\"age\"]\n",
    "predict = \"los\"\n",
    "\n",
    "\n",
    "def split_dataset(df, t, v, split_col):\n",
    "    \"\"\"Split dataset based on column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.Dataframe\n",
    "        Data encapsulated in Dataframe.\n",
    "    t: list\n",
    "        List of column values to use for training.\n",
    "    v: list\n",
    "        List of column values to use for validation.\n",
    "    split_col: str\n",
    "        Name of column to use for splitting data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train, val) Dataframes.\n",
    "    \"\"\"\n",
    "    train = df.loc[df[split_col].isin(t)]\n",
    "    val = df.loc[df[split_col].isin(v)]\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def normalize(df, features, mean=None, std=None):\n",
    "    df = df.copy()\n",
    "    df_features = df.loc[:, features]\n",
    "    if mean is None or std is None:\n",
    "        mean, std = df_features.mean(), df_features.std()\n",
    "    df.loc[:, features] = (df_features - mean) / std\n",
    "    return df, mean, std\n",
    "\n",
    "\n",
    "def discretize_los(los):\n",
    "    \"\"\"Discretize into 3 classes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    los: float\n",
    "        Length of stay (days).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Discrete class for LOS.\n",
    "    \n",
    "    e.g.\n",
    "    <= 3 days -> 0\n",
    "    > 3 days <= 7 days -> 1\n",
    "    > 7 days -> 2\n",
    "    \n",
    "    \"\"\"\n",
    "    if los <= 3:\n",
    "        return 0\n",
    "    elif los <= 7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "\n",
    "if predict == 'del_present':\n",
    "    dataset = dfs['statics'].copy()\n",
    "    # Take out only delirium cohort.\n",
    "    dataset = dataset.loc[dataset['gemini_cohort'] == True]\n",
    "    dataset = dataset.loc[dataset['del_present'] != 3]\n",
    "    dataset_size = len(dataset)\n",
    "elif predict == 'los':\n",
    "    dataset_size = 100000\n",
    "    dataset = dfs['statics'].copy().sample(frac=1).head(dataset_size)\n",
    "    dataset['label'] = dataset[predict].apply(discretize_los)\n",
    "    # dataset['label'] = dataset[predict]\n",
    "    print(dataset['label'].value_counts())\n",
    "else:\n",
    "    dataset_size = 100000\n",
    "    dataset = dfs['statics'].copy().sample(frac=1).head(dataset_size)\n",
    "    dataset['label'] = dataset[predict]\n",
    "    print(dataset['label'].value_counts())\n",
    "\n",
    "\n",
    "hospitals = sorted(dataset['hospital_id'].unique())\n",
    "rocs = []\n",
    "for hospital in hospitals:\n",
    "    train_hospitals = [h for h in hospitals if h != hospital]\n",
    "    val_hospitals = [hospital]\n",
    "\n",
    "    train_dataset, val_dataset = split_dataset(dataset, train_hospitals, val_hospitals, 'hospital_id')\n",
    "    # Normalize some of the features (not the ones that are one-hot encoded).\n",
    "    # train_dataset, mean, std = normalize(train_dataset, normalize_features)\n",
    "\n",
    "    train_dataset = train_dataset[features].join(train_dataset['label'])\n",
    "    val_dataset = val_dataset[features].join(val_dataset['label'])\n",
    "    print(f\"Train set size: {len(train_dataset)}, Val set size: {len(val_dataset)}\")\n",
    "    train_dataset = train_dataset.fillna(-1)\n",
    "    val_dataset = val_dataset.fillna(-1)\n",
    "    print(f\"Train set size: {len(train_dataset)}, Val set size: {len(val_dataset)}\")\n",
    "\n",
    "    train_inputs = train_dataset[features].to_numpy()\n",
    "    train_labels = train_dataset['label'].to_numpy().squeeze().astype(np.int32)\n",
    "    # val_inputs = normalize(val_dataset[features], normalize_features, mean, std)[0].to_numpy()\n",
    "    val_inputs = val_dataset[features].to_numpy()\n",
    "    val_labels = val_dataset['label'].to_numpy().squeeze().astype(np.int32)\n",
    "    \n",
    "    # Can't apply ROC metric.\n",
    "    if len(np.unique(val_labels)) < 2:\n",
    "        continue\n",
    "\n",
    "    clf = MLPClassifier(max_iter=500,\n",
    "                        hidden_layer_sizes=hidden_layer_sz,\n",
    "                        verbose=True,\n",
    "                        learning_rate=\"adaptive\").fit(train_inputs, train_labels)\n",
    "    preds = clf.predict(val_inputs)\n",
    "    pred_probs = clf.predict_proba(val_inputs)\n",
    "    acc = accuracy_score(val_labels, preds)\n",
    "    \n",
    "    if len(np.unique(val_labels)) == 2:\n",
    "        roc = roc_auc_score(val_labels, pred_probs[:, 1])\n",
    "        fpr, tpr, thresholds = roc_curve(val_labels, pred_probs[:, 1])\n",
    "        print(confusion_matrix(val_labels, preds))\n",
    "        f1 = f1_score(val_labels, preds)\n",
    "        prec = precision_score(val_labels, preds)\n",
    "        rec = recall_score(val_labels, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                  estimator_name='risk predictor')\n",
    "        display.plot()\n",
    "        plt.show()\n",
    "    elif len(np.unique(val_labels)) > 2:\n",
    "        print(confusion_matrix(val_labels, preds))\n",
    "        f1 = f1_score(val_labels, preds, average='micro')\n",
    "        prec = precision_score(val_labels, preds, average='micro')\n",
    "        rec = recall_score(val_labels, preds, average='micro')\n",
    "        roc = roc_auc_score(val_labels, pred_probs, average='weighted', multi_class='ovr')\n",
    "    rocs.append(roc)\n",
    "\n",
    "    print(f\"{HOSPITAL_ID[hospital]} Acc: {acc}, f1-score: {f1}, Precision: {prec}, Recall: {rec}, ROC: {roc}\")\n",
    "    break\n",
    "\n",
    "print(f\"Mean ROC AUC: {np.mean(rocs)}, using dataset size: {dataset_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector_delirium",
   "language": "python",
   "name": "vector_delirium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
