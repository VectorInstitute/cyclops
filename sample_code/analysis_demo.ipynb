{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divine-stephen",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "import tasks.analysis as analysis\n",
    "import config.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-scenario",
   "metadata": {},
   "source": [
    "##  Load environment variable overrides and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "mlflow.set_tracking_uri('../mlruns')\n",
    "conf = config.read_config('../config/gemini.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-russell",
   "metadata": {},
   "source": [
    "## Dataset Drift Report (by year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run dataset drift analysis and generate html report: compare two years 2016 and 2017\n",
    "conf.html = True\n",
    "conf.slice = 'year'\n",
    "conf.data_ref = [2016]\n",
    "conf.data_eval = [2017]\n",
    "\n",
    "conf.report_full_path = 'dataset_2016_2017.html'\n",
    "\n",
    "# Uncomment to use data with noise added:\n",
    "# config.input =  '/mnt/nfs/project/delirium/data/all_before_2018_with_noise.csv'\n",
    "\n",
    "response = analysis.main(conf)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-salvation",
   "metadata": {},
   "source": [
    "## Dataset Drift Report (by hospital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.slice = 'hospital_id'\n",
    "conf.data_ref = [3]\n",
    "conf.data_eval = [7]\n",
    "conf.report_full_path = 'dataset_hospitals.html'\n",
    "response = analysis.main(conf)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-respect",
   "metadata": {},
   "source": [
    "## Model Performance Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run analysis and generate html report (this time model performance comparison)\n",
    "# looks at the prediction of the model trained on all data before 2018. Compares 2017(val) and 2018 (test).\n",
    "conf.type = 'performance'\n",
    "conf.reference= '/mnt/nfs/project/delirium/data/demo/results_2017.csv' \n",
    "conf.test = '/mnt/nfs/project/delirium/data/demo/results_2018.csv'\n",
    "conf.report_full_path = 'dataset_performance_comparison.html'\n",
    "name = analysis.main(conf)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-discipline",
   "metadata": {},
   "source": [
    "## System Monitoring with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFlow history for analysis\n",
    "# Display dataset drift analysis runs\n",
    "\n",
    "def get_dataset_metrics_list(data):\n",
    "     if ('metrics' in data.keys()) and data['metrics']:\n",
    "        metrics = data['metrics']\n",
    "        #timestamp = data['params']['timestamp']\n",
    "        drift = 'No' if metrics['dataset_drift']==0 else 'Yes'\n",
    "        return [drift, metrics['n_features'], metrics['n_drifted_features']]\n",
    "     else:\n",
    "        return ['-', '-', '-']\n",
    "\n",
    "# List all existing experiments\n",
    "all_experiments = mlflow.list_experiments()\n",
    "exp_data = []\n",
    "for exp in all_experiments:\n",
    "    row = [exp.name, exp.artifact_location, exp.lifecycle_stage]\n",
    "    exp_data.append(row)\n",
    "exp_frame = pd.DataFrame(exp_data, columns = ['Name', 'Artifacts', 'Status'])\n",
    "display(exp_frame)\n",
    "    \n",
    "exp = mlflow.get_experiment_by_name('DatasetAnalysis')\n",
    "runs = mlflow.list_run_infos(exp.experiment_id, max_results=5)\n",
    "table = []\n",
    "for r in runs:\n",
    "    exp_run = mlflow.get_run(r.run_id).to_dictionary()\n",
    "    path = exp_run['info']['artifact_uri'][6:]\n",
    "    config_file = os.path.join(path, 'config.json')\n",
    "    if not os.path.isfile(config_file):\n",
    "        continue\n",
    "    with open(config_file) as f:\n",
    "        data = json.load(f)\n",
    "        row = [data['input'], data['slice'], data['data_ref'], data['data_eval']]\n",
    "        row = row + get_dataset_metrics_list(exp_run['data'])\n",
    "        table.append(row)\n",
    "frame = pd.DataFrame(table, columns=['Input', 'Slice', 'Ref Slice', 'Eval Slice', 'Drift', 'Feat', 'Drift_Feat'])\n",
    "print('------------------- Dataset Analysis ----------------------')\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the executions folders\n",
    "executions_folder = '/mnt/nfs/project/delirium/data/demo/executions'\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "files = [f for f in listdir(executions_folder)]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-rings",
   "metadata": {},
   "source": [
    "## Continuous Pipeline Simulation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how dataset drift changes over time compared to the reference data\n",
    "\n",
    "table = []\n",
    "y = []\n",
    "for dir_run in files:\n",
    "    with open(os.path.join(executions_folder, dir_run, 'dataset_report.json')) as f:\n",
    "        data = json.load(f)\n",
    "        results = data['data_drift']['data']['metrics']\n",
    "    \n",
    "        feat = results['n_features']\n",
    "        drift = 'Yes' if results['dataset_drift'] else 'No'\n",
    "        feat_drift = results['n_drifted_features']\n",
    "        \n",
    "        y.append(feat_drift)\n",
    "        row = [dir_run, drift, str(feat_drift)+'/'+str(feat)]\n",
    "        table.append(row)\n",
    "    \n",
    "frame = pd.DataFrame(table, columns=['Period', 'Drift Detected', 'Drifted Features / All Features'])\n",
    "print('                  Dataset Analysis Results: number of drifted features  ')\n",
    "# print(frame)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "fig.set_size_inches(16.5, 8.5)\n",
    "x = frame['Period'].values\n",
    "ax.bar(x,y)\n",
    "ax.xaxis.set_ticks(frame['Period'].values)\n",
    "ax.set_xticklabels(frame['Period'].values, rotation='vertical', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for dir_run in files:\n",
    "    with open(os.path.join(executions_folder, dir_run, 'model_report.json')) as f:\n",
    "        data = json.load(f)\n",
    "        results = data['classification_performance']['data']['metrics']\n",
    "        baseline = results['reference']['accuracy']\n",
    "        y.append(results['current']['accuracy'])\n",
    "    \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(16.5, 8.5)\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.plot(x, y, '-', linewidth=3)\n",
    "ax.hlines(y=baseline, xmin = 0, xmax = max(x), linewidth=2, color='r')\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "ax.xaxis.set_ticks(frame['Period'].values)\n",
    "ax.set_xticklabels(frame['Period'].values, rotation='vertical', fontsize=11)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector_delirium",
   "language": "python",
   "name": "vector_delirium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
