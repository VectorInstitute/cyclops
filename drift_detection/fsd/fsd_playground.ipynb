{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from os import path\n",
    "from time import time\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "from divergence import FisherDivergence, KnnKS, ModelKS\n",
    "from featureshiftdetector import FeatureShiftDetector\n",
    "from fsd_models import GaussianDensity, Knn\n",
    "from fsd_utils import (\n",
    "    create_graphical_model,\n",
    "    get_confusion_tensor,\n",
    "    get_detection_metrics,\n",
    "    get_localization_metrics,\n",
    "    marginal_attack,\n",
    "    plot_confusion_matrix,\n",
    "    sim_copula_data,\n",
    ")\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from shift_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT_EXPERIMENT = input(\"Select experiment: \")\n",
    "OUTCOME = input(\"Select outcome variable: \")\n",
    "if SHIFT_EXPERIMENT != \"hosp_type\":\n",
    "    HOSPITAL = [input(\"Select hospital: \")]\n",
    "NA_CUTOFF = 0.60\n",
    "\n",
    "if SHIFT_EXPERIMENT == \"covid\":\n",
    "    SHIFT_BS = [\"pre-covid\", \"covid\"]\n",
    "\n",
    "if SHIFT_EXPERIMENT == \"seasonal\":\n",
    "    SHIFT_BS = [\"summer\", \"winter\", \"seasonal\"]\n",
    "\n",
    "(\n",
    "    (X_train, y_train),\n",
    "    (X_val, y_val),\n",
    "    (X_test, y_test),\n",
    "    feats,\n",
    "    orig_dims,\n",
    ") = import_dataset_hospital(\n",
    "    SHIFT_EXPERIMENT, OUTCOME, HOSPITAL, NA_CUTOFF, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global Experiment Parameters\n",
    "n_samples = 100  # The number of samples in p, q (thus n_samples_total = n_samples*2)\n",
    "n_bootstrap_runs = 50\n",
    "n_conditional_expectation = 30\n",
    "n_inner_expectation = n_conditional_expectation\n",
    "alpha = 0.05  # Significance level\n",
    "data_family = \"Copula\"\n",
    "a = 0.5\n",
    "b = 0.5\n",
    "rng = np.random.RandomState(42)\n",
    "torch.manual_seed(rng.randint(1000))\n",
    "method_list = [\n",
    "    \"score-method\"\n",
    "]  # we do not take the deep method into account with the simple boot.\n",
    "# dataset_list = ['Energy', 'Gas', 'COVID']\n",
    "dataset_list = [\"COVID\"]\n",
    "t_split_interval = 50\n",
    "n_comp_sensors_list = [1]\n",
    "window_size_list = [i * 100 for i in range(0, 11)]\n",
    "n_comp_sensors = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = int(np.ceil((X_train.shape[0] - 2 * n_samples) / t_split_interval))\n",
    "n_dim = X_train.shape[1]\n",
    "sqrtn = int(np.floor(np.sqrt(n_dim)))\n",
    "n_dataset_samples = X_train[n_samples:].shape[\n",
    "    0\n",
    "]  # to account for taking out n_samples for reference dist, p\n",
    "rng = np.random.RandomState(42)\n",
    "torch.manual_seed(rng.randint(1000))\n",
    "print(n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"gemini\"\n",
    "for method in method_list:\n",
    "    for shuffle_data_set in [False, True]:\n",
    "        # Experiment Switches\n",
    "        if shuffle_data_set:\n",
    "            shuffle_string = \"time axis shuffled\"\n",
    "            experiment_name = f\"time-boot-{method}-time-axis-shuffled-on-{dataset_name}\"\n",
    "        else:\n",
    "            shuffle_string = \"time axis unshuffled\"\n",
    "            experiment_name = (\n",
    "                f\"time-boot-{method}-time-axis-unshuffled-on-{dataset_name}\"\n",
    "            )\n",
    "        print()\n",
    "        print(\n",
    "            f\"Starting {method} on {dataset_name} dataset with {shuffle_string} and simple boot\"\n",
    "        )\n",
    "\n",
    "        n_trials = int(np.ceil((X_train.shape[0] - 2 * n_samples) / t_split_interval))\n",
    "        n_dim = X_train.shape[1]\n",
    "        sqrtn = int(np.floor(np.sqrt(n_dim)))\n",
    "        n_dataset_samples = X_train[n_samples:].shape[\n",
    "            0\n",
    "        ]  # to account for taking out n_samples for reference dist, p\n",
    "\n",
    "        ## Attack testing  ##\n",
    "        rng = np.random.RandomState(42)\n",
    "        torch.manual_seed(rng.randint(1000))\n",
    "\n",
    "        time_list = np.zeros(n_trials)\n",
    "        global_truth = np.zeros(n_trials)\n",
    "        detection = np.zeros(n_trials)\n",
    "        detection_results = np.zeros(shape=(n_dim, n_trials, 3))\n",
    "        j_attack = rng.choice(np.arange(n_dim), replace=True, size=n_trials)\n",
    "        for idx, feature in enumerate(j_attack[: int(n_trials / 2)]):\n",
    "            detection_results[feature, idx, 1] = 1  # recording where attacks happen\n",
    "            global_truth[idx] = 1\n",
    "\n",
    "        exception_occured = 0\n",
    "        exception_vector = np.full(shape=(n_trials), fill_value=False)\n",
    "        for test_idx, split_idx in enumerate(\n",
    "            range(0, X_train.shape[0] - 2 * n_samples, t_split_interval)\n",
    "        ):\n",
    "            start = time()\n",
    "            test_idx = int(test_idx)\n",
    "            split_idx = int(split_idx)\n",
    "            slice1 = split_idx\n",
    "            slice2 = split_idx + 2 * n_samples\n",
    "            #     try:\n",
    "            pq = X_train[slice1:slice2]  # Two sets of samples\n",
    "            pq = transform_data(\n",
    "                pq, do_diff=do_diff, do_power_transform=do_power_transform\n",
    "            )\n",
    "            p = pq[:n_samples]\n",
    "            q = pq[n_samples : n_samples * 2].copy()\n",
    "\n",
    "            if np.any(detection_results[:, test_idx, 1] == 1):  # attack!\n",
    "                attacked_features = j_attack[test_idx]\n",
    "                q[:, attacked_features] = rng.permutation(\n",
    "                    q[:, attacked_features]\n",
    "                )  # permutes q\n",
    "\n",
    "            # Bootstrap every time\n",
    "            fsd = FeatureShiftDetection(\n",
    "                p,\n",
    "                q,\n",
    "                rng=rng,\n",
    "                samples_generator=np.nan,\n",
    "                detection_method=method,\n",
    "                n_bootstrap_runs=n_bootstrap_runs,\n",
    "                n_conditional_expectation=n_conditional_expectation,\n",
    "                n_attacks=np.nan,\n",
    "                alpha=alpha,\n",
    "                j_attack=np.nan,\n",
    "                attack_testing=False,\n",
    "            )\n",
    "            bonferroni_threshold_vector = fsd.bonferroni_threshold_vector\n",
    "            threshold_vector = fsd.threshold_vector\n",
    "            bootstrap_score_means_vector = fsd.bootstrap_distribution.mean(axis=0)\n",
    "            bootstrap_score_std_vector = (\n",
    "                np.std(fsd.bootstrap_distribution, axis=0) + 1e-5\n",
    "            )\n",
    "\n",
    "            # now check after getting new threshold\n",
    "            score_vector = np.array(fsd.get_score(p, q))\n",
    "            detection_results[:, test_idx, 0] = score_vector\n",
    "            # predicting attack\n",
    "            if np.any(score_vector >= bonferroni_threshold_vector):\n",
    "                detection[test_idx] = 1\n",
    "                normalized_score_vector = (\n",
    "                    score_vector - bootstrap_score_means_vector\n",
    "                ) / bootstrap_score_std_vector\n",
    "                attacked_features = normalized_score_vector.argsort()[-1]\n",
    "                detection_results[attacked_features, test_idx, 2] = 1\n",
    "            time_list[test_idx] = time() - start\n",
    "\n",
    "        # Recording Attack Results\n",
    "        confusion_tensor = np.zeros(shape=(n_dim, 2, 2))\n",
    "        for feature_idx, feature_results in enumerate(detection_results):\n",
    "            confusion_tensor[feature_idx] = sklearn_confusion_matrix(\n",
    "                feature_results[:, 1], feature_results[:, 2], labels=[0, 1]\n",
    "            )\n",
    "\n",
    "        # overall detection confusion matrix\n",
    "        global_detection_confusion_matrix = sklearn_confusion_matrix(\n",
    "            global_truth, detection, labels=[0, 1]\n",
    "        )\n",
    "\n",
    "        full_tn, full_fp, full_fn, full_tp = confusion_tensor.sum(axis=0).flatten()\n",
    "        micro_precision = full_tp / (full_tp + full_fp)\n",
    "        micro_recall = full_tp / (full_tp + full_fn)\n",
    "\n",
    "        if shuffle_data_set:\n",
    "            print(\"Time axis shuffled\")\n",
    "        else:\n",
    "            print(\"Time axis unshuffled\")\n",
    "\n",
    "        tn, fp, fn, tp = global_detection_confusion_matrix.flatten()\n",
    "        detection_precision = tp / (tp + fp)\n",
    "        detection_recall = tp / (tp + fn)\n",
    "\n",
    "        print(\"Results for: \", experiment_name)\n",
    "        print(f\"Precision: {detection_precision * 100:.2f}%\")\n",
    "        print(f\"Recall: {detection_recall * 100:.2f}%\")\n",
    "\n",
    "        print(f\"Micro-precision: {micro_precision * 100:.2f}%\")\n",
    "        print(f\"Micro-recall: {micro_recall * 100:.2f}%\")\n",
    "\n",
    "        print(f\"Avg time per test: {time_list.mean():.2f} sec\")\n",
    "        print(f\"Total time: {time_list.sum():.2f} sec\")\n",
    "\n",
    "        # Saving Score Distributions\n",
    "        results_dict = {\n",
    "            \"detection_results\": detection_results,\n",
    "            \"global_confusion_matrix\": global_detection_confusion_matrix,\n",
    "            \"confusion_tensor\": confusion_tensor,\n",
    "            \"times\": time_list,\n",
    "        }\n",
    "        experiment_save_name = experiment_name + \"-results_dict.p\"\n",
    "        pickle.dump(\n",
    "            results_dict,\n",
    "            open(path.join(\"..\", \"..\", \"results\", experiment_save_name), \"wb\"),\n",
    "        )\n",
    "print(f'Experiment completed at {strftime(\"%a, %d %b %Y %I:%M%p\", localtime())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops",
   "language": "python",
   "name": "cyclops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
