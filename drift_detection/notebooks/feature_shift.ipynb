{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from featureshiftdetector import FeatureShiftDetector\n",
    "from divergence import ModelKS, KnnKS, FisherDivergence\n",
    "from fsd_models import GaussianDensity, Knn\n",
    "from fsd_utils import marginal_attack, create_graphical_model,get_detection_metrics, get_localization_metrics, plot_confusion_matrix, get_confusion_tensor, sim_copula_data\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from shift_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT_EXPERIMENT = input(\"Select experiment: \")\n",
    "OUTCOME = input(\"Select outcome variable: \")\n",
    "\n",
    "(\n",
    "    (X_train, y_train),\n",
    "    (X_val, y_val),\n",
    "    (X_test, y_test),\n",
    "    feats,\n",
    "    orig_dims,\n",
    ") = import_dataset_hospital(\n",
    "    SHIFT_EXPERIMENT, OUTCOME, HOSPITAL, NA_CUTOFF, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global Experiment Parameters\n",
    "n_samples = 100  # The number of samples in p, q (thus n_samples_total = n_samples*2)\n",
    "n_bootstrap_runs = 50\n",
    "n_conditional_expectation = 30\n",
    "n_inner_expectation = n_conditional_expectation\n",
    "alpha = 0.05  # Significance level\n",
    "data_family = 'Copula'\n",
    "a = 0.5\n",
    "b = 0.5\n",
    "rng = np.random.RandomState(42)\n",
    "torch.manual_seed(rng.randint(1000))\n",
    "method_list = ['score-method']  # we do not take the deep method into account with the simple boot.\n",
    "# dataset_list = ['Energy', 'Gas', 'COVID']\n",
    "dataset_list = ['COVID']\n",
    "t_split_interval = 50\n",
    "n_comp_sensors_list = [1]\n",
    "window_size_list = [i*100 for i in range(0,11)]\n",
    "n_comp_sensors = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = int(np.ceil((X_train.shape[0] - 2 * n_samples) / t_split_interval))\n",
    "n_dim = X_train.shape[1]\n",
    "sqrtn = int(np.floor(np.sqrt(n_dim)))\n",
    "n_dataset_samples = X_train[n_samples:].shape[0]  # to account for taking out n_samples for reference dist, p\n",
    "rng = np.random.RandomState(42)\n",
    "torch.manual_seed(rng.randint(1000))\n",
    "print(n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"gemini\"\n",
    "for method in method_list:\n",
    "    for shuffle_data_set in [False, True]:\n",
    "        # Experiment Switches\n",
    "        if shuffle_data_set:\n",
    "            shuffle_string = 'time axis shuffled'\n",
    "            experiment_name = f'time-boot-{method}-time-axis-shuffled-on-{dataset_name}'\n",
    "        else:\n",
    "            shuffle_string = 'time axis unshuffled'\n",
    "            experiment_name = f'time-boot-{method}-time-axis-unshuffled-on-{dataset_name}'\n",
    "        print()\n",
    "        print(f'Starting {method} on {dataset_name} dataset with {shuffle_string} and simple boot')\n",
    "\n",
    "        n_trials = int(np.ceil((X_train.shape[0] - 2 * n_samples) / t_split_interval))\n",
    "        n_dim = X_train.shape[1]\n",
    "        sqrtn = int(np.floor(np.sqrt(n_dim)))\n",
    "        n_dataset_samples = X_train[n_samples:].shape[0]  # to account for taking out n_samples for reference dist, p\n",
    "\n",
    "        ## Attack testing  ##\n",
    "        rng = np.random.RandomState(42)\n",
    "        torch.manual_seed(rng.randint(1000))\n",
    "\n",
    "        time_list = np.zeros(n_trials)\n",
    "        global_truth = np.zeros(n_trials)\n",
    "        detection = np.zeros(n_trials)\n",
    "        detection_results = np.zeros(shape=(n_dim, n_trials, 3))\n",
    "        j_attack = rng.choice(np.arange(n_dim), replace=True, size=n_trials)\n",
    "        for idx, feature in enumerate(j_attack[:int(n_trials / 2)]):\n",
    "            detection_results[feature, idx, 1] = 1  # recording where attacks happen\n",
    "            global_truth[idx] = 1\n",
    "\n",
    "        exception_occured = 0\n",
    "        exception_vector = np.full(shape=(n_trials), fill_value=False)\n",
    "        for test_idx, split_idx in enumerate(range(0, X_train.shape[0] - 2 * n_samples, t_split_interval)):\n",
    "            start = time()\n",
    "            test_idx = int(test_idx)\n",
    "            split_idx = int(split_idx)\n",
    "            slice1 = split_idx\n",
    "            slice2 = split_idx + 2 * n_samples\n",
    "                #     try:\n",
    "            pq = X_train[slice1:slice2]  # Two sets of samples\n",
    "            pq = transform_data(pq, do_diff=do_diff, do_power_transform=do_power_transform)\n",
    "            p = pq[:n_samples]\n",
    "            q = pq[n_samples:n_samples * 2].copy()\n",
    "\n",
    "            if np.any(detection_results[:, test_idx, 1] == 1):  # attack!\n",
    "                attacked_features = j_attack[test_idx]\n",
    "                q[:, attacked_features] = rng.permutation(q[:, attacked_features])  # permutes q\n",
    "\n",
    "            # Bootstrap every time\n",
    "            fsd = FeatureShiftDetection(p, q, rng=rng, samples_generator=np.nan,\n",
    "                                            detection_method=method,\n",
    "                                            n_bootstrap_runs=n_bootstrap_runs,\n",
    "                                            n_conditional_expectation=n_conditional_expectation,\n",
    "                                            n_attacks=np.nan, alpha=alpha,\n",
    "                                            j_attack=np.nan, attack_testing=False)\n",
    "            bonferroni_threshold_vector = fsd.bonferroni_threshold_vector\n",
    "            threshold_vector = fsd.threshold_vector\n",
    "            bootstrap_score_means_vector = fsd.bootstrap_distribution.mean(axis=0)\n",
    "            bootstrap_score_std_vector = np.std(fsd.bootstrap_distribution, axis=0) + 1e-5\n",
    "\n",
    "            # now check after getting new threshold\n",
    "            score_vector = np.array(fsd.get_score(p, q))\n",
    "            detection_results[:, test_idx, 0] = score_vector\n",
    "            # predicting attack\n",
    "            if np.any(score_vector >= bonferroni_threshold_vector):\n",
    "                detection[test_idx] = 1\n",
    "                normalized_score_vector = (score_vector - bootstrap_score_means_vector) / bootstrap_score_std_vector\n",
    "                attacked_features = normalized_score_vector.argsort()[-1]\n",
    "                detection_results[attacked_features, test_idx, 2] = 1\n",
    "            time_list[test_idx] = time() - start\n",
    "            \n",
    "        # Recording Attack Results\n",
    "        confusion_tensor = np.zeros(shape=(n_dim, 2, 2))\n",
    "        for feature_idx, feature_results in enumerate(detection_results):\n",
    "            confusion_tensor[feature_idx] = sklearn_confusion_matrix(feature_results[:, 1],\n",
    "                                                                         feature_results[:, 2],\n",
    "                                                                         labels=[0, 1])\n",
    "\n",
    "        # overall detection confusion matrix\n",
    "        global_detection_confusion_matrix = sklearn_confusion_matrix(global_truth,\n",
    "                                                                         detection,\n",
    "                                                                         labels=[0, 1])\n",
    "\n",
    "        full_tn, full_fp, full_fn, full_tp = confusion_tensor.sum(axis=0).flatten()\n",
    "        micro_precision = full_tp / (full_tp + full_fp)\n",
    "        micro_recall = full_tp / (full_tp + full_fn)\n",
    "\n",
    "        if shuffle_data_set:\n",
    "            print('Time axis shuffled')\n",
    "        else:\n",
    "            print('Time axis unshuffled')\n",
    "                \n",
    "        tn, fp, fn, tp = global_detection_confusion_matrix.flatten()\n",
    "        detection_precision = tp / (tp + fp)\n",
    "        detection_recall = tp / (tp + fn)\n",
    "\n",
    "        print('Results for: ', experiment_name)\n",
    "        print(f'Precision: {detection_precision * 100:.2f}%')\n",
    "        print(f'Recall: {detection_recall * 100:.2f}%')\n",
    "\n",
    "        print(f'Micro-precision: {micro_precision * 100:.2f}%')\n",
    "        print(f'Micro-recall: {micro_recall * 100:.2f}%')\n",
    "\n",
    "        print(f'Avg time per test: {time_list.mean():.2f} sec')\n",
    "        print(f'Total time: {time_list.sum():.2f} sec')\n",
    "\n",
    "        # Saving Score Distributions\n",
    "        results_dict = {\n",
    "                'detection_results': detection_results,\n",
    "                'global_confusion_matrix': global_detection_confusion_matrix,\n",
    "                'confusion_tensor': confusion_tensor,\n",
    "                'times': time_list,\n",
    "        }\n",
    "        experiment_save_name = experiment_name + '-results_dict.p'\n",
    "        pickle.dump(results_dict,\n",
    "                        open(path.join('..', '..', 'results', experiment_save_name), 'wb'))\n",
    "print(f'Experiment completed at {strftime(\"%a, %d %b %Y %I:%M%p\", localtime())}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-KKtuQLwg-py3.9",
   "language": "python",
   "name": "cyclops-kktuqlwg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
