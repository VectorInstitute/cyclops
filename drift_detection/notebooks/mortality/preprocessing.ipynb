{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10841305-2ca3-4e00-b9ec-dd7cf3c0d69e",
   "metadata": {},
   "source": [
    "# Shared notebook for processing temporal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b737fc6-e557-453b-9967-8cefe2c8d80a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d37a3f-c67e-4548-93bd-85d39f7d8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.processors.aggregate import (\n",
    "    Aggregator,\n",
    "    tabular_as_aggregated,\n",
    "    timestamp_ffill_agg,\n",
    ")\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    DISCHARGE_TIMESTAMP,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    RESTRICT_TIMESTAMP,\n",
    "    TIMESTEP,\n",
    ")\n",
    "from cyclops.processors.constants import ALL, FEATURES, MEAN, NUMERIC, ORDINAL, STANDARD\n",
    "from cyclops.processors.feature.feature import TabularFeatures, TemporalFeatures\n",
    "from cyclops.processors.feature.vectorize import (\n",
    "    Vectorized,\n",
    "    intersect_vectorized,\n",
    "    split_vectorized,\n",
    "    vec_index_exp,\n",
    ")\n",
    "from cyclops.processors.impute import np_ffill_bfill, np_fill_null_num\n",
    "from cyclops.utils.file import (\n",
    "    join,\n",
    "    load_dataframe,\n",
    "    load_pickle,\n",
    "    save_dataframe,\n",
    "    save_pickle,\n",
    "    yield_dataframes,\n",
    "    yield_pickled_files,\n",
    ")\n",
    "from drift_detection.gemini.utils import get_use_case_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49c62b-ceae-4827-90c8-d4411619f089",
   "metadata": {},
   "source": [
    "# Choose dataset and use-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8cb82-ddad-4362-a450-6c351de6ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"gemini\"\n",
    "USE_CASE = \"mortality\"\n",
    "\n",
    "use_case_params = get_use_case_params(DATASET, USE_CASE)\n",
    "input(f\"WARNING: LOADING CONSTANTS FROM {use_case_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25686f7-9752-48e1-93a5-5d2ad6a2b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = load_dataframe(use_case_params.ENCOUNTERS_FILE)\n",
    "cohort = cohort.reset_index(drop=True)\n",
    "cohort.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccea95f-17af-4b70-8c6a-990a1ebe00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_features = TabularFeatures(\n",
    "    data=cohort,\n",
    "    features=use_case_params.TAB_FEATURES,\n",
    "    by=ENCOUNTER_ID,\n",
    "    force_types=use_case_params.TAB_FEATURES_TYPES,\n",
    ")\n",
    "\n",
    "numeric_features = tab_features.features_by_type(NUMERIC)\n",
    "ordinal_features = tab_features.features_by_type(ORDINAL)\n",
    "\n",
    "if len(ordinal_features) > 0:\n",
    "    print(ordinal_features[0], \"mapping:\")\n",
    "    print(tab_features.meta[ordinal_features[0]].get_mapping())\n",
    "\n",
    "tab_vectorized = tab_features.vectorize(to_binary_indicators=ordinal_features)\n",
    "save_pickle(tab_vectorized, use_case_params.TAB_VECTORIZED_FILE)\n",
    "save_pickle(tab_features, use_case_params.TAB_FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f7441-1fa5-4f3e-86d2-13d5ff016849",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataframe(use_case_params.ENCOUNTERS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0b8aa-701d-400b-bbe8-f6ceb0c484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = load_dataframe(use_case_params.ENCOUNTERS_FILE)[\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        DISCHARGE_TIMESTAMP,\n",
    "        use_case_params.TARGET_TIMESTAMP,\n",
    "    ]\n",
    "]\n",
    "start_timestamps = (\n",
    "    timestamps[[ENCOUNTER_ID, ADMIT_TIMESTAMP]]\n",
    "    .set_index(ENCOUNTER_ID)\n",
    "    .rename({ADMIT_TIMESTAMP: RESTRICT_TIMESTAMP}, axis=1)\n",
    ")\n",
    "start_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187007a-7fc8-4c33-b60a-44cb05562c16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Temporal-specific processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e89fbb-fffe-462c-a1fd-f9f9ce79dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which events to keep\n",
    "# Keep only the most popular events where the values are not null\n",
    "all_top_events = []\n",
    "for i, events in enumerate(yield_dataframes(use_case_params.CLEANED_DIR, log=False)):\n",
    "    top_events = (\n",
    "        events[EVENT_NAME][~events[EVENT_VALUE].isna()]\n",
    "        .value_counts()[: use_case_params.TOP_N_EVENTS]\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    all_top_events.append(top_events)\n",
    "\n",
    "    del events\n",
    "\n",
    "# Take only the events common to every file\n",
    "top_events = reduce(np.intersect1d, tuple(all_top_events))\n",
    "\n",
    "top_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44595c02-6526-4e6e-930d-53d2e4a3481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de07f3-12a3-492d-aa2b-5260480ff962",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator(\n",
    "    aggfuncs={EVENT_VALUE: MEAN},\n",
    "    timestamp_col=EVENT_TIMESTAMP,\n",
    "    time_by=ENCOUNTER_ID,\n",
    "    agg_by=[ENCOUNTER_ID, EVENT_NAME],\n",
    "    timestep_size=use_case_params.TIMESTEP_SIZE,\n",
    "    window_duration=use_case_params.WINDOW_DURATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa902f2-6213-4e7d-8ae6-b9c926aa232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate\n",
    "skip_n = 0\n",
    "generator = yield_dataframes(use_case_params.CLEANED_DIR, skip_n=skip_n, log=False)\n",
    "\n",
    "for save_count, events in enumerate(generator):\n",
    "    # Take only the top events\n",
    "    events = events[events[EVENT_NAME].isin(top_events)]\n",
    "\n",
    "    # Aggregate\n",
    "    events = events.reset_index(drop=True)\n",
    "    tmp_features = TemporalFeatures(\n",
    "        events,\n",
    "        features=EVENT_VALUE,\n",
    "        by=[ENCOUNTER_ID, EVENT_NAME],\n",
    "        timestamp_col=EVENT_TIMESTAMP,\n",
    "        aggregator=aggregator,\n",
    "    )\n",
    "\n",
    "    aggregated = tmp_features.aggregate(window_start_time=start_timestamps)\n",
    "\n",
    "    save_dataframe(\n",
    "        aggregated,\n",
    "        join(use_case_params.AGGREGATED_DIR, \"batch_\" + f\"{save_count + skip_n:04d}\"),\n",
    "    )\n",
    "    del events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04dfbd-09bf-4f95-be5c-365316989cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "skip_n = 0\n",
    "generator = yield_dataframes(use_case_params.AGGREGATED_DIR, skip_n=skip_n, log=False)\n",
    "for save_count, aggregated in enumerate(generator):\n",
    "    vec = aggregator.vectorize(aggregated)\n",
    "    save_pickle(\n",
    "        vec,\n",
    "        join(use_case_params.VECTORIZED_DIR, \"batch_\" + f\"{save_count + skip_n:04d}\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136fa3e-5df9-4ae0-8306-9e7a045c6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all Vectorized objects and turn them into a single object\n",
    "vecs = list([vec for vec in yield_pickled_files(use_case_params.VECTORIZED_DIR)])\n",
    "encounter_axis = vecs[0].get_axis(ENCOUNTER_ID)\n",
    "res = np.concatenate([vec.data for vec in vecs], axis=encounter_axis)\n",
    "indexes = vecs[0].indexes\n",
    "indexes[encounter_axis] = np.concatenate([vec.indexes[encounter_axis] for vec in vecs])\n",
    "temp_vectorized = Vectorized(res, indexes, vecs[0].axis_names)\n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc24eed-f576-4057-9464-9a91b906ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa840bb6-043c-466d-8ff1-0418a3ccd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized.axis_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f6c14-7246-41a9-8f8b-cfcdaad02355",
   "metadata": {},
   "source": [
    "## Target creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ffdc46-6f93-45a1-a9dd-7621284ac655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_timestep(timestamps, event):\n",
    "    timestamps[f\"{event}_after_admit\"] = timestamps[event] - timestamps[ADMIT_TIMESTAMP]\n",
    "    timestamps[f\"{event}_timestep\"] = (\n",
    "        timestamps[f\"{event}_after_admit\"]\n",
    "        / pd.Timedelta(f\"{use_case_params.TIMESTEP_SIZE} hour\")\n",
    "    ).apply(np.floor)\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "timestamps[\"target\"] = timestamps[use_case_params.TARGET_TIMESTAMP] - pd.DateOffset(\n",
    "    hours=use_case_params.PREDICT_OFFSET\n",
    ")\n",
    "timestamps = compute_timestep(timestamps, \"target\")\n",
    "timestamps = compute_timestep(timestamps, DISCHARGE_TIMESTAMP)\n",
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008eb58-3a91-4009-8fe9-b1c1ac3ab645",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps[~timestamps[use_case_params.TARGET_TIMESTAMP].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f526901-4292-4be6-ae40-e4fe5ea2864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_order = pd.Series(temp_vectorized.get_index(ENCOUNTER_ID))\n",
    "encounter_order = encounter_order.rename(ENCOUNTER_ID).to_frame()\n",
    "encounter_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3051ae-e472-45cc-8586-a6ce37997329",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_timestep = DISCHARGE_TIMESTAMP + \"_timestep\"\n",
    "timesteps = timestamps[[ENCOUNTER_ID, \"target_timestep\", discharge_timestep]]\n",
    "aligned_timestamps = pd.merge(encounter_order, timesteps, on=ENCOUNTER_ID, how=\"left\")\n",
    "aligned_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e82a1-9f50-4099-9683-d482974db839",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = int(use_case_params.WINDOW_DURATION / use_case_params.TIMESTEP_SIZE)\n",
    "shape = (len(aligned_timestamps), num_timesteps)\n",
    "\n",
    "arr1 = timestamp_ffill_agg(\n",
    "    aligned_timestamps[\"target_timestep\"], num_timesteps, fill_nan=2\n",
    ")\n",
    "arr2 = timestamp_ffill_agg(\n",
    "    aligned_timestamps[discharge_timestep], num_timesteps, val=-1, fill_nan=2\n",
    ")\n",
    "targets = np.minimum(arr1, arr2)\n",
    "targets[targets == 2] = 0\n",
    "targets[126:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede3ff8-3d76-4139-914a-de8239d20d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_timestamps.iloc[126:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd397c63-3626-4feb-9b86-c2b6a24216f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.expand_dims(np.expand_dims(targets, 0), 2)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc19e67-90c7-48fa-b1f7-bc06bf64976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80730791-f362-4c45-8533-33f83004c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include target\n",
    "# temp_vectorized = temp_vectorized.remove_with_index(EVENT_NAME, TEMP_TARGETS)\n",
    "# print(temp_vectorized.shape)\n",
    "temp_vectorized = temp_vectorized.concat_over_axis(\n",
    "    EVENT_NAME, targets, use_case_params.TEMP_TARGETS\n",
    ")\n",
    "temp_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af4cdc-0e82-4b7b-b9d6-69c91b2c8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_targets = temp_vectorized.take_with_index(EVENT_NAME, use_case_params.TEMP_TARGETS)\n",
    "assert np.isnan(only_targets.data).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73ccf9-d87c-4d45-9dc8-cb11271b0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(temp_vectorized, use_case_params.TEMP_VECTORIZED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103337f-dbd8-4f04-96a7-b6e721cd3642",
   "metadata": {},
   "source": [
    "# Combined processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b89e1c-a1d7-4dd1-b37c-6e78698e3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized = load_pickle(use_case_params.TEMP_VECTORIZED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7197544-6b93-4856-8959-c1a8228557db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tab_features.get_data(to_binary_indicators=ordinal_features).reset_index()\n",
    "\n",
    "# Take only the encounters with temporal events\n",
    "tab = tab[np.in1d(tab[ENCOUNTER_ID].values, temp_vectorized.get_index(ENCOUNTER_ID))]\n",
    "\n",
    "# Aggregate tabular\n",
    "tab_aggregated = tabular_as_aggregated(\n",
    "    tab=tab,\n",
    "    index=ENCOUNTER_ID,\n",
    "    var_name=EVENT_NAME,\n",
    "    value_name=EVENT_VALUE,\n",
    "    strategy=ALL,\n",
    "    num_timesteps=aggregator.window_duration // aggregator.timestep_size,\n",
    ")\n",
    "tab_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c547a8b-5ab5-4a85-8eb3-59aaa87f96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize tabular\n",
    "tab_aggregated_vec = aggregator.vectorize(tab_aggregated)\n",
    "tab_aggregated_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7b067-e628-49f7-af5d-e2ea297add57",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0b6e6-0633-4030-9863-7885d88c344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_aggregated_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406a4f3-b728-4e48-a763-b89253e73af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "comb_vectorized = temp_vectorized.concat_over_axis(\n",
    "    EVENT_NAME, tab_aggregated_vec.data, tab_aggregated_vec.get_index(EVENT_NAME)\n",
    ")\n",
    "comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfd482-0b8f-4c30-8d56-7e6b89c4f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include any of the tabular targets - split out to avoid label leakage\n",
    "comb_vectorized, _ = comb_vectorized.split_out(EVENT_NAME, use_case_params.TAB_TARGETS)\n",
    "comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f266a-007a-4e50-b525-7c47fb475d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_vectorized.get_index(EVENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd21b0c-4296-4d92-8e0e-9306011b0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(tab_aggregated_vec.data).sum() / tab_aggregated_vec.data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d283d99-9769-4f61-b423-6526a6928a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_aggregated_vec.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aee9c3-8304-41e5-9a35-8885732fdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(temp_vectorized.data).sum() / temp_vectorized.data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecbad6-29c9-471b-a42c-28fa177e6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(comb_vectorized.data).sum() / comb_vectorized.data.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0a06e-d146-45af-89cd-a90964a6b2df",
   "metadata": {},
   "source": [
    "# Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401ee79-1a30-41fd-9c42-fc13a6425649",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_vectorized.shape, temp_vectorized.shape, comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c1178-31af-4713-b3a2-fd5f5aacf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_vectorized, temp_vectorized, comb_vectorized = intersect_vectorized(\n",
    "    [tab_vectorized, temp_vectorized, comb_vectorized], axes=ENCOUNTER_ID\n",
    ")\n",
    "tab_vectorized.shape, temp_vectorized.shape, comb_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0eaa25-0014-4212-b2ae-794cd70e915d",
   "metadata": {},
   "source": [
    "Take only the encounters available in all of the datasets and align the datasets over encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c5aba-847e-4f58-9bce-69222f538f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize only numeric features (e.g., not binary indicators)\n",
    "# Note: Normalization is not occuring here, we are only doing the setup\n",
    "normalizer_map = {feat: STANDARD for feat in numeric_features}\n",
    "\n",
    "tab_vectorized.add_normalizer(\n",
    "    FEATURES,\n",
    "    normalizer_map=normalizer_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bb4b0-8948-42d6-a477-b0792a9b7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all events\n",
    "# Note: Normalization is not occuring here, we are only doing the setup\n",
    "temp_vectorized.add_normalizer(\n",
    "    EVENT_NAME,\n",
    "    normalization_method=STANDARD,\n",
    ")\n",
    "\n",
    "comb_vectorized.add_normalizer(\n",
    "    EVENT_NAME,\n",
    "    normalization_method=STANDARD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6663ae-1ed5-445e-a1d6-9021154fe1a7",
   "metadata": {},
   "source": [
    "## Dataset splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839995aa-40cb-449c-a057-ad7719998ecf",
   "metadata": {},
   "source": [
    "Split into training, validation, and testing datasets such that the tabular and temporal encounters remain aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ec3d2-a221-41ce-93ae-9f5a3018eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_splits, temp_splits, comb_splits = split_vectorized(\n",
    "    [tab_vectorized, temp_vectorized, comb_vectorized],\n",
    "    use_case_params.SPLIT_FRACTIONS,\n",
    "    axes=ENCOUNTER_ID,\n",
    ")\n",
    "tab_train, tab_val, tab_test = tab_splits\n",
    "temp_train, temp_val, temp_test = temp_splits\n",
    "comb_train, comb_val, comb_test = comb_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6a36d-499d-447e-93eb-3ac002c986c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train.shape, tab_val.shape, tab_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd710c-b21d-4dc4-b8e4-550235570768",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train.shape, temp_val.shape, temp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea0fb5-526a-4f42-af8d-d4ee9c9e60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_train.shape, comb_val.shape, comb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ec0ad-a35a-49df-8979-a32779c3ac13",
   "metadata": {},
   "source": [
    "## Split features/targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f20f3-fe51-41f5-be9f-c74d32519787",
   "metadata": {},
   "source": [
    "Split out the targets in the temporal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6b88f-8f3b-46d2-9d75-3fe8eec1745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train_X, tab_train_y = tab_train.split_out(FEATURES, use_case_params.TAB_TARGETS)\n",
    "tab_train_X.shape, tab_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a428e-fcab-4a49-b9c9-77fe5b56da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_val_X, tab_val_y = tab_val.split_out(FEATURES, use_case_params.TAB_TARGETS)\n",
    "tab_val_X.shape, tab_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adfca2-4347-475e-ad9d-aeb15a9f3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test_X, tab_test_y = tab_test.split_out(FEATURES, use_case_params.TAB_TARGETS)\n",
    "tab_test_X.shape, tab_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a251574-2635-42db-b42e-3d8b77143b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_X, temp_train_y = temp_train.split_out(\n",
    "    EVENT_NAME, use_case_params.TEMP_TARGETS\n",
    ")\n",
    "temp_train_X.shape, temp_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54795d9d-c2a4-4dd4-9ff9-d3927a0339a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_val_X, temp_val_y = temp_val.split_out(EVENT_NAME, use_case_params.TEMP_TARGETS)\n",
    "temp_val_X.shape, temp_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857300c-e98e-4af3-89eb-4665e887c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_X, temp_test_y = temp_test.split_out(EVENT_NAME, use_case_params.TEMP_TARGETS)\n",
    "temp_test_X.shape, temp_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24732d0c-cd51-44ed-b84a-ecb7d11588d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_train_X, comb_train_y = comb_train.split_out(\n",
    "    EVENT_NAME, use_case_params.TEMP_TARGETS\n",
    ")\n",
    "comb_train_X.shape, comb_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb708d63-2fff-4f76-814a-376eb6f06969",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_val_X, comb_val_y = comb_val.split_out(EVENT_NAME, use_case_params.TEMP_TARGETS)\n",
    "comb_val_X.shape, comb_val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86adf65-7931-49ce-ab02-eae544758a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test_X, comb_test_y = comb_test.split_out(EVENT_NAME, use_case_params.TEMP_TARGETS)\n",
    "comb_test_X.shape, comb_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacb495-8c8a-486a-95db-ca55c02597bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901b017-237b-4cb9-85a7-04172d7dc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(temp_vec):\n",
    "    # Forward fill then backward fill to get rid of all of the timestep nulls\n",
    "    temp_vec.impute_over_axis(TIMESTEP, np_ffill_bfill)\n",
    "\n",
    "    # Fill those all-null timesteps with feature mean\n",
    "    # (since forward and backward filling still leaves them all null)\n",
    "    axis = temp_vec.get_axis(EVENT_NAME)\n",
    "\n",
    "    for i in range(temp_vec.data.shape[axis]):\n",
    "        index_exp = vec_index_exp[:, :, i]\n",
    "        data_slice = temp_vec.data[index_exp]\n",
    "        mean = np.nanmean(data_slice)\n",
    "        func = lambda x: np_fill_null_num(x, mean)  # noqa: E731\n",
    "        temp_vec.impute_over_axis(TIMESTEP, func, index_exp=index_exp)\n",
    "\n",
    "    return temp_vec\n",
    "\n",
    "\n",
    "temp_train_X = impute(temp_train_X)\n",
    "temp_val_X = impute(temp_val_X)\n",
    "temp_test_X = impute(temp_test_X)\n",
    "\n",
    "comb_train_X = impute(comb_train_X)\n",
    "comb_val_X = impute(comb_val_X)\n",
    "comb_test_X = impute(comb_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45650c0-847e-4b13-974f-49f9b89cc5f3",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47896217-2e2c-449b-9ac9-b3ce08028f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = (\n",
    "    tab_train_X,\n",
    "    tab_val_X,\n",
    "    tab_test_X,\n",
    "    temp_train_X,\n",
    "    temp_val_X,\n",
    "    temp_test_X,\n",
    "    comb_train_X,\n",
    "    comb_val_X,\n",
    "    comb_test_X,\n",
    ")\n",
    "\n",
    "for split in splits:\n",
    "    split.fit_normalizer()\n",
    "    split.normalize()\n",
    "\n",
    "(\n",
    "    tab_train_X,\n",
    "    tab_val_X,\n",
    "    tab_test_X,\n",
    "    temp_train_X,\n",
    "    temp_val_X,\n",
    "    temp_test_X,\n",
    "    comb_train_X,\n",
    "    comb_val_X,\n",
    "    comb_test_X,\n",
    ") = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2c5b5-9979-4678-bead-ad42b547abfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88e2b1-373e-46c6-97ca-aabd2873fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "vectorized = [\n",
    "    (tab_train_X, \"tab_train_X\"),\n",
    "    (tab_train_y, \"tab_train_y\"),\n",
    "    (tab_val_X, \"tab_val_X\"),\n",
    "    (tab_val_y, \"tab_val_y\"),\n",
    "    (tab_test_X, \"tab_test_X\"),\n",
    "    (tab_test_y, \"tab_test_y\"),\n",
    "    (temp_train_X, \"temp_train_X\"),\n",
    "    (temp_train_y, \"temp_train_y\"),\n",
    "    (temp_val_X, \"temp_val_X\"),\n",
    "    (temp_val_y, \"temp_val_y\"),\n",
    "    (temp_test_X, \"temp_test_X\"),\n",
    "    (temp_test_y, \"temp_test_y\"),\n",
    "    (comb_train_X, \"comb_train_X\"),\n",
    "    (comb_train_y, \"comb_train_y\"),\n",
    "    (comb_val_X, \"comb_val_X\"),\n",
    "    (comb_val_y, \"comb_val_y\"),\n",
    "    (comb_test_X, \"comb_test_X\"),\n",
    "    (comb_test_y, \"comb_test_y\"),\n",
    "]\n",
    "for vec, name in vectorized:\n",
    "    save_pickle(vec, use_case_params.TAB_VEC_COMB + name + \".pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-KKtuQLwg-py3.9",
   "language": "python",
   "name": "cyclops-kktuqlwg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
