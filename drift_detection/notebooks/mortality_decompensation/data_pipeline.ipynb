{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5117a0d-9d1f-495e-93d5-edb057cf4c0a",
   "metadata": {},
   "source": [
    "## Data pipeline for predicting risk of mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb2625-e38f-428a-bcfd-6299b405d0f4",
   "metadata": {},
   "source": [
    "## Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.feature_handler import FeatureHandler\n",
    "from cyclops.plotter import plot_timeline, set_bars_color, setup_plot\n",
    "from cyclops.processor import run_data_pipeline\n",
    "from cyclops.processors.aggregate import Aggregator\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    AGE,\n",
    "    DIAGNOSIS_CODE,\n",
    "    DISCHARGE_DISPOSITION,\n",
    "    DISCHARGE_TIMESTAMP,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_CATEGORY,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    HOSPITAL_ID,\n",
    "    LENGTH_OF_STAY_IN_ER,\n",
    "    RESTRICT_TIMESTAMP,\n",
    "    SEX,\n",
    "    TIMESTEP,\n",
    "    TRIAGE_LEVEL,\n",
    "    WINDOW_START_TIMESTAMP,\n",
    ")\n",
    "from cyclops.processors.constants import SMH\n",
    "from cyclops.processors.events import (\n",
    "    combine_events,\n",
    "    convert_to_events,\n",
    "    normalize_events,\n",
    ")\n",
    "from cyclops.processors.impute import Imputer\n",
    "from cyclops.processors.statics import compute_statics\n",
    "from cyclops.processors.string_ops import replace_if_string_match, to_lower\n",
    "from cyclops.processors.util import (\n",
    "    create_indicator_variables,\n",
    "    fill_missing_timesteps,\n",
    "    gather_columns,\n",
    "    pivot_aggregated_events_to_features,\n",
    ")\n",
    "from cyclops.query import gemini\n",
    "from cyclops.utils.file import load_dataframe, save_dataframe\n",
    "\n",
    "MORTALITY = \"mortality\"\n",
    "LOS = \"los\"\n",
    "BASE_DATA_PATH = \"/mnt/nfs/project/delirium/drift_exp/JULY-04-2022\"\n",
    "AGGREGATION_WINDOW = 144\n",
    "AGGREGATION_BUCKET_SIZE = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8091448-5301-4631-a7d4-150667441e77",
   "metadata": {},
   "source": [
    "## Read saved query data or run query and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9efab5-8eb8-45fc-a53d-e5bfcc72ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(BASE_DATA_PATH, \"admin_er.parquet\")):\n",
    "    encounters_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"admin_er.parquet\"))\n",
    "    labs_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"labs.parquet\"))\n",
    "    imaging_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "    transfusions_data = pd.read_parquet(\n",
    "        os.path.join(BASE_DATA_PATH, \"transfusions.parquet\")\n",
    "    )\n",
    "    interventions_data = pd.read_parquet(\n",
    "        os.path.join(BASE_DATA_PATH, \"interventions.parquet\")\n",
    "    )\n",
    "    labs_data[EVENT_CATEGORY] = \"labs\"\n",
    "else:\n",
    "    os.makedirs(BASE_DATA_PATH, exist_ok=True)\n",
    "    er_admin_table = gemini.get_table(gemini.ER_ADMIN)\n",
    "\n",
    "    years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "    encounters = gemini.patient_encounters(\n",
    "        er_admin_table=er_admin_table,\n",
    "        years=years,\n",
    "        died=True,\n",
    "        died_binarize_col=\"mortality\",\n",
    "    )\n",
    "    encounters_labs = gemini.events(\n",
    "        patient_encounters_table=encounters.query, event_category=\"lab\"\n",
    "    )\n",
    "    imaging = gemini.imaging(years=years)\n",
    "    transfusions = gemini.blood_transfusions(years=years)\n",
    "    interventions = gemini.interventions(years=years)\n",
    "\n",
    "    encounters.run()\n",
    "    print(f\"{len(encounters.data)} rows extracted!\")\n",
    "\n",
    "    encounters_labs.run()\n",
    "    print(f\"{len(encounters_labs.data)} rows extracted!\")\n",
    "    encounters_labs.save(os.path.join(BASE_DATA_PATH, \"labs\"))\n",
    "    encounters_labs.clear_data()\n",
    "\n",
    "    imaging.run()\n",
    "    print(f\"{len(imaging.data)} rows extracted!\")\n",
    "    transfusions.run()\n",
    "    print(f\"{len(transfusions.data)} rows extracted!\")\n",
    "    interventions.run()\n",
    "    print(f\"{len(interventions.data)} rows extracted!\")\n",
    "\n",
    "    encounters_imaging = pd.merge(\n",
    "        encounters.data, imaging.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    "    )\n",
    "    encounters_transfusions = pd.merge(\n",
    "        encounters.data, transfusions.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    "    )\n",
    "    encounters_interventions = pd.merge(\n",
    "        encounters.data, interventions.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    "    )\n",
    "\n",
    "    encounters.save(os.path.join(BASE_DATA_PATH, \"admin_er\"))\n",
    "    encounters_imaging.to_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "    encounters_transfusions.to_parquet(os.path.join(BASE_DATA_PATH, \"transfusions.parquet\"))\n",
    "    encounters_interventions.to_parquet(\n",
    "        os.path.join(BASE_DATA_PATH, \"interventions.parquet\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91df5c-c266-45af-8541-3d1f291a93e2",
   "metadata": {},
   "source": [
    "## Map triage level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81be7a6-9a4c-4bd0-8f8a-406c2fcbba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(triage_level):\n",
    "    map_ = {\n",
    "        \"1\": \"RESUSCITATION\",\n",
    "        \"2\": \"EMERGENCY\",\n",
    "        \"3\": \"URGENT\",\n",
    "        \"4\": \"SEMI-URGENT\",\n",
    "        \"5\": \"NON-URGENT\",\n",
    "    }\n",
    "    return map_.get(triage_level, \"UNKNOWN\")\n",
    "\n",
    "\n",
    "encounters_data[TRIAGE_LEVEL] = encounters_data[TRIAGE_LEVEL].apply(remap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0af1b6-1e8b-4554-944f-f13209d282cc",
   "metadata": {},
   "source": [
    "## Map imaging and transfusions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131da1-d7e8-4564-b1ff-d6e9e2251d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_data = imaging_data.rename(\n",
    "    columns={\n",
    "        \"imaging_test_description\": EVENT_NAME,\n",
    "        \"performed_date_time\": EVENT_TIMESTAMP,\n",
    "    }\n",
    ")\n",
    "imaging_data[EVENT_CATEGORY] = \"imaging\"\n",
    "imaging_data[EVENT_VALUE] = 1\n",
    "\n",
    "transfusions_data = transfusions_data.rename(\n",
    "    columns={\"issue_date_time\": EVENT_TIMESTAMP}\n",
    ")\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[\"rbc_mapped\"]\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[EVENT_NAME].apply(\n",
    "    lambda x: \"rbc\" if x else \"non-rbc\"\n",
    ")\n",
    "transfusions_data[EVENT_VALUE] = 1\n",
    "transfusions_data[EVENT_CATEGORY] = \"transfusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564db1ba-9f9d-4b39-a57a-90a357afab8f",
   "metadata": {},
   "source": [
    "##  Process interventions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34a21c-49ec-48a9-bfac-0cae8dce5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions_data[EVENT_VALUE] = 1\n",
    "interventions_data[EVENT_CATEGORY] = \"interventions\"\n",
    "\n",
    "binary_mapped_cols = [\n",
    "    \"endoscopy_mapped\",\n",
    "    \"gi_endoscopy_mapped\",\n",
    "    \"bronch_endoscopy_mapped\",\n",
    "    \"dialysis_mapped\",\n",
    "    \"inv_mech_vent_mapped\",\n",
    "    \"surgery_mapped\",\n",
    "]\n",
    "interventions_data[\"intervention_episode_start_time\"].loc[\n",
    "    interventions_data[\"intervention_episode_start_time\"].isna()\n",
    "] = \"12:00:00\"\n",
    "interventions_data[EVENT_TIMESTAMP] = pd.to_datetime(\n",
    "    interventions_data[\"intervention_episode_start_date\"].astype(str)\n",
    "    + \" \"\n",
    "    + interventions_data[\"intervention_episode_start_time\"].astype(str)\n",
    ")\n",
    "interventions_data[EVENT_TIMESTAMP] = interventions_data[EVENT_TIMESTAMP].astype(\n",
    "    \"datetime64[ns]\"\n",
    ")\n",
    "interventions_data[\"unmapped_intervention\"] = ~(\n",
    "    interventions_data[\"endoscopy_mapped\"]\n",
    "    | interventions_data[\"gi_endoscopy_mapped\"]\n",
    "    | interventions_data[\"bronch_endoscopy_mapped\"]\n",
    "    | interventions_data[\"dialysis_mapped\"]\n",
    "    | interventions_data[\"inv_mech_vent_mapped\"]\n",
    "    | interventions_data[\"surgery_mapped\"]\n",
    ")\n",
    "interventions_data[EVENT_NAME] = interventions_data[\n",
    "    binary_mapped_cols + [\"unmapped_intervention\"]\n",
    "].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e205-6491-442d-92a2-abc9a5c2a15d",
   "metadata": {},
   "source": [
    "## Filter out encounters that had less than 24 hours LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc8075-57e8-49ac-b6f8-124d5b3591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data[LOS] = (\n",
    "    encounters_data[DISCHARGE_TIMESTAMP] - encounters_data[ADMIT_TIMESTAMP]\n",
    ")\n",
    "encounters_data_atleast_los_24_hrs = encounters_data.loc[\n",
    "    encounters_data[LOS] >= pd.to_timedelta(24, unit=\"h\")\n",
    "]\n",
    "# encounters_data_atleast_los_24_hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72566d9f-db66-47f2-8820-fdd8904aafe1",
   "metadata": {},
   "source": [
    "## Get encounters that ended in mortality outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f04573-71c9-4f25-9a31-21df233e8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality = encounters_data_atleast_los_24_hrs.loc[\n",
    "    encounters_data_atleast_los_24_hrs[MORTALITY] == True\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae3659-4304-4697-808d-75c66d2fe6f3",
   "metadata": {},
   "source": [
    "## Get encounters that didn't end up in mortality outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d191738-2eb9-4cda-83cd-fe17a7fc72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_not_mortality = encounters_data_atleast_los_24_hrs.loc[\n",
    "    encounters_data_atleast_los_24_hrs[MORTALITY] == False\n",
    "]\n",
    "\n",
    "num_encounters_not_mortality = len(encounters_mortality)\n",
    "encounters_not_mortality_subset = encounters_not_mortality[\n",
    "    0:num_encounters_not_mortality\n",
    "]\n",
    "# encounters_not_mortality_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464ae6f-2fe4-419e-8afb-57c8ab974136",
   "metadata": {},
   "source": [
    "## Combine both subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d25c8-e4a1-4529-a9de-5b02bd37c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_train_val_test = pd.concat(\n",
    "    [encounters_mortality, encounters_not_mortality_subset], ignore_index=True\n",
    ")\n",
    "# encounters_train_val_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569cdd0-1d93-4aba-a6fa-082282d95eb6",
   "metadata": {},
   "source": [
    "## Get encounters which result in death within timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8036a-a108-49a9-9a0e-9cf7b818e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = 14  # days\n",
    "encounters_mortality_within_risk_timeframe = encounters_mortality.loc[\n",
    "    encounters_mortality[LOS]\n",
    "    <= pd.to_timedelta(timeframe * 24 + AGGREGATION_WINDOW, unit=\"h\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6498e-f2a6-4383-89bd-6c9b5b4ee144",
   "metadata": {},
   "source": [
    "## Create death events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90081c16-ac41-4b60-ab48-7ff747630e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_events = convert_to_events(\n",
    "    encounters_mortality_within_risk_timeframe,\n",
    "    event_name=f\"death\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=DISCHARGE_TIMESTAMP,\n",
    ")\n",
    "mortality_events = pd.merge(\n",
    "    mortality_events, encounters_mortality, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "mortality_events = mortality_events[\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        EVENT_NAME,\n",
    "        EVENT_TIMESTAMP,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        EVENT_VALUE,\n",
    "        EVENT_CATEGORY,\n",
    "    ]\n",
    "]\n",
    "mortality_events[EVENT_VALUE] = 1\n",
    "# mortality_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2653f-b1de-4e63-8531-b92677534041",
   "metadata": {},
   "source": [
    "## Get admission/discharge events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de93b44-b260-44d5-869f-80752cc913c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_events = convert_to_events(\n",
    "    encounters_train_val_test,\n",
    "    event_name=\"admission\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=ADMIT_TIMESTAMP,\n",
    ")\n",
    "disch_events = convert_to_events(\n",
    "    encounters_train_val_test,\n",
    "    event_name=\"discharge\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=DISCHARGE_TIMESTAMP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238709bd-47f6-4cf6-8562-09717824d57f",
   "metadata": {},
   "source": [
    "## Filter events to be in train_val_test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d7deb-f5bf-4a7a-a773-5768a68f7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_train_val_test = labs_data.loc[\n",
    "    labs_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "imaging_train_val_test = imaging_data.loc[\n",
    "    imaging_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "transfusions_train_val_test = transfusions_data.loc[\n",
    "    transfusions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "interventions_train_val_test = interventions_data.loc[\n",
    "    interventions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "admit_events = admit_events.loc[\n",
    "    admit_events[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "disch_events = disch_events.loc[\n",
    "    disch_events[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e4baa-d706-4b72-88e4-7416db744d7e",
   "metadata": {},
   "source": [
    "## Normalize all event data (names, string operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e249a41-d012-4f25-9241-66c15fad7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_events = normalize_events(imaging_train_val_test)\n",
    "transfusion_events = normalize_events(transfusions_train_val_test)\n",
    "lab_events = normalize_events(labs_train_val_test)\n",
    "mortality_events = normalize_events(mortality_events)\n",
    "intervention_events = normalize_events(interventions_train_val_test)\n",
    "admit_events = normalize_events(admit_events)\n",
    "disch_events = normalize_events(disch_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950cea0-8178-4303-865d-ceb47ac2b30d",
   "metadata": {},
   "source": [
    "## Combine different event data, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080940-2a0e-45ab-987a-ef7f200a7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(BASE_DATA_PATH, \"combined_events.parquet\")):\n",
    "    combined_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"combined_events\"))\n",
    "else:\n",
    "    combined_events = combine_events(\n",
    "    [\n",
    "        intervention_events,\n",
    "        imaging_events,\n",
    "        transfusion_events,\n",
    "        lab_events,\n",
    "        mortality_events,\n",
    "        admit_events,\n",
    "        disch_events,\n",
    "        ]\n",
    "    )\n",
    "    save_dataframe(combined_events, os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc55f-d3f2-41c6-a367-c8a817b424d8",
   "metadata": {},
   "source": [
    "## Load aggregated events, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab947b7d-b489-4ce6-be32-546a8d0ae6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(BASE_DATA_PATH, \"aggregated_events.parquet\")):\n",
    "    aggregated_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"aggregated_events\"))\n",
    "    timestep_start_timestamps = load_dataframe(\n",
    "        os.path.join(BASE_DATA_PATH, \"aggmeta_start_ts\")\n",
    "    )\n",
    "    timestep_end_timestamps = load_dataframe(os.path.join(BASE_DATA_PATH, \"aggmeta_end_ts\"))\n",
    "else:\n",
    "    aggregator = Aggregator(bucket_size=AGGREGATION_BUCKET_SIZE, window=AGGREGATION_WINDOW)\n",
    "    aggregated_events = aggregator(combined_events)\n",
    "    save_dataframe(aggregated_events, os.path.join(BASE_DATA_PATH, \"aggregated_events\"))\n",
    "    save_dataframe(\n",
    "        aggregator.meta[\"timestep_start_timestamp\"],\n",
    "        os.path.join(BASE_DATA_PATH, \"aggmeta_start_ts\"),\n",
    "    )\n",
    "    save_dataframe(\n",
    "        aggregator.meta[\"timestep_end_timestamp\"],\n",
    "        os.path.join(BASE_DATA_PATH, \"aggmeta_end_ts\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a0ea6-713e-4c0f-ab5a-a31fe81c9071",
   "metadata": {},
   "source": [
    "## Some small number of events are getting aggregated to a 7th timestep!! (Debug later, remove for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281af9c0-d425-497f-ae8e-185537ce8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_events.loc[aggregated_events[\"timestep\"] == 6][\"event_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f2641-2b82-41fd-ad24-2fdde0cc64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_events = aggregated_events.loc[aggregated_events[\"timestep\"] != 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4793-a22b-4611-96b3-65e2bd0d3d3c",
   "metadata": {},
   "source": [
    "## Pivot aggregated events to get column-wise temporal features and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a50a78-9e33-4f71-b45c-a2ca21ad31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = pivot_aggregated_events_to_features(aggregated_events, np.mean)\n",
    "#save_dataframe(temporal_features, os.path.join(BASE_DATA_PATH, \"temporal_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277e20e-c9c7-427c-b3a4-e6120b342581",
   "metadata": {},
   "source": [
    "## Add to feature handler, with indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb4101-85ca-4a9f-b65f-b326417842bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler = FeatureHandler()\n",
    "\n",
    "temporal_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"temporal_features\"))\n",
    "feature_handler.add_features(temporal_features)\n",
    "feature_handler.drop_features([\"death\"])\n",
    "\n",
    "already_indicators = [\n",
    "    \"ct\",\n",
    "    \"dialysis_mapped\",\n",
    "    \"echo\",\n",
    "    \"endoscopy_mapped\",\n",
    "    \"interventional\",\n",
    "    \"inv_mech_vent_mapped\",\n",
    "    \"mri\",\n",
    "    \"non-rbc\",\n",
    "    \"other\",\n",
    "    \"rbc\",\n",
    "    \"surgery_mapped\",\n",
    "    \"ultrasound\",\n",
    "    \"unmapped_intervention\",\n",
    "    \"x-ray\",\n",
    "]\n",
    "\n",
    "temporal_features = feature_handler.features[\"temporal\"]\n",
    "indicators = create_indicator_variables(\n",
    "    temporal_features[\n",
    "        [col for col in temporal_features if col not in already_indicators]\n",
    "    ]\n",
    ")\n",
    "feature_handler.add_features(indicators)\n",
    "feature_handler.features[\"temporal\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e89d8-a325-45f5-a38e-0f0ef6abe443",
   "metadata": {},
   "source": [
    "## Compute static features, save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c4497-03d3-4304-86d1-54b4eb5b5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_train_val_test_static_cols = gather_columns(\n",
    "    encounters_train_val_test,\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        AGE,\n",
    "        SEX,\n",
    "        HOSPITAL_ID,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        DISCHARGE_TIMESTAMP,\n",
    "        TRIAGE_LEVEL,\n",
    "    ],\n",
    ")\n",
    "static_features = compute_statics(encounters_train_val_test_static_cols)\n",
    "#save_dataframe(static_features, os.path.join(BASE_DATA_PATH, \"static_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b6950-850c-4a83-a568-45088411911a",
   "metadata": {},
   "source": [
    "##  Load static features, add to feature handler, save all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f296e-8e45-44a7-bbb5-5c9feade3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"static_features\"))\n",
    "feature_handler.add_features(\n",
    "    static_features, reference_cols=[HOSPITAL_ID, ADMIT_TIMESTAMP, DISCHARGE_TIMESTAMP]\n",
    ")\n",
    "#feature_handler.save(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028c06-7771-46db-a291-f7621471ef5e",
   "metadata": {},
   "source": [
    "## Create new feature handler, load saved features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836f236-1f0b-46e8-90b8-8e811adcbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler1 = FeatureHandler()\n",
    "feature_handler1.load(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ad860-99aa-4423-b660-946d91891302",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_handler1.features[\"temporal\"].columns), len(\n",
    "    feature_handler1.features[\"static\"].columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575a670-3bb7-465c-96ab-c7173f16e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_handler1.features[\"temporal\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49719bee-1c6e-4f23-9dd9-a49f687f5370",
   "metadata": {},
   "source": [
    "## Impute temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb97e-e050-43fb-9b60-26f204260706",
   "metadata": {},
   "outputs": [],
   "source": [
    "static = feature_handler1.features[\"static\"]\n",
    "temporal = feature_handler1.features[\"temporal\"]\n",
    "\n",
    "numerical_cols = feature_handler1.get_numerical_feature_names()[\"temporal\"]\n",
    "cat_cols = feature_handler1.get_categorical_feature_names()[\"temporal\"]\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(temporal.isna().sum(axis=0))\n",
    "# numerical_cols, cat_cols\n",
    "\n",
    "\n",
    "# Remove encounters based on missingness.\n",
    "# print(temporal.shape)\n",
    "# encounter_ids = set(temporal.index.get_level_values(0))\n",
    "# for encounter_id in encounter_ids:\n",
    "#     features_encounter = temporal.loc[encounter_id]\n",
    "#     not_indicators = [col_name for col_name in temporal.columns if 'indicator' not in col_name]\n",
    "#     num_na_features = features_encounter[not_indicators].isna().sum().sum()\n",
    "#     fraction_missing = num_na_features / (\n",
    "#         len(features_encounter) * len(temporal.columns)\n",
    "#     )\n",
    "#     if fraction_missing > 0.4:\n",
    "#         temporal = temporal.drop(encounter_id, level=ENCOUNTER_ID)\n",
    "# num_encounters_dropped = len(encounter_ids) - len(\n",
    "#     set(temporal.index.get_level_values(0))\n",
    "# )\n",
    "# print(num_encounters_dropped)\n",
    "\n",
    "# mean over features (incomplete)\n",
    "# imputer(temporal[numerical_cols]).fillna(temporal[numerical_cols].mean(level=0, numeric_only=True, skipna=True))\n",
    "\n",
    "# Forward-fill, then backward\n",
    "temporal[numerical_cols] = temporal[numerical_cols].ffill().bfill()\n",
    "\n",
    "# Check no more missingness!\n",
    "assert not temporal.isna().sum().sum() and not static.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4b887-e9da-4d69-b13e-7649ca3a6659",
   "metadata": {},
   "source": [
    "## Add static features to temporal, (duplicate for each timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce62b29-67dd-4624-b973-071464e18fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with both static and temporal features.\n",
    "merged_static_temporal = temporal.combine_first(static)\n",
    "numerical_cols += [\"age\"]\n",
    "\n",
    "# Train model with just temporal features\n",
    "# merged_static_temporal = temporal\n",
    "# static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b63a8-58d1-41b7-9793-a8406514b6c1",
   "metadata": {},
   "source": [
    "## Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c4ae8-5ed3-4ab4-8d0b-c2d5d97d7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = int(AGGREGATION_WINDOW / AGGREGATION_BUCKET_SIZE)\n",
    "encounter_ids = list(merged_static_temporal.index.get_level_values(0).unique())\n",
    "num_encounters = len(encounter_ids)\n",
    "\n",
    "# All zeroes.\n",
    "labels = np.zeros((num_encounters, num_timesteps))\n",
    "\n",
    "# Set mortality within timeframe encounters to all 1s.\n",
    "labels[\n",
    "    [\n",
    "        encounter_ids.index(enc_id)\n",
    "        for enc_id in list(encounters_mortality_within_risk_timeframe[ENCOUNTER_ID])\n",
    "    ]\n",
    "] = 1\n",
    "\n",
    "# Get which timestep discharge occurs and set those and following timesteps label values to be -1.\n",
    "aggregated_discharge_events = aggregated_events.loc[\n",
    "    aggregated_events[EVENT_NAME] == \"discharge\"\n",
    "]\n",
    "aggregated_mortality_events = aggregated_events.loc[\n",
    "    aggregated_events[EVENT_NAME] == \"death\"\n",
    "]\n",
    "for enc_id in list(aggregated_discharge_events[ENCOUNTER_ID]):\n",
    "    timestep_discharge = aggregated_discharge_events.loc[\n",
    "        aggregated_discharge_events[ENCOUNTER_ID] == enc_id\n",
    "    ][\"timestep\"]\n",
    "    labels[encounter_ids.index(enc_id)][int(timestep_discharge) + 1 :] = -1\n",
    "\n",
    "# Lookahead for each timestep, and see if death occurs in risk timeframe.\n",
    "for enc_id in list(encounters_mortality_within_risk_timeframe[ENCOUNTER_ID]):\n",
    "    mortality_encounter = mortality_events.loc[mortality_events[ENCOUNTER_ID] == enc_id]\n",
    "    ts_ends = timestep_end_timestamps.loc[enc_id][\"timestep_end_timestamp\"]\n",
    "    mortality_ts = mortality_encounter[\"event_timestamp\"]\n",
    "    for ts_idx, ts_end in enumerate(ts_ends):\n",
    "        if not (\n",
    "            mortality_ts <= ts_end + pd.to_timedelta(timeframe * 24, unit=\"h\")\n",
    "        ).all():\n",
    "            labels[encounter_ids.index(enc_id)][ts_idx] = 0\n",
    "\n",
    "\n",
    "mortality_risk_targets = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da874af-fc43-4373-86b5-e3296893c27b",
   "metadata": {},
   "source": [
    "## Create train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516897c-ba3f-4449-9f71-e34a6986544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(\n",
    "    encounters: pd.DataFrame,\n",
    "    fractions: Optional[List] = [0.8, 0.2],\n",
    "    split_column: Optional[str] = None,\n",
    "    split_values: List = None,\n",
    "    seed: int = 3,\n",
    ") -> tuple:\n",
    "    \"\"\"Split encounters into train/test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encounters: pandas.DataFrame\n",
    "        Dataframe with encounter IDs.\n",
    "    fractions: list, optional\n",
    "        Fraction of samples to use for train, test sets.\n",
    "    split_column: str, optional\n",
    "        If 'split_column' is specified, then that column is used to split.\n",
    "    split_values: list, optional\n",
    "        Along with 'split_column', a list of lists can be specified for filtering.\n",
    "        e.g. [[2008], [2009, 2010]] for train/test split based on year.\n",
    "    seed: int, optional\n",
    "        Seed for random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train IDs, test IDs)\n",
    "\n",
    "    \"\"\"\n",
    "    if split_column:\n",
    "        if split_column not in encounters.columns:\n",
    "            raise ValueError(\"Specified 'split column' not found in input dataframe\")\n",
    "        if not split_values:\n",
    "            raise ValueError(\"Specify train/test split values for the 'split column'.!\")\n",
    "        train_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[0])\n",
    "        ]\n",
    "        test_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[1])\n",
    "        ]\n",
    "        return train_encounters, test_encounters\n",
    "\n",
    "    encounter_ids = list(encounters[ENCOUNTER_ID].unique())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(encounter_ids)\n",
    "    num_train = int(fractions[0] * len(encounter_ids))\n",
    "\n",
    "    return encounter_ids[0:num_train], encounter_ids[num_train:]\n",
    "\n",
    "\n",
    "split_type = \"hospital_id\"\n",
    "\n",
    "if split_type == \"year\":\n",
    "    encounters_train_val_test[\"year\"] = encounters_train_val_test[\n",
    "        \"admit_timestamp\"\n",
    "    ].dt.year\n",
    "    train_ids, val_test_ids = create_train_test_split(\n",
    "        encounters_train_val_test,\n",
    "        split_column=\"year\",\n",
    "        split_values=[[2015, 2016, 2017, 2018, 2019], [2020]],\n",
    "    )\n",
    "    encounters_train_val_test[HOSPITAL_ID].value_counts(), encounters_train_val_test[\n",
    "        \"year\"\n",
    "    ].value_counts()\n",
    "elif split_type == \"hospital_id\":\n",
    "    train_ids, val_test_ids = create_train_test_split(\n",
    "        encounters_train_val_test,\n",
    "        split_column=HOSPITAL_ID,\n",
    "        split_values=[[\"SBK\", \"UHNTG\", \"THPC\", \"THPM\", \"UHNTW\", \"SMH\"], [\"MSH\"]],\n",
    "    )\n",
    "elif split_type == \"random\":\n",
    "    train_ids, val_test_ids = create_train_test_split(encounters_train_val_test)\n",
    "\n",
    "\n",
    "val_ids, test_ids = create_train_test_split(\n",
    "    encounters_train_val_test.loc[\n",
    "        encounters_train_val_test[ENCOUNTER_ID].isin(val_test_ids)\n",
    "    ],\n",
    "    [0.5, 0.5],\n",
    ")\n",
    "print(\n",
    "    f\"Train set: {len(train_ids)}, Val set: {len(val_ids)}, Test set: {len(test_ids)}\"\n",
    ")\n",
    "\n",
    "X = merged_static_temporal[\n",
    "    np.in1d(temporal.index.get_level_values(0), static.index.get_level_values(0))\n",
    "]\n",
    "\n",
    "y_train, y_val, y_test = [\n",
    "    mortality_risk_targets[np.in1d(encounter_ids, ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "X_train, X_val, X_test = [\n",
    "    X[np.in1d(X.index.get_level_values(0), ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "\n",
    "len(X), len(X_train), len(X_val), len(X_test), len(mortality_risk_targets), len(y_train), len(y_val), len(y_test)\n",
    "\n",
    "assert len(X.index.get_level_values(0).unique()) == len(mortality_risk_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1a347-8076-434b-8bb5-95d0bcd5534e",
   "metadata": {},
   "source": [
    "## Dataset Shift Splits for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ab046-5e48-44e5-98a5-66fe2053fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = \"/mnt/nfs/project/delirium/drift_exp/JULY-04-2022/\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "def create_train_test_split(\n",
    "    encounters: pd.DataFrame,\n",
    "    fractions: Optional[List] = [0.8, 0.2],\n",
    "    split_column: Optional[str] = None,\n",
    "    split_values: List = None,\n",
    "    seed: int = 3,\n",
    ") -> tuple:\n",
    "    \"\"\"Split encounters into train/test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encounters: pandas.DataFrame\n",
    "        Dataframe with encounter IDs.\n",
    "    fractions: list, optional\n",
    "        Fraction of samples to use for train, test sets.\n",
    "    split_column: str, optional\n",
    "        If 'split_column' is specified, then that column is used to split.\n",
    "    split_values: list, optional\n",
    "        Along with 'split_column', a list of lists can be specified for filtering.\n",
    "        e.g. [[2008], [2009, 2010]] for train/test split based on year.\n",
    "    seed: int, optional\n",
    "        Seed for random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train IDs, test IDs)\n",
    "\n",
    "    \"\"\"\n",
    "    if split_column:\n",
    "        if split_column not in encounters.columns:\n",
    "            raise ValueError(\"Specified 'split column' not found in input dataframe\")\n",
    "        if not split_values:\n",
    "            raise ValueError(\"Specify train/test split values for the 'split column'.!\")\n",
    "        train_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[0])\n",
    "        ]\n",
    "        test_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[1])\n",
    "        ]\n",
    "        return train_encounters, test_encounters\n",
    "\n",
    "    encounter_ids = list(encounters[ENCOUNTER_ID].unique())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(encounter_ids)\n",
    "    num_train = int(fractions[0] * len(encounter_ids))\n",
    "\n",
    "    return encounter_ids[0:num_train], encounter_ids[num_train:]\n",
    "\n",
    "split_type = \"simulated_deployment\"\n",
    "\n",
    "if split_type == \"simulated_deployment\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            encounters_data[\"admit_timestamp\"].dt.date < datetime.date(2019, 1, 1),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            encounters_data[\"admit_timestamp\"].dt.date > datetime.date(2019, 1, 1),\n",
    "            \"encounter_id\",\n",
    "    ]    \n",
    "    \n",
    "if split_type == \"covid\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            encounters_data[\"admit_timestamp\"].dt.date < datetime.date(2020, 2, 1),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            encounters_data[\"admit_timestamp\"].dt.date > datetime.date(2020, 3, 1),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    \n",
    "if split_type == \"seasonal_summer\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"admit_timestamp\"].dt.month.isin([1,2,3,4,5,10,11,12])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"admit_timestamp\"].dt.month.isin([6, 7, 8, 9])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "if split_type == \"seasonal_winter\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"admit_timestamp\"].dt.month.isin([3,4,5,6,7,8,9,10])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"admit_timestamp\"].dt.month.isin([6, 7, 8, 9])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "        \n",
    "if split_type == \"hosp_type_academic\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"hospital_id\"].isin([\"SMH\", \"MSH\", \"UHNTG\", \"UHNTW\",\"PMH\", \"SBK\"])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"hospital_id\"].isin([\"THPC\", \"THPM\"])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "\n",
    "if split_type == \"hosp_type_community\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"hospital_id\"].isin([\"THPC\", \"THPM\"])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"hospital_id\"].isin([\"SMH\", \"MSH\", \"UHNTG\", \"UHNTW\",\"PMH\",\"SBK\"])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "        \n",
    "train_val_ids = encounters_train_val_test[ENCOUNTER_ID].loc[\n",
    "            encounters_train_val_test[ENCOUNTER_ID].isin(ids_source)\n",
    "]\n",
    "\n",
    "test_ids = encounters_train_val_test[ENCOUNTER_ID].loc[\n",
    "            encounters_train_val_test[ENCOUNTER_ID].isin(ids_target)\n",
    "]\n",
    "\n",
    "train_ids, val_ids = create_train_test_split(\n",
    "    encounters_train_val_test.loc[\n",
    "        encounters_train_val_test[ENCOUNTER_ID].isin(train_val_ids)\n",
    "    ],\n",
    "    [0.8, 0.2],\n",
    ")\n",
    "print(\n",
    "    f\"Train set: {len(train_ids)}, Val set: {len(val_ids)}, Test set: {len(test_ids)}\"\n",
    ")\n",
    "\n",
    "X = merged_static_temporal[\n",
    "    np.in1d(temporal.index.get_level_values(0), static.index.get_level_values(0))\n",
    "]\n",
    "\n",
    "y_train, y_val, y_test = [\n",
    "    mortality_risk_targets[np.in1d(encounter_ids, ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "X_train, X_val, X_test = [\n",
    "    X[np.in1d(X.index.get_level_values(0), ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "\n",
    "len(X), len(X_train), len(X_val), len(X_test), len(mortality_risk_targets), len(\n",
    "    y_train\n",
    "), len(y_val), len(y_test)\n",
    "assert len(X.index.get_level_values(0).unique()) == len(mortality_risk_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36dce7-2857-45bd-91af-69a7f4485597",
   "metadata": {},
   "source": [
    "## Save train/val/test ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a95142-e91f-43a9-a7b2-0e8a3c8b185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids_df = pd.DataFrame({\"train\": train_ids})\n",
    "val_ids_df = pd.DataFrame({\"val\": val_ids})\n",
    "test_ids_df = pd.DataFrame({\"test\": test_ids})\n",
    "train_val_test_ids = pd.concat([train_ids_df, val_ids_df, test_ids_df], axis=1)\n",
    "save_dataframe(\n",
    "    train_val_test_ids, os.path.join(BASE_DATA_PATH, split_type, \"train_val_test_ids\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bccd89-2893-45ea-b2b9-07629f4693c6",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfd46a-5ec5-4cc0-8291-fd2261abbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_normalized = X_train.copy()\n",
    "X_val_normalized = X_val.copy()\n",
    "X_test_normalized = X_test.copy()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    scaler = StandardScaler().fit(X_train[col].values.reshape(-1, 1))\n",
    "    X_train_normalized[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(X_train[col].values.reshape(-1, 1))),\n",
    "        index=X_train[col].index,\n",
    "    )\n",
    "    X_val_normalized[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(X_val[col].values.reshape(-1, 1))),\n",
    "        index=X_val[col].index,\n",
    "    )\n",
    "    X_test_normalized[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(X_test[col].values.reshape(-1, 1))),\n",
    "        index=X_test[col].index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072b711-31cc-4693-8b41-100f4dd39265",
   "metadata": {},
   "source": [
    "## Save inputs and labels as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a168e69-a265-426f-9ab2-2a0cba495691",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(BASE_DATA_PATH, split_type), exist_ok=True)\n",
    "\n",
    "X_train_normalized.to_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_train.parquet\")\n",
    ")\n",
    "X_val_normalized.to_parquet(os.path.join(BASE_DATA_PATH, split_type, \"X_val.parquet\"))\n",
    "X_test_normalized.to_parquet(os.path.join(BASE_DATA_PATH, split_type, \"X_test.parquet\"))\n",
    "\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"y_val.npy\"), y_val)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70504f5c-eeed-43c7-aa08-e63e81cb5b85",
   "metadata": {},
   "source": [
    "## Load train/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d301151-c865-4525-bf86-1e0f5bdde1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_train.parquet\")\n",
    ")\n",
    "X_val_normalized = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_val.parquet\")\n",
    ")\n",
    "X_test_normalized = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_test.parquet\")\n",
    ")\n",
    "\n",
    "y_train = np.load(os.path.join(BASE_DATA_PATH, split_type, \"y_train.npy\"))\n",
    "y_val = np.load(os.path.join(BASE_DATA_PATH, split_type, \"y_val.npy\"))\n",
    "y_test = np.load(os.path.join(BASE_DATA_PATH, split_type, \"y_test.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3219bbb-2ebb-42fb-879b-8e8f06d69861",
   "metadata": {},
   "source": [
    "## Reshape inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d73ba-d526-45b6-97e5-8d56f3442b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_inputs(inputs, num_timesteps):\n",
    "    inputs = inputs.unstack()\n",
    "    num_encounters = inputs.shape[0]\n",
    "    inputs = inputs.values.reshape((num_encounters, num_timesteps, -1))\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "X_train_normalized_npy = reshape_inputs(X_train_normalized, num_timesteps)\n",
    "X_val_normalized_npy = reshape_inputs(X_val_normalized, num_timesteps)\n",
    "X_test_normalized_npy = reshape_inputs(X_test_normalized, num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264bcee-1271-464a-a392-d8e369ec77e7",
   "metadata": {},
   "source": [
    "## Save inputs as npy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8df48d-7ecc-46c9-904e-82a9637368da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"X_train.npy\"), X_train_normalized_npy)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"X_val.npy\"), X_val_normalized_npy)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"X_test.npy\"), X_test_normalized_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812a9a8-1966-48a1-9094-66db6b4bde31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f4a47-a2f2-4071-8ae4-0b13957105a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_id = random.choice(encounter_ids)\n",
    "encounter_id = 13772609\n",
    "\n",
    "# Show examples:\n",
    "# 12779290, 13772609 - lookahead\n",
    "# 11454928, 15795997 - no mortality\n",
    "# 15253874 - all 1s\n",
    "\n",
    "print(encounter_id)\n",
    "combined_events_encounter = combined_events.loc[\n",
    "    combined_events[\"encounter_id\"] == encounter_id\n",
    "]\n",
    "fig = plot_timeline(combined_events_encounter, return_fig=True)\n",
    "\n",
    "fig = fig.update_layout(width=2000, height=800)\n",
    "\n",
    "ts_starts = timestep_start_timestamps.loc[encounter_id][\"timestep_start_timestamp\"]\n",
    "ts_ends = timestep_end_timestamps.loc[encounter_id][\"timestep_end_timestamp\"]\n",
    "for ts_end in ts_ends:\n",
    "    fig.add_vline(ts_end)\n",
    "\n",
    "label_encounter = labels[encounter_ids.index(encounter_id)]\n",
    "\n",
    "for i in range(num_timesteps):\n",
    "    label_ts = label_encounter[i]\n",
    "    color_map = {-1: \"grey\", 0: \"green\", 1: \"red\"}\n",
    "    fig.add_vrect(\n",
    "        x0=ts_starts[i],\n",
    "        x1=ts_ends[i],\n",
    "        fillcolor=color_map[label_ts],\n",
    "        opacity=0.25,\n",
    "        line_width=0,\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee092262-8a77-4ccc-933f-a4d316e28643",
   "metadata": {},
   "source": [
    "## TODO: Test out these imputation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1010c8-3c13-4dcd-803d-f5fe094749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_simple_v1(\n",
    "    dataframe: pd.DataFrame, time_index: str = TIMESTEP\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Impute features using 'Simple' method.\n",
    "\n",
    "    Concatenate the forward filled value, the mask of the measurement,\n",
    "    and the time of the last measurement.\n",
    "\n",
    "    Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu,\n",
    "    \"Recurrent Neural Networks for Multivariate Time Series with Missing Values,\"\n",
    "    Scientific Reports, vol. 8, no. 1, p. 6085, Apr 2018.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pandas.DataFrame\n",
    "        Temporal features dataframe.\n",
    "    time_index: str, optional\n",
    "        The name of the time-series index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "     Dataframe after applying imputation.\n",
    "\n",
    "    \"\"\"\n",
    "    # Mask missingness.\n",
    "    masked_df = pd.isna(dataframe)\n",
    "    masked_df = masked_df.apply(pd.to_numeric)\n",
    "\n",
    "    # Compute time since last measurement.\n",
    "    index_of_time = list(dataframe.index.names).index(time_index)\n",
    "    time_in = [item[index_of_time] for item in dataframe.index.tolist()]\n",
    "    time_df = dataframe.copy()\n",
    "    for col in time_df.columns.tolist():\n",
    "        time_df[col] = time_in\n",
    "    time_df[masked_df] = np.nan\n",
    "\n",
    "    # Concatenate the dataframes.\n",
    "    df_prime = pd.concat(\n",
    "        [dataframe, masked_df, time_df], axis=1, keys=[\"measurement\", \"mask\", \"time\"]\n",
    "    )\n",
    "    df_prime.columns = df_prime.columns.rename(\"impute_simple\", level=0)\n",
    "\n",
    "    # Fill each dataframe using either ffill or mean.\n",
    "    df_prime = df_prime.fillna(method=\"ffill\").unstack().fillna(0)\n",
    "\n",
    "    # Swap the levels so that the simple imputation feature is the lowest value.\n",
    "    col_level_names = list(df_prime.columns.names)\n",
    "    col_level_names.append(col_level_names.pop(0))\n",
    "\n",
    "    df_prime = df_prime.reorder_levels(col_level_names, axis=1)\n",
    "    df_prime.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    return df_prime\n",
    "\n",
    "\n",
    "def impute_simple_v2(mean_vals):\n",
    "    \"\"\"Another version of 'simple' imputation.\"\"\"\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    mask = 1 - mean_vals.isna()\n",
    "    measurement = mean_vals.copy()\n",
    "\n",
    "    subset_data = measurement.loc[idx[:, :, 0], :]\n",
    "    data_means = measurement.mean()\n",
    "    subset_data = subset_data.fillna(data_means)\n",
    "    measurement.loc[idx[:, :, 0], :] = subset_data.values\n",
    "    measurement = measurement.ffill()\n",
    "\n",
    "    is_absent = 1 - mask\n",
    "    hours_of_absence = is_absent.groupby([\"patient_id\", \"genc_id\"]).cumsum()\n",
    "    time_df = hours_of_absence - hours_of_absence[is_absent == 0].fillna(method=\"ffill\")\n",
    "    time_df = time_df.fillna(0)\n",
    "\n",
    "    final_data = pd.concat(\n",
    "        [measurement, mask, time_df], keys=[\"measurement\", \"mask\", \"time\"], axis=1\n",
    "    )\n",
    "    final_data.columns = final_data.columns.swaplevel(0, 1)\n",
    "    final_data.sort_index(axis=\"columns\", inplace=True)\n",
    "\n",
    "    nancols = 0\n",
    "    try:\n",
    "        nancols = np.sum(\n",
    "            [a == 0 for a in final_data.loc[:, idx[:, \"mask\"]].sum().values]\n",
    "        )\n",
    "        print(nancols)\n",
    "    except:\n",
    "        print(\"could not get nancols\")\n",
    "        pass\n",
    "\n",
    "    print(nancols, \"/\", len(sorted(set(final_data.columns.get_level_values(0)))))\n",
    "    return final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-KKtuQLwg-py3.9",
   "language": "python",
   "name": "cyclops-kktuqlwg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
