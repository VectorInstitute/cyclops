{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d995dcd3-fb7a-4398-b8b8-984c2ddc78be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 09:12:04,486 \u001b[1;37mINFO\u001b[0m cyclops.orm     - Database setup, ready to run queries!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from alibi_detect.cd import MMDDrift\n",
    "import random\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.utils import *\n",
    "from drift_detector.rolling_window import *\n",
    "from baseline_models.temporal.pytorch.optimizer import Optimizer\n",
    "from baseline_models.temporal.pytorch.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cff9cf-c682-4973-ab27-1161b70d516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/nfs/project/delirium/drift_exp/JULY-04-2022\"\n",
    "threshold=0.05\n",
    "num_timesteps = 6\n",
    "run=1\n",
    "shift=\"covid\"\n",
    "hospital = [\"SBK\", \"UHNTG\", \"THPC\", \"THPM\", \"UHNTW\", \"SMH\",\"MSH\",\"PMH\"]\n",
    "outcome=\"mortality\"\n",
    "aggregation_type=\"time\"\n",
    "scale=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93113177-c6d6-44af-9f74-0c043c781416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 09:12:04,539 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/JULY-04-2022/aggregated_events.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from aggregated events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 09:12:04,970 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/JULY-04-2022/aggmeta_start_ts.parquet\n",
      "2022-08-10 09:12:05,139 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Loading features from file...\n",
      "2022-08-10 09:12:05,143 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Found file to load for static features...\n",
      "2022-08-10 09:12:05,145 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Successfully loaded static features from file...\n",
      "2022-08-10 09:12:05,179 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Found file to load for temporal features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from feature handler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 09:12:10,451 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Successfully loaded temporal features from file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from admin data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 09:12:18,971 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/JULY-04-2022/aggmeta_end_ts.parquet\n",
      "2022-08-10 09:12:39,692 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Loading features from file...\n",
      "2022-08-10 09:12:39,694 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Found file to load for static features...\n",
      "2022-08-10 09:12:39,695 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Successfully loaded static features from file...\n",
      "2022-08-10 09:12:39,727 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Found file to load for temporal features...\n",
      "2022-08-10 09:12:44,481 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Successfully loaded temporal features from file...\n"
     ]
    }
   ],
   "source": [
    "admin_data, x, y = get_gemini_data(PATH)\n",
    "\n",
    "numerical_cols = get_numerical_cols(PATH)\n",
    "for col in numerical_cols:\n",
    "    scaler = StandardScaler().fit(x[col].values.reshape(-1, 1))\n",
    "    x[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(x[col].values.reshape(-1, 1))),\n",
    "        index=x[col].index,\n",
    "    )\n",
    "X = reshape_inputs(x, num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2094d958-185d-42f3-b3d1-eea9b91341e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test), feats, admin_data = import_dataset_hospital(admin_data, x, y, shift, outcome, hospital, run, shuffle=True)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Normalize data\n",
    "(X_tr_normalized, y_tr),(X_val_normalized, y_val), (X_t_normalized, y_t) = normalize_data(aggregation_type, admin_data, num_timesteps, x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "# Scale data\n",
    "if scale:\n",
    "    X_tr_normalized, X_val_normalized, X_t_normalized = scale_data(numerical_cols, X_tr_normalized, X_val_normalized, X_t_normalized)\n",
    "# Process data\n",
    "X_tr_final, X_val_final, X_t_final = process_data(aggregation_type, num_timesteps, X_tr_normalized, X_val_normalized, X_t_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a5be5-e6e5-48da-aa6a-94511b907a69",
   "metadata": {},
   "source": [
    "## Create Data Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950b8746-7262-489e-bd31-c4e381900525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 - 2019-01-02\n",
      "2020-01-01 - 2020-01-02\n"
     ]
    }
   ],
   "source": [
    "start_date = date(2019, 1, 1)\n",
    "end_date = date(2020, 8, 1)\n",
    "\n",
    "val_ids=list(X_val_normalized.index.get_level_values(0).unique())\n",
    "\n",
    "x_test_stream, y_test_stream, measure_dates_test = get_streams(x, y, admin_data, start_date, end_date, stride=1, window=1, ids_to_exclude=val_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b2730-50ba-44fb-9df5-8e7fcaa468cd",
   "metadata": {},
   "source": [
    "## Cumulating Rolling Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a4fa0d-5a1c-4288-bd4e-2f06ab9c2f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain  rnn  on:  0 - 40\n",
      "[1/1] Training loss: 0.7346\t Validation loss: 0.6383\n",
      "Retrain  rnn  on:  0 - 52\n",
      "[1/1] Training loss: 0.6654\t Validation loss: 0.6330\n",
      "Retrain  rnn  on:  0 - 55\n",
      "[1/1] Training loss: 0.6547\t Validation loss: 0.6305\n",
      "Retrain  rnn  on:  0 - 59\n",
      "[1/1] Training loss: 0.6531\t Validation loss: 0.6381\n",
      "Retrain  rnn  on:  0 - 60\n",
      "[1/1] Training loss: 0.6480\t Validation loss: 0.6362\n",
      "Retrain  rnn  on:  0 - 86\n",
      "[1/1] Training loss: 0.6560\t Validation loss: 0.6443\n",
      "Retrain  rnn  on:  0 - 87\n",
      "[1/1] Training loss: 0.6535\t Validation loss: 0.6436\n",
      "Retrain  rnn  on:  0 - 105\n",
      "[1/1] Training loss: 0.6486\t Validation loss: 0.6427\n",
      "Retrain  rnn  on:  0 - 242\n",
      "[1/1] Training loss: 0.6467\t Validation loss: 0.6393\n",
      "Retrain  rnn  on:  0 - 244\n",
      "[1/1] Training loss: 0.6387\t Validation loss: 0.6348\n",
      "Retrain  rnn  on:  0 - 245\n",
      "[1/1] Training loss: 0.6373\t Validation loss: 0.6333\n",
      "Retrain  rnn  on:  0 - 297\n",
      "[1/1] Training loss: 0.6346\t Validation loss: 0.6300\n",
      "Retrain  rnn  on:  0 - 300\n",
      "[1/1] Training loss: 0.6382\t Validation loss: 0.6339\n",
      "Retrain  rnn  on:  0 - 302\n",
      "[1/1] Training loss: 0.6329\t Validation loss: 0.6305\n",
      "Retrain  rnn  on:  0 - 348\n",
      "[1/1] Training loss: 0.6344\t Validation loss: 0.6313\n",
      "Retrain  rnn  on:  0 - 426\n",
      "[1/1] Training loss: 0.6322\t Validation loss: 0.6301\n",
      "Retrain  rnn  on:  0 - 442\n",
      "[1/1] Training loss: 0.6349\t Validation loss: 0.6324\n",
      "Retrain  rnn  on:  0 - 443\n",
      "[1/1] Training loss: 0.6332\t Validation loss: 0.6313\n",
      "Retrain  rnn  on:  0 - 444\n",
      "[1/1] Training loss: 0.6335\t Validation loss: 0.6313\n",
      "Retrain  rnn  on:  0 - 445\n",
      "[1/1] Training loss: 0.6332\t Validation loss: 0.6314\n",
      "Retrain  rnn  on:  0 - 446\n",
      "[1/1] Training loss: 0.6338\t Validation loss: 0.6321\n",
      "Retrain  rnn  on:  0 - 447\n",
      "[1/1] Training loss: 0.6330\t Validation loss: 0.6319\n",
      "Retrain  rnn  on:  0 - 448\n",
      "[1/1] Training loss: 0.6343\t Validation loss: 0.6325\n",
      "Retrain  rnn  on:  0 - 449\n",
      "[1/1] Training loss: 0.6332\t Validation loss: 0.6321\n",
      "Retrain  rnn  on:  0 - 450\n",
      "[1/1] Training loss: 0.6341\t Validation loss: 0.6326\n",
      "Retrain  rnn  on:  0 - 451\n",
      "[1/1] Training loss: 0.6341\t Validation loss: 0.6328\n",
      "Retrain  rnn  on:  0 - 452\n",
      "[1/1] Training loss: 0.6341\t Validation loss: 0.6328\n",
      "Retrain  rnn  on:  0 - 453\n",
      "[1/1] Training loss: 0.6343\t Validation loss: 0.6329\n",
      "Retrain  rnn  on:  0 - 454\n",
      "[1/1] Training loss: 0.6347\t Validation loss: 0.6332\n",
      "Retrain  rnn  on:  0 - 455\n",
      "[1/1] Training loss: 0.6336\t Validation loss: 0.6323\n",
      "Retrain  rnn  on:  0 - 456\n",
      "[1/1] Training loss: 0.6337\t Validation loss: 0.6326\n",
      "Retrain  rnn  on:  0 - 457\n",
      "[1/1] Training loss: 0.6341\t Validation loss: 0.6327\n",
      "Retrain  rnn  on:  0 - 458\n",
      "[1/1] Training loss: 0.6333\t Validation loss: 0.6328\n",
      "Retrain  rnn  on:  0 - 459\n",
      "[1/1] Training loss: 0.6344\t Validation loss: 0.6326\n",
      "Retrain  rnn  on:  0 - 460\n",
      "[1/1] Training loss: 0.6340\t Validation loss: 0.6334\n",
      "Retrain  rnn  on:  0 - 461\n",
      "[1/1] Training loss: 0.6340\t Validation loss: 0.6331\n",
      "Retrain  rnn  on:  0 - 462\n",
      "[1/1] Training loss: 0.6359\t Validation loss: 0.6342\n",
      "Retrain  rnn  on:  0 - 463\n",
      "[1/1] Training loss: 0.6349\t Validation loss: 0.6338\n",
      "Retrain  rnn  on:  0 - 464\n",
      "[1/1] Training loss: 0.6347\t Validation loss: 0.6337\n",
      "Retrain  rnn  on:  0 - 485\n",
      "[1/1] Training loss: 0.6412\t Validation loss: 0.6406\n",
      "Retrain  rnn  on:  0 - 487\n",
      "[1/1] Training loss: 0.6432\t Validation loss: 0.6425\n",
      "Retrain  rnn  on:  0 - 506\n",
      "[1/1] Training loss: 0.6424\t Validation loss: 0.6412\n",
      "Retrain  rnn  on:  0 - 507\n",
      "[1/1] Training loss: 0.6417\t Validation loss: 0.6408\n",
      "Retrain  rnn  on:  0 - 513\n",
      "[1/1] Training loss: 0.6415\t Validation loss: 0.6406\n",
      "Retrain  rnn  on:  0 - 514\n",
      "[1/1] Training loss: 0.6418\t Validation loss: 0.6410\n",
      "Retrain  rnn  on:  0 - 515\n",
      "[1/1] Training loss: 0.6413\t Validation loss: 0.6405\n",
      "Retrain  rnn  on:  0 - 516\n",
      "[1/1] Training loss: 0.6412\t Validation loss: 0.6403\n",
      "Retrain  rnn  on:  0 - 517\n",
      "[1/1] Training loss: 0.6408\t Validation loss: 0.6400\n",
      "Retrain  rnn  on:  0 - 518\n",
      "[1/1] Training loss: 0.6415\t Validation loss: 0.6408\n",
      "Retrain  rnn  on:  0 - 519\n",
      "[1/1] Training loss: 0.6422\t Validation loss: 0.6407\n",
      "Retrain  rnn  on:  0 - 520\n",
      "[1/1] Training loss: 0.6414\t Validation loss: 0.6405\n",
      "Retrain  rnn  on:  0 - 521\n",
      "[1/1] Training loss: 0.6416\t Validation loss: 0.6411\n",
      "Retrain  rnn  on:  0 - 522\n",
      "[1/1] Training loss: 0.6415\t Validation loss: 0.6406\n",
      "Retrain  rnn  on:  0 - 523\n",
      "[1/1] Training loss: 0.6420\t Validation loss: 0.6413\n",
      "Retrain  rnn  on:  0 - 524\n",
      "[1/1] Training loss: 0.6415\t Validation loss: 0.6408\n",
      "Retrain  rnn  on:  0 - 525\n",
      "[1/1] Training loss: 0.6412\t Validation loss: 0.6405\n",
      "Retrain  rnn  on:  0 - 526\n",
      "[1/1] Training loss: 0.6404\t Validation loss: 0.6397\n",
      "Retrain  rnn  on:  0 - 527\n",
      "[1/1] Training loss: 0.6412\t Validation loss: 0.6405\n",
      "Retrain  rnn  on:  0 - 528\n",
      "[1/1] Training loss: 0.6411\t Validation loss: 0.6403\n",
      "Retrain  rnn  on:  0 - 529\n",
      "[1/1] Training loss: 0.6410\t Validation loss: 0.6403\n",
      "Retrain  rnn  on:  0 - 530\n",
      "[1/1] Training loss: 0.6419\t Validation loss: 0.6411\n",
      "Retrain  rnn  on:  0 - 531\n",
      "[1/1] Training loss: 0.6426\t Validation loss: 0.6414\n",
      "Retrain  rnn  on:  0 - 532\n",
      "[1/1] Training loss: 0.6425\t Validation loss: 0.6419\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "\n",
    "# rolling window parameters\n",
    "threshold = 0.05\n",
    "num_timesteps = 6\n",
    "stat_window=30\n",
    "lookup_window=0\n",
    "stride=1\n",
    "# model parameters\n",
    "output_dim = 1\n",
    "batch_size = 64\n",
    "input_dim = 108\n",
    "timesteps = 6\n",
    "hidden_dim = 64\n",
    "layer_dim = 2\n",
    "dropout = 0.2\n",
    "n_epochs = 1\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 1e-6\n",
    "last_timestep_only = False\n",
    "device = get_device()\n",
    "#drift detector parameters\n",
    "dr_technique=\"BBSDs_trained_LSTM\"\n",
    "model_path=os.path.join(os.getcwd(),\"../../saved_models/\"+shift+\"_lstm.pt\")\n",
    "md_test=\"MMD\"\n",
    "sign_level=0.05\n",
    "sample=1000\n",
    "dataset=\"gemini\"\n",
    "context_type=\"lstm\"\n",
    "representation=\"rf\"\n",
    "\n",
    "shift_reductor = ShiftReductor(\n",
    "    X_tr_final, y_tr, dr_technique, dataset, var_ret=0.8, model_path=model_path,\n",
    ")\n",
    "# Get shift detector\n",
    "shift_detector = ShiftDetector(\n",
    "    dr_technique, md_test, sign_level, shift_reductor, sample, dataset, feats, model_path, context_type, representation,\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"device\": device,\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"layer_dim\": layer_dim,\n",
    "    \"output_dim\": output_dim,\n",
    "    \"dropout_prob\": dropout,\n",
    "    \"last_timestep_only\": last_timestep_only,\n",
    "}\n",
    "\n",
    "model = get_temporal_model(\"lstm\", model_params).to(device)\n",
    "model_path = os.path.join(os.getcwd(),'../../saved_models/',shift+\"_lstm.pt\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "optimizer = optim.Adagrad(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=128, gamma=0.5)\n",
    "activation = nn.Sigmoid()\n",
    "opt = Optimizer(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    activation=activation,\n",
    "    lr_scheduler=lr_scheduler,\n",
    ")\n",
    "def cumulative_rolling_window(X_train, X_stream, y_stream, shift_detector, sample, stat_window, lookup_window, stride, num_timesteps, threshold, model_name, model, opt=None, X_ref=None, y_ref=None, retrain=True):\n",
    "\n",
    "    p_vals = np.asarray([])\n",
    "    dist_vals = np.asarray([])\n",
    "    run_length = int(stat_window) \n",
    "    \n",
    "    i = stat_window\n",
    "    p_val=1\n",
    "    total_alarms = 0 \n",
    "    \n",
    "    if X_ref is not None:\n",
    "        X_prev = X_ref\n",
    "        # create val loader\n",
    "        \n",
    "    while i+stat_window+lookup_window <= len(X_stream):\n",
    "        feat_index = 0\n",
    "        \n",
    "        if p_val < threshold:\n",
    "            \n",
    "            if retrain:\n",
    "                X_update = pd.concat(X_stream[max(int(i)-run_length,0):int(i)])\n",
    "                X_update = X_update[~X_update.index.duplicated(keep='first')]\n",
    "                ind = X_update.index.get_level_values(0).unique()\n",
    "                X_update = reshape_inputs(X_update, num_timesteps)\n",
    "\n",
    "                ## Get updated source data for two-sample test (including data for retraining) \n",
    "                X_prev = np.concatenate((X_prev, X_update), axis=0)\n",
    "                tups = [tuple(row) for row in X_prev]\n",
    "                X_prev = np.unique(tups, axis=0)\n",
    "                np.random.shuffle(X_prev)                 \n",
    "\n",
    "                y_update = pd.concat(y_stream[max(int(i)-run_length,0):int(i)])\n",
    "                y_update.index = ind\n",
    "                y_update = y_update[~y_update.index.duplicated(keep='first')].to_numpy()\n",
    "\n",
    "                print(\"Retrain \",model_name,\" on: \",max(int(i)-run_length,0),\"-\",int(i))\n",
    "\n",
    "                if model_name == \"rnn\":\n",
    "                    ## create train loader \n",
    "                    update_dataset = get_data(X_update, y_update)\n",
    "                    update_loader = torch.utils.data.DataLoader(update_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "                    retrain_model_path='adaptive_window_retrain.model'\n",
    "\n",
    "                    ## train \n",
    "                    opt.train(\n",
    "                         update_loader,\n",
    "                         update_loader,\n",
    "                         batch_size=batch_size,\n",
    "                         n_epochs=n_epochs,\n",
    "                         n_features=input_dim,\n",
    "                         timesteps=timesteps,\n",
    "                         model_path=retrain_model_path,\n",
    "                    )\n",
    "\n",
    "                    model.load_state_dict(torch.load(retrain_model_path))\n",
    "                    opt.model = model\n",
    "                    shift_detector.model_path = retrain_model_path\n",
    "\n",
    "                elif model_name == \"gbt\":\n",
    "                    model = model.fit(X_retrain, y_retrain, xgb_model=model.get_booster())\n",
    "\n",
    "                else:\n",
    "                    print(\"Invalid Model Name\")\n",
    "\n",
    "        if X_ref is None:\n",
    "            X_prev = pd.concat(X_stream[max(int(i)-run_length,0):int(i)+stat_window])\n",
    "            X_prev = X_prev[~X_prev.index.duplicated(keep='first')]\n",
    "            X_prev = reshape_inputs(X_prev, num_timesteps)\n",
    "            #prev = prev.reshape(prev.shape[0]*prev.shape[1],prev.shape[2])\n",
    "            \n",
    "        X_next = pd.concat(X_stream[max(int(i)+lookup_window,0):int(i)+stat_window+lookup_window])\n",
    "        X_next = X_next[~X_next.index.duplicated(keep='first')]\n",
    "        X_next = reshape_inputs(X_next, num_timesteps)\n",
    "        \n",
    "        if X_next.shape[0]<=2 or X_prev.shape[0]<=2:\n",
    "            break\n",
    "            \n",
    "        ## run distribution shift check here\n",
    "        (p_val, dist, val_acc, te_acc) = shift_detector.detect_data_shift(X_train, \n",
    "                                                                          X_prev[:1000,:], \n",
    "                                                                          X_next[:sample,:]\n",
    "        )\n",
    "\n",
    "        #print(\"Ref -->\",max(int(i)+lookup_window,0),\"-\",int(i)+stat_window+lookup_window,\"\\tP-Value: \",p_val)\n",
    "\n",
    "        dist_vals = np.concatenate((dist_vals, np.repeat(dist, 1)))\n",
    "        p_vals = np.concatenate((p_vals, np.repeat(p_val, 1)))\n",
    "        i += stride\n",
    "        run_length += stat_window\n",
    "        \n",
    "        if p_val < threshold:\n",
    "            total_alarms += 1\n",
    "    \n",
    "    return dist_vals, p_vals, total_alarms\n",
    "\n",
    "dist_test, pvals_test, total_alarms = cumulative_rolling_window(X_tr_final, x_test_stream, y_test_stream, shift_detector, sample, stat_window, lookup_window, stride, num_timesteps, threshold, model_name=\"rnn\", model=model,opt=opt, X_ref=X_val_final, retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01e30835-d800-4a78-ba58-3f08e98d5b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain  rnn  on:  9 - 39\n",
      "[1/1] Training loss: 0.8294\t Validation loss: 0.6653\n",
      "Retrain  rnn  on:  23 - 53\n",
      "[1/1] Training loss: 0.6793\t Validation loss: 0.6444\n",
      "Retrain  rnn  on:  24 - 54\n",
      "[1/1] Training loss: 0.6573\t Validation loss: 0.6341\n",
      "Retrain  rnn  on:  25 - 55\n",
      "[1/1] Training loss: 0.6593\t Validation loss: 0.6403\n",
      "Retrain  rnn  on:  26 - 56\n",
      "[1/1] Training loss: 0.6516\t Validation loss: 0.6319\n",
      "Retrain  rnn  on:  29 - 59\n",
      "[1/1] Training loss: 0.6313\t Validation loss: 0.6165\n",
      "Retrain  rnn  on:  44 - 74\n",
      "[1/1] Training loss: 0.6506\t Validation loss: 0.6374\n",
      "Retrain  rnn  on:  54 - 84\n",
      "[1/1] Training loss: 0.6827\t Validation loss: 0.6757\n",
      "Retrain  rnn  on:  55 - 85\n",
      "[1/1] Training loss: 0.6806\t Validation loss: 0.6621\n",
      "Retrain  rnn  on:  56 - 86\n",
      "[1/1] Training loss: 0.6815\t Validation loss: 0.6657\n",
      "Retrain  rnn  on:  78 - 108\n",
      "[1/1] Training loss: 0.6748\t Validation loss: 0.6619\n",
      "Retrain  rnn  on:  209 - 239\n",
      "[1/1] Training loss: 0.6639\t Validation loss: 0.6552\n",
      "Retrain  rnn  on:  211 - 241\n",
      "[1/1] Training loss: 0.6578\t Validation loss: 0.6492\n",
      "Retrain  rnn  on:  212 - 242\n",
      "[1/1] Training loss: 0.6691\t Validation loss: 0.6560\n",
      "Retrain  rnn  on:  213 - 243\n",
      "[1/1] Training loss: 0.6733\t Validation loss: 0.6614\n",
      "Retrain  rnn  on:  214 - 244\n",
      "[1/1] Training loss: 0.6547\t Validation loss: 0.6497\n",
      "Retrain  rnn  on:  266 - 296\n",
      "[1/1] Training loss: 0.6495\t Validation loss: 0.6417\n",
      "Retrain  rnn  on:  267 - 297\n",
      "[1/1] Training loss: 0.6432\t Validation loss: 0.6375\n",
      "Retrain  rnn  on:  268 - 298\n",
      "[1/1] Training loss: 0.6576\t Validation loss: 0.6472\n",
      "Retrain  rnn  on:  317 - 347\n",
      "[1/1] Training loss: 0.6471\t Validation loss: 0.6412\n",
      "Retrain  rnn  on:  320 - 350\n",
      "[1/1] Training loss: 0.6472\t Validation loss: 0.6415\n",
      "Retrain  rnn  on:  321 - 351\n",
      "[1/1] Training loss: 0.6475\t Validation loss: 0.6419\n",
      "Retrain  rnn  on:  322 - 352\n",
      "[1/1] Training loss: 0.6305\t Validation loss: 0.6271\n",
      "Retrain  rnn  on:  328 - 358\n",
      "[1/1] Training loss: 0.6493\t Validation loss: 0.6390\n",
      "Retrain  rnn  on:  329 - 359\n",
      "[1/1] Training loss: 0.6400\t Validation loss: 0.6328\n",
      "Retrain  rnn  on:  412 - 442\n",
      "[1/1] Training loss: 0.6655\t Validation loss: 0.6598\n",
      "Retrain  rnn  on:  413 - 443\n",
      "[1/1] Training loss: 0.6625\t Validation loss: 0.6580\n",
      "Retrain  rnn  on:  414 - 444\n",
      "[1/1] Training loss: 0.6570\t Validation loss: 0.6539\n",
      "Retrain  rnn  on:  415 - 445\n",
      "[1/1] Training loss: 0.6542\t Validation loss: 0.6510\n",
      "Retrain  rnn  on:  416 - 446\n",
      "[1/1] Training loss: 0.6689\t Validation loss: 0.6627\n",
      "Retrain  rnn  on:  417 - 447\n",
      "[1/1] Training loss: 0.6671\t Validation loss: 0.6605\n",
      "Retrain  rnn  on:  418 - 448\n",
      "[1/1] Training loss: 0.6749\t Validation loss: 0.6705\n",
      "Retrain  rnn  on:  419 - 449\n",
      "[1/1] Training loss: 0.6737\t Validation loss: 0.6701\n",
      "Retrain  rnn  on:  420 - 450\n",
      "[1/1] Training loss: 0.6811\t Validation loss: 0.6782\n",
      "Retrain  rnn  on:  421 - 451\n",
      "[1/1] Training loss: 0.6858\t Validation loss: 0.6831\n",
      "Retrain  rnn  on:  422 - 452\n",
      "[1/1] Training loss: 0.6795\t Validation loss: 0.6760\n",
      "Retrain  rnn  on:  423 - 453\n",
      "[1/1] Training loss: 0.6826\t Validation loss: 0.6777\n",
      "Retrain  rnn  on:  424 - 454\n",
      "[1/1] Training loss: 0.6952\t Validation loss: 0.6876\n",
      "Retrain  rnn  on:  425 - 455\n",
      "[1/1] Training loss: 0.6899\t Validation loss: 0.6830\n",
      "Retrain  rnn  on:  426 - 456\n",
      "[1/1] Training loss: 0.6800\t Validation loss: 0.6734\n",
      "Retrain  rnn  on:  427 - 457\n",
      "[1/1] Training loss: 0.6873\t Validation loss: 0.6838\n",
      "Retrain  rnn  on:  428 - 458\n",
      "[1/1] Training loss: 0.6906\t Validation loss: 0.6876\n",
      "Retrain  rnn  on:  454 - 484\n",
      "[1/1] Training loss: 0.7154\t Validation loss: 0.7039\n",
      "Retrain  rnn  on:  455 - 485\n",
      "[1/1] Training loss: 0.7145\t Validation loss: 0.7079\n",
      "Retrain  rnn  on:  456 - 486\n",
      "[1/1] Training loss: 0.7059\t Validation loss: 0.7006\n",
      "Retrain  rnn  on:  457 - 487\n",
      "[1/1] Training loss: 0.7040\t Validation loss: 0.6992\n",
      "Retrain  rnn  on:  458 - 488\n",
      "[1/1] Training loss: 0.6985\t Validation loss: 0.6913\n",
      "Retrain  rnn  on:  466 - 496\n",
      "[1/1] Training loss: 0.6990\t Validation loss: 0.6938\n",
      "Retrain  rnn  on:  469 - 499\n",
      "[1/1] Training loss: 0.6902\t Validation loss: 0.6829\n",
      "Retrain  rnn  on:  473 - 503\n",
      "[1/1] Training loss: 0.6844\t Validation loss: 0.6796\n",
      "Retrain  rnn  on:  474 - 504\n",
      "[1/1] Training loss: 0.6757\t Validation loss: 0.6732\n",
      "Retrain  rnn  on:  475 - 505\n",
      "[1/1] Training loss: 0.6806\t Validation loss: 0.6788\n",
      "Retrain  rnn  on:  476 - 506\n",
      "[1/1] Training loss: 0.6774\t Validation loss: 0.6736\n",
      "Retrain  rnn  on:  477 - 507\n",
      "[1/1] Training loss: 0.6666\t Validation loss: 0.6649\n",
      "Retrain  rnn  on:  478 - 508\n",
      "[1/1] Training loss: 0.6496\t Validation loss: 0.6469\n",
      "Retrain  rnn  on:  479 - 509\n",
      "[1/1] Training loss: 0.6663\t Validation loss: 0.6582\n",
      "Retrain  rnn  on:  481 - 511\n",
      "[1/1] Training loss: 0.6595\t Validation loss: 0.6562\n",
      "Retrain  rnn  on:  482 - 512\n",
      "[1/1] Training loss: 0.6588\t Validation loss: 0.6540\n",
      "Retrain  rnn  on:  483 - 513\n",
      "[1/1] Training loss: 0.6657\t Validation loss: 0.6606\n",
      "Retrain  rnn  on:  484 - 514\n",
      "[1/1] Training loss: 0.6614\t Validation loss: 0.6589\n",
      "Retrain  rnn  on:  485 - 515\n",
      "[1/1] Training loss: 0.6576\t Validation loss: 0.6519\n",
      "Retrain  rnn  on:  486 - 516\n",
      "[1/1] Training loss: 0.6522\t Validation loss: 0.6495\n",
      "Retrain  rnn  on:  487 - 517\n",
      "[1/1] Training loss: 0.6438\t Validation loss: 0.6403\n",
      "Retrain  rnn  on:  488 - 518\n",
      "[1/1] Training loss: 0.6468\t Validation loss: 0.6447\n",
      "Retrain  rnn  on:  489 - 519\n",
      "[1/1] Training loss: 0.6572\t Validation loss: 0.6514\n",
      "Retrain  rnn  on:  490 - 520\n",
      "[1/1] Training loss: 0.6580\t Validation loss: 0.6537\n",
      "Retrain  rnn  on:  491 - 521\n",
      "[1/1] Training loss: 0.6510\t Validation loss: 0.6512\n",
      "Retrain  rnn  on:  492 - 522\n",
      "[1/1] Training loss: 0.6646\t Validation loss: 0.6676\n",
      "Retrain  rnn  on:  493 - 523\n",
      "[1/1] Training loss: 0.6717\t Validation loss: 0.6684\n",
      "Retrain  rnn  on:  494 - 524\n",
      "[1/1] Training loss: 0.6760\t Validation loss: 0.6744\n",
      "Retrain  rnn  on:  495 - 525\n",
      "[1/1] Training loss: 0.6605\t Validation loss: 0.6598\n",
      "Retrain  rnn  on:  496 - 526\n",
      "[1/1] Training loss: 0.6630\t Validation loss: 0.6600\n",
      "Retrain  rnn  on:  497 - 527\n",
      "[1/1] Training loss: 0.6595\t Validation loss: 0.6529\n",
      "Retrain  rnn  on:  498 - 528\n",
      "[1/1] Training loss: 0.6666\t Validation loss: 0.6627\n",
      "Retrain  rnn  on:  499 - 529\n",
      "[1/1] Training loss: 0.6741\t Validation loss: 0.6663\n",
      "Retrain  rnn  on:  500 - 530\n",
      "[1/1] Training loss: 0.6749\t Validation loss: 0.6716\n",
      "Retrain  rnn  on:  501 - 531\n",
      "[1/1] Training loss: 0.6747\t Validation loss: 0.6728\n",
      "Retrain  rnn  on:  502 - 532\n",
      "[1/1] Training loss: 0.6734\t Validation loss: 0.6699\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "\n",
    "# rolling window parameters\n",
    "threshold = 0.05\n",
    "num_timesteps = 6\n",
    "stat_window=30\n",
    "lookup_window=0\n",
    "stride=1\n",
    "# model parameters\n",
    "output_dim = 1\n",
    "batch_size = 64\n",
    "input_dim = 108\n",
    "timesteps = 6\n",
    "hidden_dim = 64\n",
    "layer_dim = 2\n",
    "dropout = 0.2\n",
    "n_epochs = 1\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 1e-6\n",
    "last_timestep_only = False\n",
    "device = get_device()\n",
    "#drift detector parameters\n",
    "dr_technique=\"BBSDs_trained_LSTM\"\n",
    "model_path=os.path.join(os.getcwd(),\"../../saved_models/\"+shift+\"_lstm.pt\")\n",
    "md_test=\"MMD\"\n",
    "sign_level=0.05\n",
    "sample=1000\n",
    "dataset=\"gemini\"\n",
    "context_type=\"lstm\"\n",
    "representation=\"rf\"\n",
    "\n",
    "shift_reductor = ShiftReductor(\n",
    "    X_tr_final, y_tr, dr_technique, dataset, var_ret=0.8, model_path=model_path,\n",
    ")\n",
    "# Get shift detector\n",
    "shift_detector = ShiftDetector(\n",
    "    dr_technique, md_test, sign_level, shift_reductor, sample, dataset, feats, model_path, context_type, representation,\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"device\": device,\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"layer_dim\": layer_dim,\n",
    "    \"output_dim\": output_dim,\n",
    "    \"dropout_prob\": dropout,\n",
    "    \"last_timestep_only\": last_timestep_only,\n",
    "}\n",
    "\n",
    "model = get_temporal_model(\"lstm\", model_params).to(device)\n",
    "model_path = os.path.join(os.getcwd(),'../../saved_models/',shift+\"_lstm.pt\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "optimizer = optim.Adagrad(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=128, gamma=0.5)\n",
    "activation = nn.Sigmoid()\n",
    "opt = Optimizer(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    activation=activation,\n",
    "    lr_scheduler=lr_scheduler,\n",
    ")\n",
    "\n",
    "#####################################################\n",
    "## dynamically adjusting drift detector - if drift is significant, reference dataset is reset to current time \n",
    "#####################################################\n",
    "def dynamic_rolling_window(X_train, X_stream, y_stream, shift_detector, sample, stat_window, lookup_window, stride, num_timesteps, threshold, model_name, model, opt=None, X_ref=None, retrain=True):\n",
    "\n",
    "    p_vals = np.asarray([])\n",
    "    dist_vals = np.asarray([])\n",
    "    run_length = int(stat_window) \n",
    "    \n",
    "    i = stat_window\n",
    "    p_val=1\n",
    "    total_alarms = 0 \n",
    "    \n",
    "    if X_ref is not None:\n",
    "        X_prev = X_ref\n",
    "        # create val loader\n",
    "        \n",
    "    while i+stat_window+lookup_window <= len(X_stream):\n",
    "        feat_index = 0\n",
    "        \n",
    "        if p_val < threshold:\n",
    "            \n",
    "            if retrain:\n",
    "                X_update = pd.concat(X_stream[max(int(i)-run_length,0):int(i)])\n",
    "                X_update = X_update[~X_update.index.duplicated(keep='first')]\n",
    "                ind = X_update.index.get_level_values(0).unique()\n",
    "                X_update = reshape_inputs(X_update, num_timesteps)\n",
    "\n",
    "                ## Get updated source data for two-sample test (including data for retraining) \n",
    "                X_prev = np.concatenate((X_prev, X_update), axis=0)\n",
    "                tups = [tuple(row) for row in X_prev]\n",
    "                X_prev = np.unique(tups, axis=0)\n",
    "                np.random.shuffle(X_prev)                 \n",
    "\n",
    "                y_update = pd.concat(y_stream[max(int(i)-run_length,0):int(i)])\n",
    "                y_update.index = ind\n",
    "                y_update = y_update[~y_update.index.duplicated(keep='first')].to_numpy()\n",
    "\n",
    "                print(\"Retrain \",model_name,\" on: \",max(int(i)-run_length,0),\"-\",int(i))\n",
    "\n",
    "                if model_name == \"rnn\":\n",
    "                    ## create train loader \n",
    "                    update_dataset = get_data(X_update, y_update)\n",
    "                    update_loader = torch.utils.data.DataLoader(update_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "                    retrain_model_path='adaptive_window_retrain.model'\n",
    "\n",
    "                    ## train \n",
    "                    opt.train(\n",
    "                         update_loader,\n",
    "                         update_loader,\n",
    "                         batch_size=batch_size,\n",
    "                         n_epochs=n_epochs,\n",
    "                         n_features=input_dim,\n",
    "                         timesteps=timesteps,\n",
    "                         model_path=retrain_model_path,\n",
    "                    )\n",
    "\n",
    "                    model.load_state_dict(torch.load(retrain_model_path))\n",
    "                    opt.model = model\n",
    "                    shift_detector.model_path = retrain_model_path\n",
    "\n",
    "                elif model_name == \"gbt\":\n",
    "                    model = model.fit(X_retrain, y_retrain, xgb_model=model.get_booster())\n",
    "\n",
    "                else:\n",
    "                    print(\"Invalid Model Name\")\n",
    "\n",
    "            i += stride\n",
    "\n",
    "        if X_ref is None:\n",
    "            X_prev = pd.concat(X_stream[max(int(i)-run_length,0):int(i)+stat_window])\n",
    "            X_prev = X_prev[~X_prev.index.duplicated(keep='first')]\n",
    "            X_prev = reshape_inputs(X_prev, num_timesteps)\n",
    "            #prev = prev.reshape(prev.shape[0]*prev.shape[1],prev.shape[2])\n",
    "            \n",
    "        X_next = pd.concat(X_stream[max(int(i)+lookup_window,0):int(i)+stat_window+lookup_window])\n",
    "        X_next = X_next[~X_next.index.duplicated(keep='first')]\n",
    "        X_next = reshape_inputs(X_next, num_timesteps)\n",
    "        \n",
    "        if X_next.shape[0]<=2 or X_prev.shape[0]<=2:\n",
    "            break\n",
    "            \n",
    "        ## run distribution shift check here\n",
    "        (p_val, dist, val_acc, te_acc) = shift_detector.detect_data_shift(X_train, \n",
    "                                                                          X_prev[:1000,:], \n",
    "                                                                          X_next[:sample,:]\n",
    "        )\n",
    "\n",
    "    #    print(max(int(i)-run_length,0),\"-\", int(i),\"-->\",max(int(i)+lookup_window,0),\"-\",int(i)+stat_window+lookup_window,\"\\tP-Value: \",p_val)\n",
    "        \n",
    "        dist_vals = np.concatenate((dist_vals, np.repeat(dist, 1)))\n",
    "        p_vals = np.concatenate((p_vals, np.repeat(p_val, 1)))\n",
    "\n",
    "        if p_val >= threshold:\n",
    "            run_length += stride\n",
    "            i += stride\n",
    "        else:\n",
    "            run_length= stat_window\n",
    "            total_alarms += 1\n",
    "    \n",
    "    return dist_vals, p_vals, total_alarms\n",
    "\n",
    "dist_test, pvals_test, total_alarms = dynamic_rolling_window(X_tr_final, x_test_stream, y_test_stream, shift_detector, sample, stat_window, lookup_window, stride, num_timesteps, threshold, model_name=\"rnn\", model=model,opt=opt, X_ref=X_val_final, retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea9d11b4-6910-4064-b447-7e216a97735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f8b29a-b947-4b5f-be9f-477882b7b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010327868621613159 (0.006767784025553591, 0.013887953217672727)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "mean = np.mean(pvals_test[pvals_test<0.05])\n",
    "ci = st.t.interval(0.95, len(pvals_test[pvals_test<0.05])-1, loc=np.mean(pvals_test[pvals_test<0.05]), scale=st.sem(pvals_test[pvals_test<0.05]))\n",
    "print(mean, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa8e7c42-7214-45f2-85d7-bda7e76febec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2a618c5-5422-401e-ae29-d84c30c24933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015063290802549712 (0.011817926154194571, 0.018308655450904855)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "mean = np.mean(pvals_test[pvals_test<0.05])\n",
    "ci = st.t.interval(0.95, len(pvals_test[pvals_test<0.05])-1, loc=np.mean(pvals_test[pvals_test<0.05]), scale=st.sem(pvals_test[pvals_test<0.05]))\n",
    "print(mean, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f77ec4-918d-4c91-94bd-ee5cac93df94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84f4edd0-7859-49ad-b203-9658e2460252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014687499671708792 (0.011100127117103094, 0.018274872226314492)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "mean = np.mean(pvals_test[pvals_test<0.05])\n",
    "ci = st.t.interval(0.95, len(pvals_test[pvals_test<0.05])-1, loc=np.mean(pvals_test[pvals_test<0.05]), scale=st.sem(pvals_test[pvals_test<0.05]))\n",
    "print(mean, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a662a-f68e-454d-ae30-33c4acafc703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-KKtuQLwg-py3.9",
   "language": "python",
   "name": "cyclops-kktuqlwg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
