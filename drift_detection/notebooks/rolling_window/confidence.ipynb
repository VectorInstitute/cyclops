{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995dcd3-fb7a-4398-b8b8-984c2ddc78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from gemini.utils import *\n",
    "from drift_detector.rolling_window import *\n",
    "from baseline_models.temporal.pytorch.optimizer import Optimizer\n",
    "from baseline_models.temporal.pytorch.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cff9cf-c682-4973-ab27-1161b70d516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/nfs/project/delirium/drift_exp/JULY-04-2022\"\n",
    "threshold=0.05\n",
    "num_timesteps = 6\n",
    "run=1\n",
    "shift=\"covid\"\n",
    "hospital = [\"SBK\", \"UHNTG\", \"THPC\", \"THPM\", \"UHNTW\", \"SMH\",\"MSH\",\"PMH\"]\n",
    "outcome=\"mortality\"\n",
    "aggregation_type=\"time\"\n",
    "scale=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93113177-c6d6-44af-9f74-0c043c781416",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_data, x, y = get_gemini_data(PATH)\n",
    "x = scale_temporal_data(x)\n",
    "X = reshape_inputs(x, num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d91c48-3b72-4510-bfbe-82f70193090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test), feats, admin_data = import_dataset_hospital(admin_data, x, y, shift, outcome, hospital, run, shuffle=True)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Normalize data\n",
    "(X_tr_normalized, y_tr),(X_val_normalized, y_val), (X_t_normalized, y_t) = normalize_data(aggregation_type, admin_data, num_timesteps, x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "# Scale data\n",
    "if scale:\n",
    "    X_tr_normalized, X_val_normalized, X_t_normalized = scale_data(numerical_cols, X_tr_normalized, X_val_normalized, X_t_normalized)\n",
    "# Process data\n",
    "X_tr_final, X_val_final, X_t_final = process_data(aggregation_type, num_timesteps, X_tr_normalized, X_val_normalized, X_t_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a5be5-e6e5-48da-aa6a-94511b907a69",
   "metadata": {},
   "source": [
    "## Create Data Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b8746-7262-489e-bd31-c4e381900525",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2019, 1, 1)\n",
    "end_date = date(2020, 8, 1)\n",
    "\n",
    "val_ids=list(X_val_normalized.index.get_level_values(0).unique())\n",
    "\n",
    "x_test_stream, y_test_stream, measure_dates_test = get_streams(x, y, admin_data, start_date, end_date, stride=1, window=1, ids_to_exclude=val_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b18be-663a-42fc-a4c6-056e2be2fe7a",
   "metadata": {},
   "source": [
    "## Dynamic Rolling Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f657a-4e2b-4933-ad74-c91bd69b9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "# rolling window parameters\n",
    "threshold = 0.05\n",
    "num_timesteps = 6\n",
    "stat_window=30\n",
    "lookup_window=0\n",
    "stride=1\n",
    "# model parameters\n",
    "output_dim = 1\n",
    "batch_size = 64\n",
    "input_dim = 108\n",
    "timesteps = 6\n",
    "hidden_dim = 64\n",
    "layer_dim = 2\n",
    "dropout = 0.2\n",
    "n_epochs = 1\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 1e-6\n",
    "last_timestep_only = False\n",
    "device = get_device()\n",
    "model_params = {\n",
    "    \"device\": device,\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"layer_dim\": layer_dim,\n",
    "    \"output_dim\": output_dim,\n",
    "    \"dropout_prob\": dropout,\n",
    "    \"last_timestep_only\": last_timestep_only,\n",
    "}\n",
    "#drift detector parameters\n",
    "dr_technique=\"BBSDs_trained_LSTM\"\n",
    "model_path=os.path.join(os.getcwd(),\"../../saved_models/\"+shift+\"_lstm.pt\")\n",
    "md_test=\"MMD\"\n",
    "sign_level=0.05\n",
    "sample=1000\n",
    "dataset=\"gemini\"\n",
    "context_type=\"rnn\"\n",
    "representation=\"rf\"\n",
    "\n",
    "#retrain parameters\n",
    "model_name=\"rnn\"\n",
    "retrain=True\n",
    "\n",
    "# Get shift reductor\n",
    "shift_reductor = ShiftReductor(\n",
    "    X_tr_final, y_tr, dr_technique, dataset, var_ret=0.8, model_path=model_path,\n",
    ")\n",
    "# Get shift detector\n",
    "shift_detector = ShiftDetector(\n",
    "    dr_technique, md_test, sign_level, shift_reductor, sample, dataset, feats, model_path, context_type, representation,\n",
    ")\n",
    "\n",
    "if model_name == \"rnn\":\n",
    "    model = get_temporal_model(\"lstm\", model_params).to(device)\n",
    "    model_path = os.path.join(os.getcwd(),'../../saved_models/',shift+\"_lstm.pt\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    optimizer = optim.Adagrad(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=128, gamma=0.5)\n",
    "    activation = nn.Sigmoid()\n",
    "    opt = Optimizer(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        activation=activation,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "    )\n",
    "if model_name == \"gbt\":\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)    \n",
    "\n",
    "#####################################################\n",
    "## low confidence adjusting drift detector - if drift is significant, reference dataset is reset to current time \n",
    "#####################################################\n",
    "def confidence_rolling_window(X_train, X_stream, y_stream, shift_detector, sample, stat_window, lookup_window, stride, num_timesteps, threshold, model_name, model, opt=None, X_ref=None, retrain=True):\n",
    "\n",
    "    p_vals = np.asarray([])\n",
    "    dist_vals = np.asarray([])\n",
    "    rolling_metrics = []\n",
    "    run_length = int(stat_window) \n",
    "    \n",
    "    i = stat_window\n",
    "    p_val=1\n",
    "    total_alarms = 0 \n",
    "    verbose=0\n",
    "    \n",
    "    if X_ref is not None:\n",
    "        X_prev = X_ref\n",
    "        # create val loader\n",
    "        \n",
    "    while i+stat_window+lookup_window <= len(X_stream):\n",
    "        feat_index = 0\n",
    "        \n",
    "        if p_val < threshold:\n",
    "            \n",
    "            if retrain:\n",
    "                \n",
    "                ## Get data for updated fit \n",
    "                X_update = pd.concat(X_stream[max(int(i)-run_length,0):int(i)])\n",
    "                X_update = X_update[~X_update.index.duplicated(keep='first')]\n",
    "                ind = X_update.index.get_level_values(0).unique()\n",
    "                X_update = reshape_inputs(X_update, num_timesteps)\n",
    "                \n",
    "                y_update = pd.concat(y_stream[max(int(i)-run_length,0):int(i)])\n",
    "                y_update.index = ind\n",
    "                y_update = y_update[~y_update.index.duplicated(keep='first')].to_numpy()\n",
    "\n",
    "                ## Get updated source (validation) data for two-sample test (including data for retraining) \n",
    "                X_prev = np.concatenate((X_prev, X_update), axis=0)\n",
    "                tups = [tuple(row) for row in X_prev]\n",
    "                X_prev = np.unique(tups, axis=0)\n",
    "                np.random.shuffle(X_prev)                 \n",
    "\n",
    "                \n",
    "                print(\"Retrain \",model_name,\" on: \",max(int(i)-run_length,0),\"-\",int(i))\n",
    "\n",
    "                if model_name == \"rnn\":\n",
    "                    ## create train loader \n",
    "                    update_dataset = get_data(X_update, y_update)\n",
    "                    update_loader = torch.utils.data.DataLoader(update_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "                    retrain_model_path='adaptive_window_retrain.model'\n",
    "\n",
    "                    ## train \n",
    "                    opt.train(\n",
    "                         update_loader,\n",
    "                         update_loader,\n",
    "                         batch_size=batch_size,\n",
    "                         n_epochs=n_epochs,\n",
    "                         n_features=input_dim,\n",
    "                         timesteps=timesteps,\n",
    "                         model_path=retrain_model_path,\n",
    "                    )\n",
    "\n",
    "                    model.load_state_dict(torch.load(retrain_model_path))\n",
    "                    opt.model = model\n",
    "                    shift_detector.model_path = retrain_model_path\n",
    "\n",
    "                elif model_name == \"gbt\":\n",
    "                    model = model.fit(X_retrain, y_retrain, xgb_model=model.get_booster())\n",
    "\n",
    "                else:\n",
    "                    print(\"Invalid Model Name\")\n",
    "\n",
    "            i += stride\n",
    "\n",
    "        if X_ref is None:\n",
    "            X_prev = pd.concat(X_stream[max(int(i)-run_length,0):int(i)+stat_window])\n",
    "            X_prev = X_prev[~X_prev.index.duplicated(keep='first')]\n",
    "            X_prev = reshape_inputs(X_prev, num_timesteps)\n",
    "        \n",
    "        ## Get next stream of test data\n",
    "        X_next = pd.concat(X_stream[max(int(i)+lookup_window,0):int(i)+stat_window+lookup_window])\n",
    "        X_next = X_next[~X_next.index.duplicated(keep='first')]\n",
    "        next_ind = X_next.index.get_level_values(0).unique()\n",
    "        X_next = reshape_inputs(X_next, num_timesteps)\n",
    "        \n",
    "        y_next = pd.concat(y_stream[max(int(i)+lookup_window,0):int(i)+stat_window+lookup_window])\n",
    "        y_next.index = next_ind\n",
    "        y_next = y_next[~y_next.index.duplicated(keep='first')].to_numpy()\n",
    "        \n",
    "        ## Ensure next stream of test data is not empty\n",
    "        if X_next.shape[0]<=2 or X_prev.shape[0]<=2:\n",
    "            break\n",
    "        \n",
    "        ## Check Performance \n",
    "        test_dataset = get_data(X_next, y_next)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "        y_test_labels, y_pred_values, y_pred_labels = opt.evaluate(\n",
    "            test_loader, batch_size=1, n_features=input_dim, timesteps=num_timesteps\n",
    "        )\n",
    "\n",
    "        y_pred_values = y_pred_values[y_test_labels != -1]\n",
    "        y_pred_labels = y_pred_labels[y_test_labels != -1]\n",
    "        y_test_labels = y_test_labels[y_test_labels != -1]\n",
    "        \n",
    "       # print(y_pred_values)\n",
    "            \n",
    "        pred_metrics = print_metrics_binary(y_test_labels, y_pred_values, y_pred_labels, verbose=verbose)\n",
    "        rolling_metrics.append(pd.DataFrame(pred_metrics.values(),index=pred_metrics.keys()).T)\n",
    "        \n",
    "            \n",
    "        ## Run distribution shift check here\n",
    "        (p_val, dist, val_acc, te_acc) = shift_detector.detect_data_shift(X_train, \n",
    "                                                                          X_prev[:1000,:], \n",
    "                                                                          X_next[:sample,:]\n",
    "        )\n",
    "\n",
    "    #    print(max(int(i)-run_length,0),\"-\", int(i),\"-->\",max(int(i)+lookup_window,0),\"-\",int(i)+stat_window+lookup_window,\"\\tP-Value: \",p_val)\n",
    "        \n",
    "        dist_vals = np.concatenate((dist_vals, np.repeat(dist, 1)))\n",
    "        p_vals = np.concatenate((p_vals, np.repeat(p_val, 1)))\n",
    "\n",
    "        if p_val >= threshold:\n",
    "            run_length += stride\n",
    "            i += stride\n",
    "        else:\n",
    "            run_length= stat_window\n",
    "            total_alarms += 1\n",
    "    \n",
    "    rolling_metrics = pd.concat(rolling_metrics).reset_index(drop=True)\n",
    "    \n",
    "    return dist_vals, p_vals, rolling_metrics, total_alarms\n",
    "\n",
    "dist_test, pvals_test, performance_metrics, total_alarms = confidence_rolling_window(X_tr_final, x_test_stream, y_test_stream, shift_detector, sample, stat_window, lookup_window, stride, num_timesteps, threshold, model_name=model_name, model=model,opt=opt, X_ref=X_val_final, retrain=retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b667946-1a75-4be0-8ccf-df05396b0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdf7ad-9b65-4370-b18f-f1980a8f2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "mean = np.mean(pvals_test[pvals_test<0.05])\n",
    "ci = st.t.interval(0.95, len(pvals_test[pvals_test<0.05])-1, loc=np.mean(pvals_test[pvals_test<0.05]), scale=st.sem(pvals_test[pvals_test<0.05]))\n",
    "print(mean, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9d8fd-0dd6-4645-9a7f-1b7060109d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = performance_metrics.shape[0]\n",
    "threshold=0.05\n",
    "measure_dates_test_adjust = [(datetime.datetime.strptime(date,\"%Y-%m-%d\")+datetime.timedelta(days=lookup_window+stat_window)).strftime(\"%Y-%m-%d\") for date in measure_dates_test]\n",
    "fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6, 1, figsize=(22,12))\n",
    "results = pd.DataFrame(\n",
    "    {'dates': measure_dates_test_adjust[0:end],\n",
    "     'pval': pvals_test[0:end],\n",
    "     'dist': dist_test[0:end],\n",
    "     'detection': np.where(pvals_test[0:end]<threshold,1,0)\n",
    "    })\n",
    "results = pd.concat([results,performance_metrics],axis=1)\n",
    "results.to_pickle(os.path.join(PATH,shift,shift+\"_\"+dr_technique+\"_\"+md_test+\"_results.pkl\")) \n",
    "start = 0 \n",
    "end = performance_metrics.shape[0]-1\n",
    "cmap = ListedColormap(['lightgrey','red'])\n",
    "ax1.plot(results['dates'], results['pval'], '.-', color=\"red\", linewidth=0.5, markersize=2)\n",
    "ax1.set_xlim(results['dates'][start], results['dates'][end])\n",
    "ax1.axhline(y=threshold, color='dimgrey', linestyle='--')\n",
    "ax1.set_ylabel('P-Values',fontsize=16)\n",
    "ax1.set_xticklabels([])\n",
    "ax1.pcolorfast(ax1.get_xlim(), ax1.get_ylim(),results['detection'].values[np.newaxis], cmap = cmap, alpha = 0.4)\n",
    "  \n",
    "ax2.plot(results['dates'], results['dist'], '.-',color=\"red\", linewidth=0.5, markersize=2)\n",
    "ax2.set_xlim(results['dates'][start], results['dates'][end])\n",
    "ax2.set_ylabel('Distance',fontsize=16)\n",
    "ax2.axhline(y=np.mean(results['dist']), color='dimgrey', linestyle='--')\n",
    "ax2.set_xticklabels([])\n",
    "ax2.pcolorfast(ax2.get_xlim(), ax2.get_ylim(),results['detection'].values[np.newaxis], cmap = cmap, alpha = 0.4)\n",
    "\n",
    "ax3.plot(results['dates'], results['auroc'], '.-',color=\"blue\", linewidth=0.5, markersize=2)\n",
    "ax3.set_xlim(results['dates'][start], results['dates'][end])\n",
    "ax3.set_ylabel('AUROC', fontsize=16)\n",
    "ax3.axhline(y=np.mean(results['auroc']), color='dimgrey', linestyle='--')\n",
    "ax3.set_xticklabels([])\n",
    "ax3.pcolorfast(ax3.get_xlim(), ax3.get_ylim(),results['detection'].values[np.newaxis], cmap = cmap, alpha = 0.4)\n",
    "\n",
    "ax4.plot(results['dates'], results['auprc'], '.-',color=\"blue\", linewidth=0.5, markersize=2)\n",
    "ax4.set_xlim(results['dates'][start], results['dates'][end])\n",
    "ax4.set_ylabel('AUPRC',fontsize=16)\n",
    "ax4.axhline(y=np.mean(results['auprc']), color='dimgrey', linestyle='--')\n",
    "ax4.set_xticklabels([])\n",
    "ax4.pcolorfast(ax4.get_xlim(), ax4.get_ylim(),results['detection'].values[np.newaxis], cmap = cmap, alpha = 0.4)\n",
    "\n",
    "ax5.plot(results['dates'], results['prec1'], '.-',color=\"blue\", linewidth=0.5, markersize=2)\n",
    "ax5.set_xlim(results['dates'][start], results['dates'][end])\n",
    "ax5.set_ylabel('PPV', fontsize=16)\n",
    "ax5.axhline(y=np.mean(results['prec1']), color='dimgrey', linestyle='--')\n",
    "ax5.set_xticklabels([])\n",
    "ax5.pcolorfast(ax5.get_xlim(), ax5.get_ylim(),results['detection'].values[np.newaxis], cmap = cmap, alpha = 0.4)\n",
    "\n",
    "ax6.plot(results['dates'], results['rec1'], '.-',color=\"blue\", linewidth=0.5, markersize=2)\n",
    "ax6.set_xlim(results['dates'][start], results['dates'][end])\n",
    "ax6.set_ylabel('Sensitivity',fontsize=16)\n",
    "ax6.set_xlabel('time (s)', fontsize=16)\n",
    "ax6.axhline(y=np.mean(results['rec1']), color='dimgrey', linestyle='--')\n",
    "ax6.tick_params(axis='x', labelrotation=45)\n",
    "ax6.pcolorfast(ax6.get_xlim(), ax6.get_ylim(),results['detection'].values[np.newaxis], cmap = cmap, alpha = 0.4)\n",
    "\n",
    "for index, label in enumerate(ax6.xaxis.get_ticklabels()):\n",
    "    if index % 28 != 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-KKtuQLwg-py3.9",
   "language": "python",
   "name": "cyclops-kktuqlwg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
