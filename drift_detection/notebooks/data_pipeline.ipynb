{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5117a0d-9d1f-495e-93d5-edb057cf4c0a",
   "metadata": {},
   "source": [
    "## Data pipeline for predicting risk of mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb2625-e38f-428a-bcfd-6299b405d0f4",
   "metadata": {},
   "source": [
    "## Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "international-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:54:12,359 \u001b[1;37mINFO\u001b[0m cyclops.orm     - Database setup, ready to run queries!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cyclops.feature_handler import FeatureHandler\n",
    "from cyclops.plotter import plot_timeline, set_bars_color, setup_plot\n",
    "from cyclops.processor import run_data_pipeline\n",
    "from cyclops.processors.aggregate import Aggregator\n",
    "from cyclops.processors.column_names import (\n",
    "    ADMIT_TIMESTAMP,\n",
    "    AGE,\n",
    "    DIAGNOSIS_CODE,\n",
    "    DISCHARGE_DISPOSITION,\n",
    "    DISCHARGE_TIMESTAMP,\n",
    "    ENCOUNTER_ID,\n",
    "    EVENT_CATEGORY,\n",
    "    EVENT_NAME,\n",
    "    EVENT_TIMESTAMP,\n",
    "    EVENT_VALUE,\n",
    "    HOSPITAL_ID,\n",
    "    LENGTH_OF_STAY_IN_ER,\n",
    "    RESTRICT_TIMESTAMP,\n",
    "    SEX,\n",
    "    TIMESTEP,\n",
    "    TRIAGE_LEVEL,\n",
    "    WINDOW_START_TIMESTAMP,\n",
    ")\n",
    "from cyclops.processors.constants import SMH\n",
    "from cyclops.processors.events import (\n",
    "    combine_events,\n",
    "    convert_to_events,\n",
    "    normalize_events,\n",
    ")\n",
    "from cyclops.processors.impute import Imputer\n",
    "from cyclops.processors.statics import compute_statics\n",
    "from cyclops.processors.string_ops import replace_if_string_match, to_lower\n",
    "from cyclops.processors.util import (\n",
    "    create_indicator_variables,\n",
    "    fill_missing_timesteps,\n",
    "    gather_columns,\n",
    "    pivot_aggregated_events_to_features,\n",
    ")\n",
    "from cyclops.query import gemini\n",
    "from cyclops.utils.file import load_dataframe, save_dataframe\n",
    "\n",
    "MORTALITY = \"mortality\"\n",
    "LOS = \"los\"\n",
    "BASE_DATA_PATH = \"/mnt/nfs/project/delirium/drift_exp/risk_of_mortality\"\n",
    "AGGREGATION_WINDOW = 144\n",
    "AGGREGATION_BUCKET_SIZE = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8091448-5301-4631-a7d4-150667441e77",
   "metadata": {},
   "source": [
    "## Run query, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9efab5-8eb8-45fc-a53d-e5bfcc72ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 18:44:45,689 \u001b[1;37mINFO\u001b[0m cyclops.orm     - Query returned successfully!\n",
      "2022-07-07 18:44:45,691 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function run_query in 9.714826 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154532 rows extracted!\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(BASE_DATA_PATH, exist_ok=True)\n",
    "er_admin_table = gemini.get_table(gemini.ER_ADMIN)\n",
    "\n",
    "years = [2015, 2016, 2018, 2019, 2020]\n",
    "encounters = gemini.patient_encounters(\n",
    "    er_admin_table=er_admin_table,\n",
    "    years=years,\n",
    "    died=True,\n",
    "    died_binarize_col=\"mortality\",\n",
    ")\n",
    "encounters_labs = gemini.events(\n",
    "    patient_encounters_table=encounters.query, event_category=\"lab\"\n",
    ")\n",
    "imaging = gemini.imaging(years=years)\n",
    "transfusions = gemini.blood_transfusions(years=years)\n",
    "interventions = gemini.interventions(years=years)\n",
    "\n",
    "encounters.run()\n",
    "print(f\"{len(encounters.data)} rows extracted!\")\n",
    "\n",
    "encounters_labs.run()\n",
    "print(f\"{len(encounters_labs.data)} rows extracted!\")\n",
    "encounters_labs.save(os.path.join(BASE_DATA_PATH, \"labs\"))\n",
    "encounters_labs.clear_data()\n",
    "\n",
    "imaging.run()\n",
    "print(f\"{len(imaging.data)} rows extracted!\")\n",
    "transfusions.run()\n",
    "print(f\"{len(transfusions.data)} rows extracted!\")\n",
    "interventions.run()\n",
    "print(f\"{len(interventions.data)} rows extracted!\")\n",
    "\n",
    "encounters_imaging = pd.merge(\n",
    "    encounters.data, imaging.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "encounters_transfusions = pd.merge(\n",
    "    encounters.data, transfusions.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "encounters_interventions = pd.merge(\n",
    "    encounters.data, interventions.data, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "\n",
    "encounters.save(os.path.join(BASE_DATA_PATH, \"admin_er\"))\n",
    "encounters_imaging.to_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "encounters_transfusions.to_parquet(os.path.join(BASE_DATA_PATH, \"transfusions.parquet\"))\n",
    "encounters_interventions.to_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, \"interventions.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde24be8-7759-4241-a24d-e63eaa9ce9ac",
   "metadata": {},
   "source": [
    "## Read saved query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964a868e-11ab-432e-9d45-006f57d5a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"admin_er.parquet\"))\n",
    "labs_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"labs.parquet\"))\n",
    "imaging_data = pd.read_parquet(os.path.join(BASE_DATA_PATH, \"imaging.parquet\"))\n",
    "transfusions_data = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, \"transfusions.parquet\")\n",
    ")\n",
    "interventions_data = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, \"interventions.parquet\")\n",
    ")\n",
    "labs_data[EVENT_CATEGORY] = \"labs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91df5c-c266-45af-8541-3d1f291a93e2",
   "metadata": {},
   "source": [
    "## Map triage level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81be7a6-9a4c-4bd0-8f8a-406c2fcbba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(triage_level):\n",
    "    map_ = {\n",
    "        \"1\": \"RESUSCITATION\",\n",
    "        \"2\": \"EMERGENCY\",\n",
    "        \"3\": \"URGENT\",\n",
    "        \"4\": \"SEMI-URGENT\",\n",
    "        \"5\": \"NON-URGENT\",\n",
    "    }\n",
    "    return map_.get(triage_level, \"UNKNOWN\")\n",
    "\n",
    "\n",
    "encounters_data[TRIAGE_LEVEL] = encounters_data[TRIAGE_LEVEL].apply(remap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0af1b6-1e8b-4554-944f-f13209d282cc",
   "metadata": {},
   "source": [
    "## Map imaging and transfusions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3131da1-d7e8-4564-b1ff-d6e9e2251d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_data = imaging_data.rename(\n",
    "    columns={\n",
    "        \"imaging_test_description\": EVENT_NAME,\n",
    "        \"performed_date_time\": EVENT_TIMESTAMP,\n",
    "    }\n",
    ")\n",
    "imaging_data[EVENT_CATEGORY] = \"imaging\"\n",
    "imaging_data[EVENT_VALUE] = 1\n",
    "\n",
    "transfusions_data = transfusions_data.rename(\n",
    "    columns={\"issue_date_time\": EVENT_TIMESTAMP}\n",
    ")\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[\"rbc_mapped\"]\n",
    "transfusions_data[EVENT_NAME] = transfusions_data[EVENT_NAME].apply(\n",
    "    lambda x: \"rbc\" if x else \"non-rbc\"\n",
    ")\n",
    "transfusions_data[EVENT_VALUE] = 1\n",
    "transfusions_data[EVENT_CATEGORY] = \"transfusions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564db1ba-9f9d-4b39-a57a-90a357afab8f",
   "metadata": {},
   "source": [
    "##  Process interventions such that they can be treated as events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f34a21c-49ec-48a9-bfac-0cae8dce5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_668190/3783487325.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interventions_data[\"intervention_episode_start_time\"].loc[\n"
     ]
    }
   ],
   "source": [
    "interventions_data[EVENT_VALUE] = 1\n",
    "interventions_data[EVENT_CATEGORY] = \"interventions\"\n",
    "\n",
    "binary_mapped_cols = [\n",
    "    \"endoscopy_mapped\",\n",
    "    \"gi_endoscopy_mapped\",\n",
    "    \"bronch_endoscopy_mapped\",\n",
    "    \"dialysis_mapped\",\n",
    "    \"inv_mech_vent_mapped\",\n",
    "    \"surgery_mapped\",\n",
    "]\n",
    "interventions_data[\"intervention_episode_start_time\"].loc[\n",
    "    interventions_data[\"intervention_episode_start_time\"].isna()\n",
    "] = \"12:00:00\"\n",
    "interventions_data[EVENT_TIMESTAMP] = pd.to_datetime(\n",
    "    interventions_data[\"intervention_episode_start_date\"].astype(str)\n",
    "    + \" \"\n",
    "    + interventions_data[\"intervention_episode_start_time\"].astype(str)\n",
    ")\n",
    "interventions_data[EVENT_TIMESTAMP] = interventions_data[EVENT_TIMESTAMP].astype(\n",
    "    \"datetime64[ns]\"\n",
    ")\n",
    "interventions_data[\"unmapped_intervention\"] = ~(\n",
    "    interventions_data[\"endoscopy_mapped\"]\n",
    "    | interventions_data[\"gi_endoscopy_mapped\"]\n",
    "    | interventions_data[\"bronch_endoscopy_mapped\"]\n",
    "    | interventions_data[\"dialysis_mapped\"]\n",
    "    | interventions_data[\"inv_mech_vent_mapped\"]\n",
    "    | interventions_data[\"surgery_mapped\"]\n",
    ")\n",
    "interventions_data[EVENT_NAME] = interventions_data[\n",
    "    binary_mapped_cols + [\"unmapped_intervention\"]\n",
    "].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303e205-6491-442d-92a2-abc9a5c2a15d",
   "metadata": {},
   "source": [
    "## Filter out encounters that had less than 24 hours LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fc8075-57e8-49ac-b6f8-124d5b3591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_data[LOS] = (\n",
    "    encounters_data[DISCHARGE_TIMESTAMP] - encounters_data[ADMIT_TIMESTAMP]\n",
    ")\n",
    "encounters_data_atleast_los_24_hrs = encounters_data.loc[\n",
    "    encounters_data[LOS] >= pd.to_timedelta(24, unit=\"h\")\n",
    "]\n",
    "# encounters_data_atleast_los_24_hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72566d9f-db66-47f2-8820-fdd8904aafe1",
   "metadata": {},
   "source": [
    "## Get encounters that ended in mortality outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f04573-71c9-4f25-9a31-21df233e8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_mortality = encounters_data_atleast_los_24_hrs.loc[\n",
    "    encounters_data_atleast_los_24_hrs[MORTALITY] == True\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae3659-4304-4697-808d-75c66d2fe6f3",
   "metadata": {},
   "source": [
    "## Get encounters that didn't end up in mortality outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d191738-2eb9-4cda-83cd-fe17a7fc72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_not_mortality = encounters_data_atleast_los_24_hrs.loc[\n",
    "    encounters_data_atleast_los_24_hrs[MORTALITY] == False\n",
    "]\n",
    "\n",
    "num_encounters_not_mortality = len(encounters_mortality)\n",
    "encounters_not_mortality_subset = encounters_not_mortality[\n",
    "    0:num_encounters_not_mortality\n",
    "]\n",
    "# encounters_not_mortality_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464ae6f-2fe4-419e-8afb-57c8ab974136",
   "metadata": {},
   "source": [
    "## Combine both subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35d25c8-e4a1-4529-a9de-5b02bd37c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_train_val_test = pd.concat(\n",
    "    [encounters_mortality, encounters_not_mortality_subset], ignore_index=True\n",
    ")\n",
    "# encounters_train_val_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569cdd0-1d93-4aba-a6fa-082282d95eb6",
   "metadata": {},
   "source": [
    "## Get encounters which result in death within timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b8036a-a108-49a9-9a0e-9cf7b818e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = 14  # days\n",
    "encounters_mortality_within_risk_timeframe = encounters_mortality.loc[\n",
    "    encounters_mortality[LOS]\n",
    "    <= pd.to_timedelta(timeframe * 24 + AGGREGATION_WINDOW, unit=\"h\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6498e-f2a6-4383-89bd-6c9b5b4ee144",
   "metadata": {},
   "source": [
    "## Create death events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90081c16-ac41-4b60-ab48-7ff747630e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_events = convert_to_events(\n",
    "    encounters_mortality_within_risk_timeframe,\n",
    "    event_name=f\"death\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=DISCHARGE_TIMESTAMP,\n",
    ")\n",
    "mortality_events = pd.merge(\n",
    "    mortality_events, encounters_mortality, on=ENCOUNTER_ID, how=\"inner\"\n",
    ")\n",
    "mortality_events = mortality_events[\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        EVENT_NAME,\n",
    "        EVENT_TIMESTAMP,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        EVENT_VALUE,\n",
    "        EVENT_CATEGORY,\n",
    "    ]\n",
    "]\n",
    "mortality_events[EVENT_VALUE] = 1\n",
    "# mortality_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2653f-b1de-4e63-8531-b92677534041",
   "metadata": {},
   "source": [
    "## Get admission/discharge events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de93b44-b260-44d5-869f-80752cc913c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_events = convert_to_events(\n",
    "    encounters_train_val_test,\n",
    "    event_name=\"admission\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=ADMIT_TIMESTAMP,\n",
    ")\n",
    "disch_events = convert_to_events(\n",
    "    encounters_train_val_test,\n",
    "    event_name=\"discharge\",\n",
    "    event_category=\"general\",\n",
    "    timestamp_col=DISCHARGE_TIMESTAMP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238709bd-47f6-4cf6-8562-09717824d57f",
   "metadata": {},
   "source": [
    "## Filter events to be in train_val_test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "773d7deb-f5bf-4a7a-a773-5768a68f7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_train_val_test = labs_data.loc[\n",
    "    labs_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "imaging_train_val_test = imaging_data.loc[\n",
    "    imaging_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "transfusions_train_val_test = transfusions_data.loc[\n",
    "    transfusions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "interventions_train_val_test = interventions_data.loc[\n",
    "    interventions_data[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "admit_events = admit_events.loc[\n",
    "    admit_events[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]\n",
    "disch_events = disch_events.loc[\n",
    "    disch_events[ENCOUNTER_ID].isin(encounters_train_val_test[ENCOUNTER_ID])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e4baa-d706-4b72-88e4-7416db744d7e",
   "metadata": {},
   "source": [
    "## Normalize all event data (names, string operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e249a41-d012-4f25-9241-66c15fad7d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:56:22,388 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:56:22,391 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 107033, # encounters: 16701\n",
      "2022-07-08 10:56:22,393 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 16701\n",
      "2022-07-08 10:56:22,740 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:56:22,744 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 107033, # encounters: 16701\n",
      "2022-07-08 10:56:22,745 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 16701\n",
      "2022-07-08 10:56:22,977 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:56:22,981 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 107033, # encounters: 16701\n",
      "2022-07-08 10:56:22,982 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 16701\n",
      "2022-07-08 10:56:23,071 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:56:23,074 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 107033, # encounters: 16701\n",
      "2022-07-08 10:56:23,075 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 16701\n",
      "2022-07-08 10:56:23,076 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 0.746759 s\n",
      "2022-07-08 10:56:23,080 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:56:23,081 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 16724, # encounters: 2597\n",
      "2022-07-08 10:56:23,082 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 2597\n",
      "2022-07-08 10:56:23,140 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:56:23,141 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 16724, # encounters: 2597\n",
      "2022-07-08 10:56:23,142 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 2597\n",
      "2022-07-08 10:56:23,170 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:56:23,172 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 16724, # encounters: 2597\n",
      "2022-07-08 10:56:23,173 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 2597\n",
      "2022-07-08 10:56:23,187 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:56:23,189 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 16724, # encounters: 2597\n",
      "2022-07-08 10:56:23,190 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 2597\n",
      "2022-07-08 10:56:23,191 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 0.113853 s\n",
      "2022-07-08 10:56:24,967 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:56:24,993 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 3193469, # encounters: 17329\n",
      "2022-07-08 10:56:24,994 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:56:35,649 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:56:35,674 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 3193469, # encounters: 17329\n",
      "2022-07-08 10:56:35,675 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:56:42,882 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:56:42,905 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 2939327, # encounters: 17329\n",
      "2022-07-08 10:56:42,906 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:57:05,017 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Convert Positive/Negative to 1/0...\n",
      "2022-07-08 10:57:05,041 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 2939327, # encounters: 17329\n",
      "2022-07-08 10:57:05,042 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:57:12,230 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove any text in paranthesis\n",
      "2022-07-08 10:57:12,252 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 2939327, # encounters: 17329\n",
      "2022-07-08 10:57:12,253 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:57:24,573 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Fixing inequalities and removing outlier values...\n",
      "2022-07-08 10:57:24,596 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 2939327, # encounters: 17329\n",
      "2022-07-08 10:57:24,597 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:57:25,937 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Fill empty result string values with NaN...\n",
      "2022-07-08 10:57:25,958 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 2939327, # encounters: 17329\n",
      "2022-07-08 10:57:25,959 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:57:26,889 \u001b[1;37mINFO\u001b[0m cyclops.processors.events - Converting string result values to numeric...\n",
      "2022-07-08 10:57:26,891 \u001b[1;37mINFO\u001b[0m cyclops.processors.events - Normalizing units...\n",
      "2022-07-08 10:57:51,457 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:57:51,479 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 2939327, # encounters: 17329\n",
      "2022-07-08 10:57:51,480 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 60, # encounters: 17329\n",
      "2022-07-08 10:57:51,482 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 88.289569 s\n",
      "2022-07-08 10:57:51,483 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:57:51,485 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 6368, # encounters: 6368\n",
      "2022-07-08 10:57:51,486 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 6, # encounters: 6368\n",
      "2022-07-08 10:57:51,508 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:57:51,509 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 6368, # encounters: 6368\n",
      "2022-07-08 10:57:51,510 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 6, # encounters: 6368\n",
      "2022-07-08 10:57:51,518 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:57:51,520 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 6368, # encounters: 6368\n",
      "2022-07-08 10:57:51,521 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 6, # encounters: 6368\n",
      "2022-07-08 10:57:51,526 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:57:51,528 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 6368, # encounters: 6368\n",
      "2022-07-08 10:57:51,529 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 6, # encounters: 6368\n",
      "2022-07-08 10:57:51,530 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 0.047270 s\n",
      "2022-07-08 10:57:51,538 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:57:51,540 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 24701, # encounters: 8044\n",
      "2022-07-08 10:57:51,541 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 72, # encounters: 8044\n",
      "2022-07-08 10:57:51,625 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:57:51,627 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 24701, # encounters: 8044\n",
      "2022-07-08 10:57:51,628 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 72, # encounters: 8044\n",
      "2022-07-08 10:57:51,670 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:57:51,672 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 24701, # encounters: 8044\n",
      "2022-07-08 10:57:51,673 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 72, # encounters: 8044\n",
      "2022-07-08 10:57:51,696 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:57:51,698 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 24701, # encounters: 8044\n",
      "2022-07-08 10:57:51,699 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 72, # encounters: 8044\n",
      "2022-07-08 10:57:51,701 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 0.169169 s\n",
      "2022-07-08 10:57:51,702 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:57:51,704 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:51,706 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:51,760 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:57:51,762 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:51,763 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:51,781 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:57:51,783 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:51,784 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:51,895 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Convert Positive/Negative to 1/0...\n",
      "2022-07-08 10:57:51,897 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:51,898 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:51,940 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove any text in paranthesis\n",
      "2022-07-08 10:57:51,941 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:51,942 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:51,981 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Fixing inequalities and removing outlier values...\n",
      "2022-07-08 10:57:51,983 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:51,984 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,000 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Fill empty result string values with NaN...\n",
      "2022-07-08 10:57:52,002 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,003 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,004 \u001b[1;37mINFO\u001b[0m cyclops.processors.events - Converting string result values to numeric...\n",
      "2022-07-08 10:57:52,015 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:57:52,017 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,018 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,019 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 0.316808 s\n",
      "2022-07-08 10:57:52,021 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Cleaning raw event data...\n",
      "2022-07-08 10:57:52,022 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,023 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,077 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove text in parentheses and normalize event names...\n",
      "2022-07-08 10:57:52,079 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,080 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,098 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Drop unsupported events...\n",
      "2022-07-08 10:57:52,100 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,101 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,211 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Convert Positive/Negative to 1/0...\n",
      "2022-07-08 10:57:52,213 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,214 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,256 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Remove any text in paranthesis\n",
      "2022-07-08 10:57:52,258 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,259 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,298 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Fixing inequalities and removing outlier values...\n",
      "2022-07-08 10:57:52,300 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,301 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,316 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Fill empty result string values with NaN...\n",
      "2022-07-08 10:57:52,318 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,319 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,321 \u001b[1;37mINFO\u001b[0m cyclops.processors.events - Converting string result values to numeric...\n",
      "2022-07-08 10:57:52,332 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Normalize event categories...\n",
      "2022-07-08 10:57:52,334 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:57:52,334 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 5, # encounters: 17378\n",
      "2022-07-08 10:57:52,335 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function normalize_events in 0.315532 s\n"
     ]
    }
   ],
   "source": [
    "imaging_events = normalize_events(imaging_train_val_test)\n",
    "transfusion_events = normalize_events(transfusions_train_val_test)\n",
    "lab_events = normalize_events(labs_train_val_test)\n",
    "mortality_events = normalize_events(mortality_events)\n",
    "intervention_events = normalize_events(interventions_train_val_test)\n",
    "admit_events = normalize_events(admit_events)\n",
    "disch_events = normalize_events(disch_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950cea0-8178-4303-865d-ceb47ac2b30d",
   "metadata": {},
   "source": [
    "## Combine different event data, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080940-2a0e-45ab-987a-ef7f200a7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_events = combine_events(\n",
    "    [\n",
    "        intervention_events,\n",
    "        imaging_events,\n",
    "        transfusion_events,\n",
    "        lab_events,\n",
    "        mortality_events,\n",
    "        admit_events,\n",
    "        disch_events,\n",
    "    ]\n",
    ")\n",
    "#save_dataframe(combined_events, os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692cd1a-e9ec-456d-9df2-3a9cfef6eeab",
   "metadata": {},
   "source": [
    "## Load combined events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d9306e6-90a0-4adb-8691-972424912f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:57:52,345 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/risk_of_mortality/combined_events.parquet\n"
     ]
    }
   ],
   "source": [
    "combined_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"combined_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e10d6-b372-45e6-b0b3-8aa45593ef38",
   "metadata": {},
   "source": [
    "## Aggregate combined events, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e02af-1c6e-4cec-8b84-52e2cf24c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = Aggregator(bucket_size=AGGREGATION_BUCKET_SIZE, window=AGGREGATION_WINDOW)\n",
    "aggregated_events = aggregator(combined_events)\n",
    "save_dataframe(aggregated_events, os.path.join(BASE_DATA_PATH, \"aggregated_events\"))\n",
    "#save_dataframe(\n",
    "#    aggregator.meta[\"timestep_start_timestamp\"],\n",
    "#    os.path.join(BASE_DATA_PATH, \"aggmeta_start_ts\"),\n",
    "#)\n",
    "#save_dataframe(\n",
    "#    aggregator.meta[\"timestep_end_timestamp\"],\n",
    "#    os.path.join(BASE_DATA_PATH, \"aggmeta_end_ts\"),\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc55f-d3f2-41c6-a367-c8a817b424d8",
   "metadata": {},
   "source": [
    "## Load aggregated events, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab947b7d-b489-4ce6-be32-546a8d0ae6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:58:11,923 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/risk_of_mortality/aggregated_events.parquet\n",
      "2022-07-08 10:58:12,128 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/risk_of_mortality/aggmeta_start_ts.parquet\n",
      "2022-07-08 10:58:12,161 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/risk_of_mortality/aggmeta_end_ts.parquet\n"
     ]
    }
   ],
   "source": [
    "aggregated_events = load_dataframe(os.path.join(BASE_DATA_PATH, \"aggregated_events\"))\n",
    "timestep_start_timestamps = load_dataframe(\n",
    "    os.path.join(BASE_DATA_PATH, \"aggmeta_start_ts\")\n",
    ")\n",
    "timestep_end_timestamps = load_dataframe(os.path.join(BASE_DATA_PATH, \"aggmeta_end_ts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a0ea6-713e-4c0f-ab5a-a31fe81c9071",
   "metadata": {},
   "source": [
    "## Some small number of events are getting aggregated to a 7th timestep!! (Debug later, remove for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281af9c0-d425-497f-ae8e-185537ce8bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unmapped_intervention        59\n",
       "platelet count                9\n",
       "sodium                        9\n",
       "potassium                     9\n",
       "bicarbonate                   9\n",
       "white blood cell count        9\n",
       "hematocrit                    9\n",
       "hemoglobin                    9\n",
       "lymphocyte                    9\n",
       "mean cell volume              9\n",
       "neutrophils                   9\n",
       "creatinine                    8\n",
       "glucose point of care         6\n",
       "glucose random                5\n",
       "calcium                       3\n",
       "arterial paco2                3\n",
       "arterial pao2                 3\n",
       "blood urea nitrogen           3\n",
       "inv_mech_vent_mapped          3\n",
       "x-ray                         3\n",
       "endoscopy_mapped              3\n",
       "lactate arterial              2\n",
       "pt                            2\n",
       "inr                           2\n",
       "albumin                       2\n",
       "arterial ph                   2\n",
       "aptt                          1\n",
       "high sensitivity troponin     1\n",
       "discharge                     1\n",
       "fibrinogen                    1\n",
       "lactate venous                1\n",
       "venous pco2                   1\n",
       "venous ph                     1\n",
       "dialysis_mapped               1\n",
       "bilirubin                     1\n",
       "ct                            1\n",
       "Name: event_name, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_events.loc[aggregated_events[\"timestep\"] == 6][\"event_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "315f2641-2b82-41fd-ad24-2fdde0cc64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_events = aggregated_events.loc[aggregated_events[\"timestep\"] != 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4793-a22b-4611-96b3-65e2bd0d3d3c",
   "metadata": {},
   "source": [
    "## Pivot aggregated events to get column-wise temporal features and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a50a78-9e33-4f71-b45c-a2ca21ad31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = pivot_aggregated_events_to_features(aggregated_events, np.mean)\n",
    "#save_dataframe(temporal_features, os.path.join(BASE_DATA_PATH, \"temporal_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277e20e-c9c7-427c-b3a4-e6120b342581",
   "metadata": {},
   "source": [
    "## Add to feature handler, with indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7cf2259-d1e8-4d72-acb5-d4c1ebd4cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:58:13,697 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/risk_of_mortality/temporal_features.parquet\n",
      "2022-07-08 10:58:14,531 \u001b[1;33mWARNING\u001b[0m cyclops.feature_handler - Feature admission has all NaNs, will not be added.\n",
      "2022-07-08 10:58:15,153 \u001b[1;33mWARNING\u001b[0m cyclops.feature_handler - Feature discharge has all NaNs, will not be added.\n",
      "2022-07-08 10:58:15,669 \u001b[1;33mWARNING\u001b[0m cyclops.feature_handler - Feature influenza has all NaNs, will not be added.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['albumin', 'aptt', 'arterial paco2', 'arterial pao2', 'arterial ph',\n",
       "       'bicarbonate', 'bilirubin', 'blood urea nitrogen', 'calcium',\n",
       "       'calcium, ionized', 'creatinine', 'crp', 'ct', 'dialysis_mapped',\n",
       "       'echo', 'endoscopy_mapped', 'esr', 'ferritin', 'fibrinogen',\n",
       "       'glucose fasting', 'glucose point of care', 'glucose random', 'hba1c',\n",
       "       'hematocrit', 'hemoglobin', 'high sensitivity troponin', 'inr',\n",
       "       'interventional', 'inv_mech_vent_mapped', 'ketone', 'lactate arterial',\n",
       "       'lactate venous', 'lipase', 'lymphocyte', 'mean cell volume', 'mri',\n",
       "       'neutrophils', 'non-rbc', 'other', 'platelet count', 'potassium', 'pt',\n",
       "       'rbc', 'serum alcohol', 'sodium', 'surgery_mapped', 'troponin',\n",
       "       'ultrasound', 'unmapped_intervention', 'urine sodium',\n",
       "       'urine specific gravity', 'venous pco2', 'venous ph', 'vitamin b12',\n",
       "       'vitamin d', 'white blood cell count', 'x-ray', 'albumin_indicator',\n",
       "       'aptt_indicator', 'arterial paco2_indicator', 'arterial pao2_indicator',\n",
       "       'arterial ph_indicator', 'bicarbonate_indicator', 'bilirubin_indicator',\n",
       "       'blood urea nitrogen_indicator', 'calcium_indicator',\n",
       "       'calcium, ionized_indicator', 'creatinine_indicator', 'crp_indicator',\n",
       "       'esr_indicator', 'ferritin_indicator', 'fibrinogen_indicator',\n",
       "       'glucose fasting_indicator', 'glucose point of care_indicator',\n",
       "       'glucose random_indicator', 'hba1c_indicator', 'hematocrit_indicator',\n",
       "       'hemoglobin_indicator', 'high sensitivity troponin_indicator',\n",
       "       'inr_indicator', 'ketone_indicator', 'lactate arterial_indicator',\n",
       "       'lactate venous_indicator', 'lipase_indicator', 'lymphocyte_indicator',\n",
       "       'mean cell volume_indicator', 'neutrophils_indicator',\n",
       "       'platelet count_indicator', 'potassium_indicator', 'pt_indicator',\n",
       "       'serum alcohol_indicator', 'sodium_indicator', 'troponin_indicator',\n",
       "       'urine sodium_indicator', 'urine specific gravity_indicator',\n",
       "       'venous pco2_indicator', 'venous ph_indicator', 'vitamin b12_indicator',\n",
       "       'vitamin d_indicator', 'white blood cell count_indicator'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_handler = FeatureHandler()\n",
    "\n",
    "temporal_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"temporal_features\"))\n",
    "feature_handler.add_features(temporal_features)\n",
    "feature_handler.drop_features([\"death\"])\n",
    "\n",
    "already_indicators = [\n",
    "    \"ct\",\n",
    "    \"dialysis_mapped\",\n",
    "    \"echo\",\n",
    "    \"endoscopy_mapped\",\n",
    "    \"interventional\",\n",
    "    \"inv_mech_vent_mapped\",\n",
    "    \"mri\",\n",
    "    \"non-rbc\",\n",
    "    \"other\",\n",
    "    \"rbc\",\n",
    "    \"surgery_mapped\",\n",
    "    \"ultrasound\",\n",
    "    \"unmapped_intervention\",\n",
    "    \"x-ray\",\n",
    "]\n",
    "\n",
    "temporal_features = feature_handler.features[\"temporal\"]\n",
    "indicators = create_indicator_variables(\n",
    "    temporal_features[\n",
    "        [col for col in temporal_features if col not in already_indicators]\n",
    "    ]\n",
    ")\n",
    "feature_handler.add_features(indicators)\n",
    "feature_handler.features[\"temporal\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e89d8-a325-45f5-a38e-0f0ef6abe443",
   "metadata": {},
   "source": [
    "## Compute static features, save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d87c4497-03d3-4304-86d1-54b4eb5b5f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:58:19,338 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - Computing static columns...\n",
      "2022-07-08 10:58:19,341 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # samples: 17378, # encounters: 17378\n",
      "2022-07-08 10:58:19,342 \u001b[1;37mINFO\u001b[0m cyclops.processors.util - # columns: 7, # encounters: 17378\n",
      "2022-07-08 10:58:47,633 \u001b[1;37mINFO\u001b[0m cyclops.processors.statics - Found {'hospital_id', 'age', 'sex', 'discharge_timestamp', 'admit_timestamp', 'triage_level'} static feature columns.\n",
      "2022-07-08 10:58:47,635 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function infer_statics in 28.292427 s\n",
      "2022-07-08 10:59:12,215 \u001b[1;37mINFO\u001b[0m cyclops.utils.profile - Finished executing function compute_statics in 52.876462 s\n"
     ]
    }
   ],
   "source": [
    "encounters_train_val_test_static_cols = gather_columns(\n",
    "    encounters_train_val_test,\n",
    "    [\n",
    "        ENCOUNTER_ID,\n",
    "        AGE,\n",
    "        SEX,\n",
    "        HOSPITAL_ID,\n",
    "        ADMIT_TIMESTAMP,\n",
    "        DISCHARGE_TIMESTAMP,\n",
    "        TRIAGE_LEVEL,\n",
    "    ],\n",
    ")\n",
    "static_features = compute_statics(encounters_train_val_test_static_cols)\n",
    "#save_dataframe(static_features, os.path.join(BASE_DATA_PATH, \"static_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b6950-850c-4a83-a568-45088411911a",
   "metadata": {},
   "source": [
    "##  Load static features, add to feature handler, save all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c4759c9-bfd8-42ec-adeb-c518489caae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 10:59:12,226 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Loading dataframe to /mnt/nfs/project/delirium/drift_exp/risk_of_mortality/static_features.parquet\n"
     ]
    }
   ],
   "source": [
    "static_features = load_dataframe(os.path.join(BASE_DATA_PATH, \"static_features\"))\n",
    "feature_handler.add_features(\n",
    "    static_features, reference_cols=[HOSPITAL_ID, ADMIT_TIMESTAMP, DISCHARGE_TIMESTAMP]\n",
    ")\n",
    "#feature_handler.save(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028c06-7771-46db-a291-f7621471ef5e",
   "metadata": {},
   "source": [
    "## Create new feature handler, load saved features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0836f236-1f0b-46e8-90b8-8e811adcbe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 11:21:02,281 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Loading features from file...\n",
      "2022-07-08 11:21:02,286 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Found file to load for static features...\n",
      "2022-07-08 11:21:02,287 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Successfully loaded static features from file...\n",
      "2022-07-08 11:21:02,336 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Found file to load for temporal features...\n",
      "2022-07-08 11:21:07,262 \u001b[1;37mINFO\u001b[0m cyclops.feature_handler - Successfully loaded temporal features from file...\n"
     ]
    }
   ],
   "source": [
    "feature_handler1 = FeatureHandler()\n",
    "feature_handler1.load(BASE_DATA_PATH, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "443ad860-99aa-4423-b660-946d91891302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_handler1.features[\"temporal\"].columns), len(\n",
    "    feature_handler1.features[\"static\"].columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49719bee-1c6e-4f23-9dd9-a49f687f5370",
   "metadata": {},
   "source": [
    "## Impute temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5eddb97e-e050-43fb-9b60-26f204260706",
   "metadata": {},
   "outputs": [],
   "source": [
    "static = feature_handler1.features[\"static\"]\n",
    "temporal = feature_handler1.features[\"temporal\"]\n",
    "\n",
    "numerical_cols = feature_handler1.get_numerical_feature_names()[\"temporal\"]\n",
    "cat_cols = feature_handler1.get_categorical_feature_names()[\"temporal\"]\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(temporal.isna().sum(axis=0))\n",
    "# numerical_cols, cat_cols\n",
    "\n",
    "\n",
    "# Remove encounters based on missingness.\n",
    "# print(temporal.shape)\n",
    "# encounter_ids = set(temporal.index.get_level_values(0))\n",
    "# for encounter_id in encounter_ids:\n",
    "#     features_encounter = temporal.loc[encounter_id]\n",
    "#     not_indicators = [col_name for col_name in temporal.columns if 'indicator' not in col_name]\n",
    "#     num_na_features = features_encounter[not_indicators].isna().sum().sum()\n",
    "#     fraction_missing = num_na_features / (\n",
    "#         len(features_encounter) * len(temporal.columns)\n",
    "#     )\n",
    "#     if fraction_missing > 0.4:\n",
    "#         temporal = temporal.drop(encounter_id, level=ENCOUNTER_ID)\n",
    "# num_encounters_dropped = len(encounter_ids) - len(\n",
    "#     set(temporal.index.get_level_values(0))\n",
    "# )\n",
    "# print(num_encounters_dropped)\n",
    "\n",
    "# mean over features (incomplete)\n",
    "# imputer(temporal[numerical_cols]).fillna(temporal[numerical_cols].mean(level=0, numeric_only=True, skipna=True))\n",
    "\n",
    "# Forward-fill, then backward\n",
    "temporal[numerical_cols] = temporal[numerical_cols].ffill().bfill()\n",
    "\n",
    "# Check no more missingness!\n",
    "assert not temporal.isna().sum().sum() and not static.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4b887-e9da-4d69-b13e-7649ca3a6659",
   "metadata": {},
   "source": [
    "## Add static features to temporal, (duplicate for each timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ce62b29-67dd-4624-b973-071464e18fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with both static and temporal features.\n",
    "merged_static_temporal = temporal.combine_first(static)\n",
    "numerical_cols += [\"age\"]\n",
    "\n",
    "# Train model with just temporal features\n",
    "# merged_static_temporal = temporal\n",
    "# static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b63a8-58d1-41b7-9793-a8406514b6c1",
   "metadata": {},
   "source": [
    "## Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "155c4ae8-5ed3-4ab4-8d0b-c2d5d97d7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = int(AGGREGATION_WINDOW / AGGREGATION_BUCKET_SIZE)\n",
    "encounter_ids = list(merged_static_temporal.index.get_level_values(0).unique())\n",
    "num_encounters = len(encounter_ids)\n",
    "\n",
    "# All zeroes.\n",
    "labels = np.zeros((num_encounters, num_timesteps))\n",
    "\n",
    "# Set mortality within timeframe encounters to all 1s.\n",
    "labels[\n",
    "    [\n",
    "        encounter_ids.index(enc_id)\n",
    "        for enc_id in list(encounters_mortality_within_risk_timeframe[ENCOUNTER_ID])\n",
    "    ]\n",
    "] = 1\n",
    "\n",
    "# Get which timestep discharge occurs and set those and following timesteps label values to be -1.\n",
    "aggregated_discharge_events = aggregated_events.loc[\n",
    "    aggregated_events[EVENT_NAME] == \"discharge\"\n",
    "]\n",
    "aggregated_mortality_events = aggregated_events.loc[\n",
    "    aggregated_events[EVENT_NAME] == \"death\"\n",
    "]\n",
    "for enc_id in list(aggregated_discharge_events[ENCOUNTER_ID]):\n",
    "    timestep_discharge = aggregated_discharge_events.loc[\n",
    "        aggregated_discharge_events[ENCOUNTER_ID] == enc_id\n",
    "    ][\"timestep\"]\n",
    "    labels[encounter_ids.index(enc_id)][int(timestep_discharge) + 1 :] = -1\n",
    "\n",
    "# Lookahead for each timestep, and see if death occurs in risk timeframe.\n",
    "for enc_id in list(encounters_mortality_within_risk_timeframe[ENCOUNTER_ID]):\n",
    "    mortality_encounter = mortality_events.loc[mortality_events[ENCOUNTER_ID] == enc_id]\n",
    "    ts_ends = timestep_end_timestamps.loc[enc_id][\"timestep_end_timestamp\"]\n",
    "    mortality_ts = mortality_encounter[\"event_timestamp\"]\n",
    "    for ts_idx, ts_end in enumerate(ts_ends):\n",
    "        if not (\n",
    "            mortality_ts <= ts_end + pd.to_timedelta(timeframe * 24, unit=\"h\")\n",
    "        ).all():\n",
    "            labels[encounter_ids.index(enc_id)][ts_idx] = 0\n",
    "\n",
    "\n",
    "mortality_risk_targets = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da874af-fc43-4373-86b5-e3296893c27b",
   "metadata": {},
   "source": [
    "## Create train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a57ca-2e72-4f17-8452-3c3e5f4d6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(\n",
    "    encounters: pd.DataFrame,\n",
    "    fractions: Optional[List] = [0.8, 0.2],\n",
    "    split_column: Optional[str] = None,\n",
    "    split_values: List = None,\n",
    "    seed: int = 3,\n",
    ") -> tuple:\n",
    "    \"\"\"Split encounters into train/test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encounters: pandas.DataFrame\n",
    "        Dataframe with encounter IDs.\n",
    "    fractions: list, optional\n",
    "        Fraction of samples to use for train, test sets.\n",
    "    split_column: str, optional\n",
    "        If 'split_column' is specified, then that column is used to split.\n",
    "    split_values: list, optional\n",
    "        Along with 'split_column', a list of lists can be specified for filtering.\n",
    "        e.g. [[2008], [2009, 2010]] for train/test split based on year.\n",
    "    seed: int, optional\n",
    "        Seed for random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train IDs, test IDs)\n",
    "\n",
    "    \"\"\"\n",
    "    if split_column:\n",
    "        if split_column not in encounters.columns:\n",
    "            raise ValueError(\"Specified 'split column' not found in input dataframe\")\n",
    "        if not split_values:\n",
    "            raise ValueError(\"Specify train/test split values for the 'split column'.!\")\n",
    "        train_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[0])\n",
    "        ]\n",
    "        test_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[1])\n",
    "        ]\n",
    "        return train_encounters, test_encounters\n",
    "\n",
    "    encounter_ids = list(encounters[ENCOUNTER_ID].unique())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(encounter_ids)\n",
    "    num_train = int(fractions[0] * len(encounter_ids))\n",
    "\n",
    "    return encounter_ids[0:num_train], encounter_ids[num_train:]\n",
    "\n",
    "\n",
    "split_type = \"hospital_id\"\n",
    "\n",
    "if split_type == \"year\":\n",
    "    encounters_train_val_test[\"year\"] = encounters_train_val_test[\n",
    "        \"admit_timestamp\"\n",
    "    ].dt.year\n",
    "    train_ids, val_test_ids = create_train_test_split(\n",
    "        encounters_train_val_test,\n",
    "        split_column=\"year\",\n",
    "        split_values=[[2015, 2016, 2017, 2018, 2019], [2020]],\n",
    "    )\n",
    "    encounters_train_val_test[HOSPITAL_ID].value_counts(), encounters_train_val_test[\n",
    "        \"year\"\n",
    "    ].value_counts()\n",
    "elif split_type == \"hospital_id\":\n",
    "    train_ids, val_test_ids = create_train_test_split(\n",
    "        encounters_train_val_test,\n",
    "        split_column=HOSPITAL_ID,\n",
    "        split_values=[[\"SBK\", \"UHNTG\", \"THPC\", \"THPM\", \"UHNTW\", \"SMH\"], [\"MSH\"]],\n",
    "    )\n",
    "elif split_type == \"random\":\n",
    "    train_ids, val_test_ids = create_train_test_split(encounters_train_val_test)\n",
    "\n",
    "\n",
    "val_ids, test_ids = create_train_test_split(\n",
    "    encounters_train_val_test.loc[\n",
    "        encounters_train_val_test[ENCOUNTER_ID].isin(val_test_ids)\n",
    "    ],\n",
    "    [0.5, 0.5],\n",
    ")\n",
    "print(\n",
    "    f\"Train set: {len(train_ids)}, Val set: {len(val_ids)}, Test set: {len(test_ids)}\"\n",
    ")\n",
    "\n",
    "X = merged_static_temporal[\n",
    "    np.in1d(temporal.index.get_level_values(0), static.index.get_level_values(0))\n",
    "]\n",
    "\n",
    "y_train, y_val, y_test = [\n",
    "    mortality_risk_targets[np.in1d(encounter_ids, ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "X_train, X_val, X_test = [\n",
    "    X[np.in1d(X.index.get_level_values(0), ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "\n",
    "len(X), len(X_train), len(X_val), len(X_test), len(mortality_risk_targets), len(\n",
    "    y_train\n",
    "), len(y_val), len(y_test)\n",
    "assert len(X.index.get_level_values(0).unique()) == len(mortality_risk_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa18ef40-e56b-4767-af93-5f065f1cf69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 13015, Val set: 3254, Test set: 1099\n"
     ]
    }
   ],
   "source": [
    "BASE_DATA_PATH = \"/mnt/nfs/project/delirium/drift_exp/JULY-04-2022/\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "def create_train_test_split(\n",
    "    encounters: pd.DataFrame,\n",
    "    fractions: Optional[List] = [0.8, 0.2],\n",
    "    split_column: Optional[str] = None,\n",
    "    split_values: List = None,\n",
    "    seed: int = 3,\n",
    ") -> tuple:\n",
    "    \"\"\"Split encounters into train/test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encounters: pandas.DataFrame\n",
    "        Dataframe with encounter IDs.\n",
    "    fractions: list, optional\n",
    "        Fraction of samples to use for train, test sets.\n",
    "    split_column: str, optional\n",
    "        If 'split_column' is specified, then that column is used to split.\n",
    "    split_values: list, optional\n",
    "        Along with 'split_column', a list of lists can be specified for filtering.\n",
    "        e.g. [[2008], [2009, 2010]] for train/test split based on year.\n",
    "    seed: int, optional\n",
    "        Seed for random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train IDs, test IDs)\n",
    "\n",
    "    \"\"\"\n",
    "    if split_column:\n",
    "        if split_column not in encounters.columns:\n",
    "            raise ValueError(\"Specified 'split column' not found in input dataframe\")\n",
    "        if not split_values:\n",
    "            raise ValueError(\"Specify train/test split values for the 'split column'.!\")\n",
    "        train_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[0])\n",
    "        ]\n",
    "        test_encounters = encounters[ENCOUNTER_ID].loc[\n",
    "            encounters[split_column].isin(split_values[1])\n",
    "        ]\n",
    "        return train_encounters, test_encounters\n",
    "\n",
    "    encounter_ids = list(encounters[ENCOUNTER_ID].unique())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(encounter_ids)\n",
    "    num_train = int(fractions[0] * len(encounter_ids))\n",
    "\n",
    "    return encounter_ids[0:num_train], encounter_ids[num_train:]\n",
    "\n",
    "split_type = \"covid\"\n",
    "\n",
    "if split_type == \"covid\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            encounters_data[\"admit_timestamp\"].dt.date < datetime.date(2020, 2, 28),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            encounters_data[\"admit_timestamp\"].dt.date > datetime.date(2020, 2, 28),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    \n",
    "if split_type == \"seasonal\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"admit_timestamp\"].dt.month.isin([6, 7, 8])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"admit_timestamp\"].dt.month.isin([12, 1, 2])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "        \n",
    "if split_type == \"hosp_type\":\n",
    "    ids_source = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"hospital_id\"].isin([\"SMH\", \"MSH\", \"UHNTG\", \"UHNTW\",\"PMH\"])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "    ids_target = encounters_data.loc[\n",
    "            (\n",
    "                encounters_data[\"hospital_id\"].isin([\"THPC\", \"THPM\"])\n",
    "            ),\n",
    "            \"encounter_id\",\n",
    "    ]\n",
    "        \n",
    "train_val_ids = encounters_train_val_test[ENCOUNTER_ID].loc[\n",
    "            encounters_train_val_test[ENCOUNTER_ID].isin(ids_source)\n",
    "]\n",
    "\n",
    "test_ids = encounters_train_val_test[ENCOUNTER_ID].loc[\n",
    "            encounters_train_val_test[ENCOUNTER_ID].isin(ids_target)\n",
    "]\n",
    "\n",
    "train_ids, val_ids = create_train_test_split(\n",
    "    encounters_train_val_test.loc[\n",
    "        encounters_train_val_test[ENCOUNTER_ID].isin(train_val_ids)\n",
    "    ],\n",
    "    [0.8, 0.2],\n",
    ")\n",
    "print(\n",
    "    f\"Train set: {len(train_ids)}, Val set: {len(val_ids)}, Test set: {len(test_ids)}\"\n",
    ")\n",
    "\n",
    "X = merged_static_temporal[\n",
    "    np.in1d(temporal.index.get_level_values(0), static.index.get_level_values(0))\n",
    "]\n",
    "\n",
    "y_train, y_val, y_test = [\n",
    "    mortality_risk_targets[np.in1d(encounter_ids, ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "X_train, X_val, X_test = [\n",
    "    X[np.in1d(X.index.get_level_values(0), ids)]\n",
    "    for ids in [train_ids, val_ids, test_ids]\n",
    "]\n",
    "\n",
    "len(X), len(X_train), len(X_val), len(X_test), len(mortality_risk_targets), len(\n",
    "    y_train\n",
    "), len(y_val), len(y_test)\n",
    "assert len(X.index.get_level_values(0).unique()) == len(mortality_risk_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36dce7-2857-45bd-91af-69a7f4485597",
   "metadata": {},
   "source": [
    "## Save train/val/test ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "15a95142-e91f-43a9-a7b2-0e8a3c8b185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 11:44:42,570 \u001b[1;37mINFO\u001b[0m cyclops.utils.file - Saving dataframe to /mnt/nfs/project/delirium/drift_exp/JULY-04-2022/covid/train_val_test_ids.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/nfs/project/delirium/drift_exp/JULY-04-2022/covid/train_val_test_ids.parquet'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids_df = pd.DataFrame({\"train\": train_ids})\n",
    "val_ids_df = pd.DataFrame({\"val\": val_ids})\n",
    "test_ids_df = pd.DataFrame({\"test\": test_ids})\n",
    "train_val_test_ids = pd.concat([train_ids_df, val_ids_df, test_ids_df], axis=1)\n",
    "save_dataframe(\n",
    "    train_val_test_ids, os.path.join(BASE_DATA_PATH, split_type, \"train_val_test_ids\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bccd89-2893-45ea-b2b9-07629f4693c6",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "86cfd46a-5ec5-4cc0-8291-fd2261abbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_normalized = X_train.copy()\n",
    "X_val_normalized = X_val.copy()\n",
    "X_test_normalized = X_test.copy()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    scaler = StandardScaler().fit(X_train[col].values.reshape(-1, 1))\n",
    "    X_train_normalized[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(X_train[col].values.reshape(-1, 1))),\n",
    "        index=X_train[col].index,\n",
    "    )\n",
    "    X_val_normalized[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(X_val[col].values.reshape(-1, 1))),\n",
    "        index=X_val[col].index,\n",
    "    )\n",
    "    X_test_normalized[col] = pd.Series(\n",
    "        np.squeeze(scaler.transform(X_test[col].values.reshape(-1, 1))),\n",
    "        index=X_test[col].index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072b711-31cc-4693-8b41-100f4dd39265",
   "metadata": {},
   "source": [
    "## Save inputs and labels as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a168e69-a265-426f-9ab2-2a0cba495691",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(BASE_DATA_PATH, split_type), exist_ok=True)\n",
    "\n",
    "X_train_normalized.to_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_train.parquet\")\n",
    ")\n",
    "X_val_normalized.to_parquet(os.path.join(BASE_DATA_PATH, split_type, \"X_val.parquet\"))\n",
    "X_test_normalized.to_parquet(os.path.join(BASE_DATA_PATH, split_type, \"X_test.parquet\"))\n",
    "\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"y_val.npy\"), y_val)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70504f5c-eeed-43c7-aa08-e63e81cb5b85",
   "metadata": {},
   "source": [
    "## Load train/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1d301151-c865-4525-bf86-1e0f5bdde1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_train.parquet\")\n",
    ")\n",
    "X_val_normalized = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_val.parquet\")\n",
    ")\n",
    "X_test_normalized = pd.read_parquet(\n",
    "    os.path.join(BASE_DATA_PATH, split_type, \"X_test.parquet\")\n",
    ")\n",
    "\n",
    "y_train = np.load(os.path.join(BASE_DATA_PATH, split_type, \"y_train.npy\"))\n",
    "y_val = np.load(os.path.join(BASE_DATA_PATH, split_type, \"y_val.npy\"))\n",
    "y_test = np.load(os.path.join(BASE_DATA_PATH, split_type, \"y_test.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3219bbb-2ebb-42fb-879b-8e8f06d69861",
   "metadata": {},
   "source": [
    "## Reshape inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b10d73ba-d526-45b6-97e5-8d56f3442b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_inputs(inputs, num_timesteps):\n",
    "    inputs = inputs.unstack()\n",
    "    num_encounters = inputs.shape[0]\n",
    "    inputs = inputs.values.reshape((num_encounters, num_timesteps, -1))\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "X_train_normalized_npy = reshape_inputs(X_train_normalized, num_timesteps)\n",
    "X_val_normalized_npy = reshape_inputs(X_val_normalized, num_timesteps)\n",
    "X_test_normalized_npy = reshape_inputs(X_test_normalized, num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264bcee-1271-464a-a392-d8e369ec77e7",
   "metadata": {},
   "source": [
    "## Save inputs as npy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ad8df48d-7ecc-46c9-904e-82a9637368da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"X_train.npy\"), X_train_normalized_npy)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"X_val.npy\"), X_val_normalized_npy)\n",
    "np.save(os.path.join(BASE_DATA_PATH, split_type, \"X_test.npy\"), X_test_normalized_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812a9a8-1966-48a1-9094-66db6b4bde31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f4a47-a2f2-4071-8ae4-0b13957105a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_id = random.choice(encounter_ids)\n",
    "encounter_id = 13772609\n",
    "\n",
    "# Show examples:\n",
    "# 12779290, 13772609 - lookahead\n",
    "# 11454928, 15795997 - no mortality\n",
    "# 15253874 - all 1s\n",
    "\n",
    "print(encounter_id)\n",
    "combined_events_encounter = combined_events.loc[\n",
    "    combined_events[\"encounter_id\"] == encounter_id\n",
    "]\n",
    "fig = plot_timeline(combined_events_encounter, return_fig=True)\n",
    "\n",
    "fig = fig.update_layout(width=2000, height=800)\n",
    "\n",
    "ts_starts = timestep_start_timestamps.loc[encounter_id][\"timestep_start_timestamp\"]\n",
    "ts_ends = timestep_end_timestamps.loc[encounter_id][\"timestep_end_timestamp\"]\n",
    "for ts_end in ts_ends:\n",
    "    fig.add_vline(ts_end)\n",
    "\n",
    "label_encounter = labels[encounter_ids.index(encounter_id)]\n",
    "\n",
    "for i in range(num_timesteps):\n",
    "    label_ts = label_encounter[i]\n",
    "    color_map = {-1: \"grey\", 0: \"green\", 1: \"red\"}\n",
    "    fig.add_vrect(\n",
    "        x0=ts_starts[i],\n",
    "        x1=ts_ends[i],\n",
    "        fillcolor=color_map[label_ts],\n",
    "        opacity=0.25,\n",
    "        line_width=0,\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee092262-8a77-4ccc-933f-a4d316e28643",
   "metadata": {},
   "source": [
    "## TODO: Test out these imputation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1010c8-3c13-4dcd-803d-f5fe094749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_simple_v1(\n",
    "    dataframe: pd.DataFrame, time_index: str = TIMESTEP\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Impute features using 'Simple' method.\n",
    "\n",
    "    Concatenate the forward filled value, the mask of the measurement,\n",
    "    and the time of the last measurement.\n",
    "\n",
    "    Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu,\n",
    "    \"Recurrent Neural Networks for Multivariate Time Series with Missing Values,\"\n",
    "    Scientific Reports, vol. 8, no. 1, p. 6085, Apr 2018.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: pandas.DataFrame\n",
    "        Temporal features dataframe.\n",
    "    time_index: str, optional\n",
    "        The name of the time-series index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "     Dataframe after applying imputation.\n",
    "\n",
    "    \"\"\"\n",
    "    # Mask missingness.\n",
    "    masked_df = pd.isna(dataframe)\n",
    "    masked_df = masked_df.apply(pd.to_numeric)\n",
    "\n",
    "    # Compute time since last measurement.\n",
    "    index_of_time = list(dataframe.index.names).index(time_index)\n",
    "    time_in = [item[index_of_time] for item in dataframe.index.tolist()]\n",
    "    time_df = dataframe.copy()\n",
    "    for col in time_df.columns.tolist():\n",
    "        time_df[col] = time_in\n",
    "    time_df[masked_df] = np.nan\n",
    "\n",
    "    # Concatenate the dataframes.\n",
    "    df_prime = pd.concat(\n",
    "        [dataframe, masked_df, time_df], axis=1, keys=[\"measurement\", \"mask\", \"time\"]\n",
    "    )\n",
    "    df_prime.columns = df_prime.columns.rename(\"impute_simple\", level=0)\n",
    "\n",
    "    # Fill each dataframe using either ffill or mean.\n",
    "    df_prime = df_prime.fillna(method=\"ffill\").unstack().fillna(0)\n",
    "\n",
    "    # Swap the levels so that the simple imputation feature is the lowest value.\n",
    "    col_level_names = list(df_prime.columns.names)\n",
    "    col_level_names.append(col_level_names.pop(0))\n",
    "\n",
    "    df_prime = df_prime.reorder_levels(col_level_names, axis=1)\n",
    "    df_prime.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    return df_prime\n",
    "\n",
    "\n",
    "def impute_simple_v2(mean_vals):\n",
    "    \"\"\"Another version of 'simple' imputation.\"\"\"\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    mask = 1 - mean_vals.isna()\n",
    "    measurement = mean_vals.copy()\n",
    "\n",
    "    subset_data = measurement.loc[idx[:, :, 0], :]\n",
    "    data_means = measurement.mean()\n",
    "    subset_data = subset_data.fillna(data_means)\n",
    "    measurement.loc[idx[:, :, 0], :] = subset_data.values\n",
    "    measurement = measurement.ffill()\n",
    "\n",
    "    is_absent = 1 - mask\n",
    "    hours_of_absence = is_absent.groupby([\"patient_id\", \"genc_id\"]).cumsum()\n",
    "    time_df = hours_of_absence - hours_of_absence[is_absent == 0].fillna(method=\"ffill\")\n",
    "    time_df = time_df.fillna(0)\n",
    "\n",
    "    final_data = pd.concat(\n",
    "        [measurement, mask, time_df], keys=[\"measurement\", \"mask\", \"time\"], axis=1\n",
    "    )\n",
    "    final_data.columns = final_data.columns.swaplevel(0, 1)\n",
    "    final_data.sort_index(axis=\"columns\", inplace=True)\n",
    "\n",
    "    nancols = 0\n",
    "    try:\n",
    "        nancols = np.sum(\n",
    "            [a == 0 for a in final_data.loc[:, idx[:, \"mask\"]].sum().values]\n",
    "        )\n",
    "        print(nancols)\n",
    "    except:\n",
    "        print(\"could not get nancols\")\n",
    "        pass\n",
    "\n",
    "    print(nancols, \"/\", len(sorted(set(final_data.columns.get_level_values(0)))))\n",
    "    return final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops-KKtuQLwg-py3.9",
   "language": "python",
   "name": "cyclops-kktuqlwg-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
