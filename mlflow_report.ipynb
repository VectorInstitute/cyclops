{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "piano-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "def get_parameters_list(data):\n",
    "    params  = data['params']\n",
    "    return [params['model'], params['dataset']]\n",
    "\n",
    "\n",
    "def get_metrics_list(data):\n",
    "    if ('metrics' in data.keys()) and data['metrics']:\n",
    "        metrics = data['metrics']\n",
    "        return [metrics['accuracy'], metrics['f1_score']]\n",
    "    else:\n",
    "        return ['-', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "manual-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name                                          Artifacts  Status\n",
      "0  DatasetAnalysis  file:///mnt/nfs/home/koshkinam/vector-delirium...  active\n",
      "1          Default  file:///mnt/nfs/home/koshkinam/vector-delirium...  active\n",
      "2  ModelComparison  file:///mnt/nfs/home/koshkinam/vector-delirium...  active\n"
     ]
    }
   ],
   "source": [
    "# List all existing experiments\n",
    "all_experiments = mlflow.list_experiments()\n",
    "exp_data = []\n",
    "for exp in all_experiments:\n",
    "    row = [exp.name, exp.artifact_location, exp.lifecycle_stage]\n",
    "    exp_data.append(row)\n",
    "exp_frame = pd.DataFrame(exp_data, columns = ['Name', 'Artifacts', 'Status'])\n",
    "print(exp_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "copyrighted-recording",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Model Training Runs ----------------------\n",
      "       Start time Model   Dataset  Accuracy  F1 Score\n",
      "0   1635434580535   mlp    gemini         -         -\n",
      "1   1634918781255   mlp    gemini   0.67013  0.282486\n",
      "2   1634915740928   mlp    gemini  0.728733  0.350343\n",
      "3   1634915064631   mlp    gemini  0.728415  0.349581\n",
      "4   1634914831011   mlp    gemini  0.728097   0.35129\n",
      "5   1634914772549   mlp    gemini         -         -\n",
      "6   1634850932565   mlp    gemini   0.69493       0.0\n",
      "7   1634850843801   mlp    gemini  0.667506       0.0\n",
      "8   1634850407660   mlp    gemini         -         -\n",
      "9   1634849799501   mlp    gemini         -         -\n",
      "10  1634849591931   mlp    gemini         -         -\n",
      "11  1634849438673   mlp    gemini         -         -\n",
      "12  1634849231167   mlp    gemini         -         -\n",
      "13  1634848865744   mlp    gemini         -         -\n",
      "14  1634848406375   mlp    gemini         -         -\n",
      "15  1634848303526   mlp    gemini         -         -\n",
      "16  1634848185530   mlp    gemini         -         -\n",
      "17  1634846942311   mlp  fakedata    0.9845  0.984224\n",
      "18  1634846203988   mlp  fakedata         -         -\n",
      "19  1634845741648   mlp  fakedata         -         -\n",
      "20  1634845692384   mlp  fakedata         -         -\n",
      "21  1634845502178   mlp  fakedata         -         -\n",
      "22  1634845017135   mlp  fakedata         -         -\n"
     ]
    }
   ],
   "source": [
    "# For model training experiment - display last 100 runs with a subset of parameters and metrics\n",
    "runs = mlflow.list_run_infos('0', max_results=100)\n",
    "data = []\n",
    "for r in runs:\n",
    "    run_data = mlflow.get_run(r.run_id).to_dictionary()['data']\n",
    "    row = [r.start_time] + get_parameters_list(run_data) + get_metrics_list(run_data)\n",
    "    data.append(row)\n",
    "frame = pd.DataFrame(data, columns=['Start time', 'Model', 'Dataset', 'Accuracy', 'F1 Score'])\n",
    "print('------------------- Model Training Runs ----------------------')\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "political-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Dataset Analysis ----------------------\n",
      "                Input Slice Ref Slice Eval Slice Drift  Feat Drift_Feat\n",
      "0  ../gemini_data.csv  year    [2015]     [2016]    No  29.0        3.0\n",
      "1  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "2  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "3  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "4  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "5  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "6  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "7  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "8  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n",
      "9  ../gemini_data.csv  year    [2015]     [2016]     -     -          -\n"
     ]
    }
   ],
   "source": [
    "# Display dataset drift analysis runs\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_dataset_metrics_list(data):\n",
    "     if ('metrics' in data.keys()) and data['metrics']:\n",
    "        metrics = data['metrics']\n",
    "        #timestamp = data['params']['timestamp']\n",
    "        drift = 'No' if metrics['dataset_drift']==0 else 'Yes'\n",
    "        return [drift, metrics['n_features'], metrics['n_drifted_features']]\n",
    "     else:\n",
    "        return ['-', '-', '-']\n",
    "\n",
    "exp = mlflow.get_experiment_by_name('DatasetAnalysis')\n",
    "runs = mlflow.list_run_infos(exp.experiment_id, max_results=100)\n",
    "table = []\n",
    "for r in runs:\n",
    "    exp_run = mlflow.get_run(r.run_id).to_dictionary()\n",
    "    path = exp_run['info']['artifact_uri'][6:]\n",
    "    config_file = os.path.join(path, 'config.json')\n",
    "    if not os.path.isfile(config_file):\n",
    "        continue\n",
    "    with open(config_file) as f:\n",
    "        data = json.load(f)\n",
    "        row = [data['input'], data['slice'], data['data_ref'], data['data_eval']]\n",
    "        row = row + get_dataset_metrics_list(exp_run['data'])\n",
    "        table.append(row)\n",
    "frame = pd.DataFrame(table, columns=['Input', 'Slice', 'Ref Slice', 'Eval Slice', 'Drift', 'Feat', 'Drift_Feat'])\n",
    "print('------------------- Dataset Analysis ----------------------')\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset drift analysis runs\n",
    "\n",
    "exp = mlflow.get_experiment_by_name('ModelComparison')\n",
    "runs = mlflow.list_run_infos(exp.experiment_id, max_results=100)\n",
    "table = []\n",
    "for r in runs:\n",
    "    exp_run = mlflow.get_run(r.run_id).to_dictionary()\n",
    "    path = exp_run['info']['artifact_uri'][6:]\n",
    "    config_file = os.path.join(path, 'config.json')\n",
    "    if not os.path.isfile(config_file):\n",
    "        continue\n",
    "    with open(config_file) as f:\n",
    "        data = json.load(f)\n",
    "        row = [data['referece'], data['test']]\n",
    "        #row = row + get_dataset_metrics_list(exp_run['data'])\n",
    "        table.append(row)\n",
    "frame = pd.DataFrame(table, columns=['Reference', 'Eval', 'Ref Acc', 'Eval Acc', 'Ref F1 Score', 'Eval F1 Score'])\n",
    "print('------------------- Model Peformance Comparison  ----------------------')\n",
    "print(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MK Env",
   "language": "python",
   "name": "mkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
