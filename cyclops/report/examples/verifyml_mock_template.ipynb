{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# parent directory to work with dev\n",
    "sys.path.insert(0, \"..\")\n",
    "import verifyml.model_card_toolkit as mctlib\n",
    "from verifyml.model_card_toolkit import ModelCard, model_card_pb2\n",
    "from verifyml.model_tests.FEAT import (\n",
    "    DataShift,\n",
    "    FeatureImportance,\n",
    "    MinMaxMetricThreshold,\n",
    "    Perturbation,\n",
    "    SHAPFeatureImportance,\n",
    "    SubgroupDisparity,\n",
    ")\n",
    "from verifyml.model_tests.utils import plot_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit card fraud Dataset\n",
    "df = pd.read_csv(\"data/fraud.csv\")\n",
    "# get 5000 samples of fraud and 5000 samples of non-fraud\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df.loc[df.is_fraud == 1].sample(5000, replace=True),\n",
    "        df.loc[df.is_fraud == 0].sample(5000, replace=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "x = df.drop(\"is_fraud\", axis=1)\n",
    "y = df[\"is_fraud\"]\n",
    "\n",
    "\n",
    "# Train-Test data Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.5, random_state=50\n",
    ")\n",
    "\n",
    "\n",
    "## Build ML model with protected attributes as model features\n",
    "\n",
    "# Apply one hot encoding to categorical columns (auto-detect object columns) and random forest model in the pipeline\n",
    "estimator = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", ce.OneHotEncoder(use_cat_names=True)),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=4, max_features=\"sqrt\", random_state=882\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Fit, predict and compute performance metrics\n",
    "estimator.fit(x_train, y_train)\n",
    "\n",
    "output = x_test.copy()  # x_test df with output columns, to be appended later\n",
    "y_pred = estimator.predict(x_test)\n",
    "y_probas = estimator.predict_proba(x_test)[::, 1]\n",
    "\n",
    "precision_train = round(precision_score(y_train, estimator.predict(x_train)), 3)\n",
    "recall_train = round(recall_score(y_train, estimator.predict(x_train)), 3)\n",
    "precision_test = round(precision_score(y_test, y_pred), 3)\n",
    "recall_test = round(recall_score(y_test, y_pred), 3)\n",
    "\n",
    "\n",
    "# Add output columns to this dataframe, to be used as a input for feat tests\n",
    "output[\"truth\"] = y_test\n",
    "output[\"prediction\"] = y_pred\n",
    "output[\"prediction_probas\"] = y_probas\n",
    "\n",
    "\n",
    "# Dataframe with categorical features encoded\n",
    "x_train_encoded = estimator[0].transform(x_train)\n",
    "x_test_encoded = estimator[0].transform(x_test)\n",
    "\n",
    "\n",
    "# Get feature importance values\n",
    "df_importance = pd.DataFrame(\n",
    "    {\"features\": x_test_encoded.columns, \"value\": estimator[-1].feature_importances_}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stuck-worship",
   "metadata": {},
   "source": [
    "## Get confusion matrix and ROC curve on train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "ConfusionMatrixDisplay.from_estimator(estimator, x_train, y_train)\n",
    "confusion_matrix_train = plot_to_str()\n",
    "RocCurveDisplay.from_estimator(estimator, x_train, y_train)\n",
    "roc_curve_train = plot_to_str()\n",
    "\n",
    "# Test set\n",
    "ConfusionMatrixDisplay.from_estimator(estimator, x_test, y_test)\n",
    "confusion_matrix_test = plot_to_str()\n",
    "RocCurveDisplay.from_estimator(estimator, x_test, y_test)\n",
    "roc_curve_test = plot_to_str()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "prospective-preserve",
   "metadata": {},
   "source": [
    "## Run some FEAT Tests on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-aircraft",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ROC/Min Max Threshold Test\n",
    "\n",
    "smt_test = MinMaxMetricThreshold(\n",
    "    # test_name=\"\",        # Default test name and description will be used accordingly if not specified\n",
    "    # test_desc=\"\",\n",
    "    attr=\"age\",\n",
    "    metric=\"fpr\",\n",
    "    threshold=0.025,\n",
    "    # proba_threshold = 0.6 # Outcome probability threshold, default at 0.5\n",
    ")\n",
    "smt_test.run(df_test_with_output=output)\n",
    "smt_test.plot()\n",
    "\n",
    "smt_test2 = MinMaxMetricThreshold(\n",
    "    attr=\"age\",\n",
    "    metric=\"fpr\",\n",
    "    threshold=0.025,\n",
    ")\n",
    "smt_test2.run(df_test_with_output=output)\n",
    "smt_test2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-newport",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subgroup Disparity Test\n",
    "\n",
    "sgd_test = SubgroupDisparity(\n",
    "    attr=\"age\",\n",
    "    metric=\"fpr\",\n",
    "    method=\"ratio\",\n",
    "    threshold=1.5,\n",
    ")\n",
    "sgd_test.run(output)\n",
    "sgd_test.plot(alpha=0.05)  # default alpha argument shows 95% C.I bands\n",
    "\n",
    "sgd_test2 = SubgroupDisparity(\n",
    "    attr=\"age\",\n",
    "    metric=\"fpr\",\n",
    "    method=\"ratio\",\n",
    "    threshold=1.5,\n",
    ")\n",
    "sgd_test2.run(output)\n",
    "sgd_test2.plot(alpha=0.05)  # default alpha argument shows 95% C.I bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroup Perturbation Test\n",
    "\n",
    "np.random.seed(123)\n",
    "pmt = Perturbation(\n",
    "    attr=\"age\",\n",
    "    metric=\"fpr\",\n",
    "    method=\"ratio\",\n",
    "    threshold=1.5,\n",
    "    # proba_threshold=0.6,  # Outcome probability threshold, default at 0.5\n",
    ")\n",
    "\n",
    "pmt.run(x_test=x_test, y_test=y_test, encoder=estimator[0], model=estimator[-1])\n",
    "\n",
    "pmt.plot(alpha=0.05)  # default alpha argument shows 95% C.I bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmt.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-reset",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User inputted Feature importance test\n",
    "\n",
    "imp_test = FeatureImportance(attrs=[\"age\"], threshold=10)\n",
    "\n",
    "imp_test.run(df_importance)\n",
    "imp_test.plot(df_importance, show_n=10)  # Show top 10 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-breed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data distribution Shift Test\n",
    "\n",
    "shift_test = DataShift(protected_attr=[\"age\"], method=\"chi2\", threshold=0.05)\n",
    "\n",
    "shift_test.run(x_train=x_train, x_test=x_test)\n",
    "shift_test.plot(alpha=0.05)  # default alpha argument shows 95% C.I bands"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "human-martin",
   "metadata": {},
   "source": [
    "## Bootstrap model card from VerifyML model card editor and scaffold assets\n",
    "We can add the quantitative analysis, explainability analysis and fairness analysis sections to a bootstrap model card for convenience. In this example, we use an existing model card which we created from the [VerifyML model card editor](https://report.verifyml.com/create). This is meant only as an example - the dataset and risk evaluation in the model card is a fictional use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mct and scaffold using the existing protobuf\n",
    "mct = mctlib.ModelCardToolkit(\n",
    "    output_dir=\"model_card_output\", file_name=\"credit_card_fraud_example\"\n",
    ")\n",
    "mc = mct.scaffold_assets(path=\"initial_model_card.proto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "affecting-miniature",
   "metadata": {},
   "source": [
    "## Convert test objects to a model-card-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model card test objects\n",
    "mc_smt_test = mctlib.Test()\n",
    "mc_smt_test2 = mctlib.Test()\n",
    "mc_sgd_test = mctlib.Test()\n",
    "mc_sgd_test2 = mctlib.Test()\n",
    "mc_pmt_test = mctlib.Test()\n",
    "# mc_shap_test = mctlib.Test()\n",
    "mc_imp_test = mctlib.Test()\n",
    "mc_shift_test = mctlib.Test()\n",
    "\n",
    "# assign tests to them\n",
    "mc_smt_test.read_model_test(smt_test)\n",
    "mc_smt_test2.read_model_test(smt_test2)\n",
    "mc_sgd_test.read_model_test(sgd_test)\n",
    "mc_sgd_test2.read_model_test(sgd_test2)\n",
    "mc_pmt_test.read_model_test(pmt)\n",
    "mc_imp_test.read_model_test(imp_test)\n",
    "# mc_shap_test.read_model_test(shap_test)\n",
    "mc_shift_test.read_model_test(shift_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-hawaiian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add quantitative analysis\n",
    "\n",
    "# Create 4 PerformanceMetric to store our results\n",
    "mc.quantitative_analysis.performance_metrics = [\n",
    "    mctlib.PerformanceMetric() for i in range(0, 4)\n",
    "]\n",
    "mc.quantitative_analysis.performance_metrics[0].type = \"Recall\"\n",
    "mc.quantitative_analysis.performance_metrics[0].value = str(recall_train)\n",
    "mc.quantitative_analysis.performance_metrics[0].slice = \"Training Set\"\n",
    "\n",
    "mc.quantitative_analysis.performance_metrics[1].type = \"Precision\"\n",
    "mc.quantitative_analysis.performance_metrics[1].value = str(precision_train)\n",
    "mc.quantitative_analysis.performance_metrics[1].slice = \"Training Set\"\n",
    "mc.quantitative_analysis.performance_metrics[\n",
    "    1\n",
    "].graphics.description = \"Confusion matrix and ROC Curve\"\n",
    "mc.quantitative_analysis.performance_metrics[1].graphics.collection = [\n",
    "    mctlib.Graphic(image=confusion_matrix_train),\n",
    "    mctlib.Graphic(image=roc_curve_train),\n",
    "]\n",
    "\n",
    "mc.quantitative_analysis.performance_metrics[2].type = \"Recall\"\n",
    "mc.quantitative_analysis.performance_metrics[2].value = str(recall_test)\n",
    "mc.quantitative_analysis.performance_metrics[2].slice = \"Test Set\"\n",
    "\n",
    "mc.quantitative_analysis.performance_metrics[3].type = \"Precision\"\n",
    "mc.quantitative_analysis.performance_metrics[3].value = str(precision_test)\n",
    "mc.quantitative_analysis.performance_metrics[3].slice = \"Test Set\"\n",
    "mc.quantitative_analysis.performance_metrics[\n",
    "    3\n",
    "].graphics.description = \"Confusion matrix and ROC Curve\"\n",
    "mc.quantitative_analysis.performance_metrics[3].graphics.collection = [\n",
    "    mctlib.Graphic(image=confusion_matrix_test),\n",
    "    mctlib.Graphic(image=roc_curve_test),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add the components of a test (e.g. on explainability) in a report\n",
    "mc.explainability_analysis.explainability_reports = [\n",
    "    mctlib.ExplainabilityReport(\n",
    "        type=\"Top 10 most important features\",\n",
    "        graphics=mctlib.GraphicsCollection(\n",
    "            collection=[\n",
    "                mctlib.Graphic(name=n, image=i) for n, i in imp_test.plots.items()\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bootstrap template comes with two requirements on fairness analysis:\n",
    "# Minimum acceptable service and Equal false positive rate\n",
    "# We add the relevant tests associated with it\n",
    "# mc.fairness_analysis.fairness_reports[0].tests = [mc_smt_test,mc_smt_test2]\n",
    "# mc.fairness_analysis.fairness_reports[1].tests = [mc_sgd_test,mc_sgd_test2]\n",
    "\n",
    "# We also add a test for attribute shift between the training and testing dataset for additional reliablity check\n",
    "mc.fairness_analysis.fairness_reports.append(\n",
    "    mctlib.FairnessReport(\n",
    "        type=\"Distribution of attribute subgroups should be silimiar across different datasets\",\n",
    "        tests=[mc_shift_test],\n",
    "    )\n",
    ")\n",
    "mc.fairness_analysis.fairness_reports.append(\n",
    "    mctlib.FairnessReport(\n",
    "        type=\"Fairness metric for subgroups in original data and perturbed data should be similar\",\n",
    "        tests=[mc_pmt_test],\n",
    "    )\n",
    ")\n",
    "\n",
    "mct.update_model_card(mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "superior-compact",
   "metadata": {},
   "source": [
    "## Model Card Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2366d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.model_details.name = \"Credit Card Fraud Detection\"\n",
    "mc.model_details.overview = (\n",
    "    \"This model predicts whether a credit card transaction is fraudulent or not.\"\n",
    ")\n",
    "mc.model_details.documentation = \"This model is trained on the Credit Card Fraud Detection dataset from Kaggle. The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. The model is trained on 80% of the data and tested on the remaining 20%.\"\n",
    "mc.model_details.owners = [mctlib.Owner(name=\"John Doe\", contact=\"\", role=\"Researcher\")]\n",
    "mc.model_details.version.name = \"1.0\"\n",
    "mc.model_details.version.date = \"2021-01-01\"\n",
    "mc.model_details.version.diff = \"Initial release\"\n",
    "mc.model_details.licenses = [\n",
    "    mctlib.License(\n",
    "        identifier=\"Apache 2.0\",\n",
    "        custom_text=\"https://www.apache.org/licenses/LICENSE-2.0\",\n",
    "    )\n",
    "]\n",
    "mc.model_details.references = [\n",
    "    mctlib.Reference(reference=\"https://www.kaggle.com/mlg-ulb/creditcardfraud\")\n",
    "]\n",
    "mc.model_details.citations = [\n",
    "    mctlib.Citation(\n",
    "        style=\"APA\",\n",
    "        citation=\"Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "mct.update_model_card(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8881b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.considerations = mctlib.Considerations(\n",
    "    users=[mctlib.User(\"Data Scientists\")],\n",
    "    use_cases=[\n",
    "        mctlib.UseCase(\n",
    "            description=\"This model predicts whether a credit card transaction is fraudulent or not.\"\n",
    "        )\n",
    "    ],\n",
    "    limitations=[\n",
    "        mctlib.Limitation(\n",
    "            description=\"The model is trained on a dataset that is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\"\n",
    "        )\n",
    "    ],\n",
    "    tradeoffs=[\n",
    "        mctlib.Tradeoff(\n",
    "            description=\"The tradeoffs of using this model are that it can help banks to detect fraudulent transactions, but it can lead to false positives, which can lead to inconvenience for customers.\"\n",
    "        )\n",
    "    ],\n",
    "    ethical_considerations=[\n",
    "        mctlib.Risk(\n",
    "            name=\"The model is trained on a dataset that is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\",\n",
    "            mitigation_strategy=\"We can mitigate this by using a different dataset that is more balanced.\",\n",
    "        )\n",
    "    ],\n",
    "    fairness_assessment=[\n",
    "        mctlib.FairnessAssessment(\n",
    "            group_at_risk=\"Fraudulent transactions\",\n",
    "            benefits=\"The model can help banks to detect fraudulent transactions.\",\n",
    "            harms=\"The model can lead to false positives, which can lead to inconvenience for customers.\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "mct.update_model_card(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to html\n",
    "html = mct.export_format(\n",
    "    template_path=\"../model_card/template/cyclops_template.jinja\",\n",
    "    output_file=\"credit_card_fraud_example.html\",\n",
    ")\n",
    "# display.display(display.HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ca2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycyclops-py3.9",
   "language": "python",
   "name": "pycyclops-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
