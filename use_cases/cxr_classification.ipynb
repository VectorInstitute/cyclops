{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-ray Classification\n",
    "\n",
    "This notebook presents the chest X-ray classification use-case on NIH chest X-ray dataset which supports prediction and evaluation of models provided by `torchxrayvision` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets.features import Image\n",
    "from monai.transforms import AddChanneld, Compose, Lambdad, Resized, SqueezeDimd\n",
    "\n",
    "from cyclops.data.slicer import SliceSpec\n",
    "from cyclops.evaluate.metrics import AUROC, MetricCollection, create_metric\n",
    "from cyclops.models.catalog import create_model\n",
    "from cyclops.models.utils import get_device\n",
    "from cyclops.tasks.cxr_classification import CXRClassificationTask\n",
    "from cyclops.utils.file import join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHCXR_DIR = \"/mnt/data/clinical_datasets/NIHCXR\"\n",
    "FEATURE_COL = \"image\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Loading the NIH dataset as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    join(NIHCXR_DIR, \"test_list.txt\"), header=None, names=[\"Image Index\"],\n",
    ")\n",
    "\n",
    "# select only the images in the test list\n",
    "df = pd.read_csv(join(NIHCXR_DIR, \"Data_Entry_2017.csv\"))\n",
    "df.dropna(how=\"all\", axis=\"columns\", inplace=True)  # drop empty columns\n",
    "df = df[df[\"Image Index\"].isin(test_df[\"Image Index\"])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step the labels are added to the dataframe with a one-hot encoding representation. The dataframe is converted to Hugging Face dataset, which will be later used to load and preprocess the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nihcxr_preprocess(df: pd.DataFrame, nihcxr_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess NIHCXR dataframe.\n",
    "\n",
    "    Add a column with the path to the image and create one-hot encoded pathogies\n",
    "    from Finding Labels column.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        df (pd.DataFrame): NIHCXR dataframe.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        pd.DataFrame: pre-processed NIHCXR dataframe.\n",
    "    \"\"\"\n",
    "    # Add path column\n",
    "    df[FEATURE_COL] = df[\"Image Index\"].apply(\n",
    "        lambda x: os.path.join(nihcxr_dir, \"images\", x),\n",
    "    )\n",
    "\n",
    "    # Create one-hot encoded pathologies\n",
    "    pathologies = df[\"Finding Labels\"].str.get_dummies(sep=\"|\")\n",
    "    # Add one-hot encoded pathologies to dataframe\n",
    "    df = pd.concat([df, pathologies], axis=1)\n",
    "\n",
    "    return df, list(pathologies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, data_pathologies = nihcxr_preprocess(df, NIHCXR_DIR)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pathologies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Hugging Face Dataset from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dataset object\n",
    "nih_ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "nih_ds = nih_ds.cast_column(FEATURE_COL, Image())\n",
    "nih_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "The CyclOps Model API is used to wrapp the models from `torchxrayvision`. The configuration of the model is based on the corresponding config files, which include the necessary parameters for instantiating the models including the weights of pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_name = \"densenet\"\n",
    "densenet = create_model(densenet_name)\n",
    "# initalize the model to load weights\n",
    "densenet.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_name = \"resnet\"\n",
    "resnet = create_model(resnet_name)\n",
    "resnet.initialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels in the dataset and those that are learned by the pretrained models are not necessary the same. Later on the common labels will be used for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pathologies = densenet.model_.pathologies\n",
    "model_pathologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.model_.pathologies == model_pathologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pathologies = list(set(model_pathologies) & set(data_pathologies))\n",
    "common_pathologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extra_labels = [\n",
    "    label for label in data_pathologies if label not in common_pathologies\n",
    "]\n",
    "data_extra_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extra_labels = [\n",
    "    label for label in model_pathologies if label not in common_pathologies\n",
    "]\n",
    "model_extra_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest X-Ray Classification Task\n",
    "\n",
    "The CyclOps Task API is used to create a CXR Classification Task based on the available models and dataset. The task can contain multiple models that can be used for prediction individually. This is particularly useful when comparing the performance of multiple models during the evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxr_task = CXRClassificationTask(\n",
    "    {densenet_name: densenet, resnet_name: resnet},\n",
    "    task_features=[FEATURE_COL],\n",
    "    task_target=model_pathologies,\n",
    ")\n",
    "cxr_task.list_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "For prediction, the task object allows for numpy arrays and Hugging Face Datasets as input data features.\n",
    "\n",
    "When using a Hugging Face dataset as the input, you have the option to obtain the entire dataset with the added prediction column as the output of the predict method. This is particularly useful when dealing with large datasets that cannot fit into memory or when batched prediction is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms from `MONAI` are used to be applied to the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose(\n",
    "    [\n",
    "        AddChanneld(keys=(FEATURE_COL,)),\n",
    "        Resized(keys=(FEATURE_COL,), spatial_size=(1, 224, 224)),\n",
    "        Lambdad(keys=(FEATURE_COL), func=lambda x: ((2 * (x / 255.0)) - 1.0) * 1024),\n",
    "        SqueezeDimd(keys=(FEATURE_COL), dim=1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cxr_task.predict(\n",
    "    nih_ds,\n",
    "    model_name=densenet_name,\n",
    "    transforms=transforms,\n",
    "    prediction_column_prefix=\"predictions\",\n",
    "    only_predictions=False,\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cxr_task.predict(\n",
    "    nih_ds, model_name=resnet_name, transforms=transforms, only_predictions=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluation is typically performed on a Hugging Face dataset. To evaluate the models, you can also provide a slice specification on the meta data.\n",
    "\n",
    "In addition to the dataset and slice specification, you need to specify the desired evaluation metrics. This can be done by providing a MetricCollection object, a list of metrics, or metric names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the slices\n",
    "spec_list = [\n",
    "    {\"Patient Gender\": {\"value\": \"M\"}},\n",
    "    {\"Patient Gender\": {\"value\": \"F\"}},\n",
    "    #     {\"Patient Age\": {\"min_value\": 25, \"max_value\": 40}},\n",
    "    {\"Patient Age\": {\"min_value\": 65}},\n",
    "    {\"View Position\": {\"value\": \"PA\"}},\n",
    "]\n",
    "\n",
    "# create the slice functions\n",
    "slice_spec = SliceSpec(spec_list=spec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "\n",
    "metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "metrics = [\n",
    "    create_metric(metric_name, task=\"multilabel\", num_labels=len(model_pathologies))\n",
    "    for metric_name in metric_names\n",
    "]\n",
    "auroc = AUROC(\n",
    "    task=\"multilabel\",\n",
    "    thresholds=np.arange(0, 1, 0.01),\n",
    "    num_labels=len(model_pathologies),\n",
    ")\n",
    "metrics.append(auroc)\n",
    "metric_collection = MetricCollection(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, dataset_with_preds = cxr_task.evaluate(\n",
    "    nih_ds,\n",
    "    metric_collection,\n",
    "    transforms=transforms,\n",
    "    batch_size=128,\n",
    "    prediction_column_prefix=\"preds\",\n",
    "    slice_spec=slice_spec,\n",
    "    remove_columns=[FEATURE_COL],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[densenet_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[resnet_name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
