{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from getpass import getpass\n",
    "\n",
    "# import ipdb\n",
    "# import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f446d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = getpass(\"Username\")\n",
    "SPLIT_METHOD = \"quarter\"  # quarter, random, ordered\n",
    "SCORING = \"f1\"  # f1, roc_auc, f1_weighted\n",
    "VECTORIZER = \"tfidf\"  # tfidf, count\n",
    "DEL_LABEL_WEIGHT = (\n",
    "    2.9  # for class imbalance, use 1 to equally weigh positives and negatives\n",
    ")\n",
    "DIMENSIONALITY_REDUCTION = None  # pca, clustering\n",
    "SCALING = None  # maxabs, standard (necessary if using dimensionality reduction)\n",
    "HARDCODED_IMAGING_SYNONYMOUS_DIAGNOSES = \"granular\"  # aggregate, granular\n",
    "HARDCODED_IMAGING_TRIGGER_WORDS = \"granular\"  # aggregate, granular\n",
    "HARDCODED_IMAGING_SUPPORTING_WORDS = \"granular\"  # aggregate, granular\n",
    "LAB_FREQUENCY_ONLY = False  # True, False\n",
    "beers_list_path = \"H:\\\\repos\\\\delirium\\\\beers_list.csv\"\n",
    "antipsychotics_path = \"H:\\\\repos\\\\delirium\\\\antipsychotics.csv\"\n",
    "nootropics_path = \"H:\\\\repos\\\\delirium\\\\nootropics.csv\"\n",
    "benzodiazepine_path = \"H:\\\\repos\\\\delirium\\\\benzodiazepine.csv\"\n",
    "antibiotics_path = \"H:\\\\repos\\\\delirium\\\\antibiotics.csv\"\n",
    "sedatives_path = \"H:\\\\repos\\\\delirium\\\\sedatives.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927013c7",
   "metadata": {},
   "source": [
    "# 1 Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd91fe",
   "metadata": {},
   "source": [
    "Create connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"db.gemini-hpc.ca\",\n",
    "    database=\"delirium_v3_0_0\",\n",
    "    user=USERNAME,\n",
    "    password=getpass(\"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_sbk = psycopg2.connect(\n",
    "    host=\"172.27.12.113\",\n",
    "    database=\"delirium_sbk_v2_2_0\",\n",
    "    user=USERNAME,\n",
    "    password=getpass(\"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594ff0b",
   "metadata": {},
   "source": [
    "Just diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4e2cd",
   "metadata": {},
   "source": [
    "## 1.1 `ip_administrative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04205090",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_administrative_query = \"\"\"\n",
    "    select\n",
    "           ia.admit_date_time,\n",
    "           ia.genc_id,\n",
    "           ia.admit_category,\n",
    "           ia.discharge_disposition,\n",
    "           ia.number_of_alc_days,\n",
    "           ia.gender,\n",
    "           ia.age,\n",
    "           ia.institution_to,\n",
    "           ia.institution_to_type,\n",
    "           ia.institution_from,\n",
    "           ia.institution_from_type,\n",
    "           ia.del_present\n",
    "    from ip_administrative ia\n",
    "    where ia.del_present is not null and ia.del_present <> 3;\n",
    "\"\"\"\n",
    "\n",
    "ip_administrative = pd.read_sql_query(ip_administrative_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cdc0f8",
   "metadata": {},
   "source": [
    "## 1.2 `room_transfer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08428077",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_transfer_query = \"\"\"\n",
    "    select\n",
    "        rt.genc_id, count(*) as room_transfers\n",
    "    from room_transfer rt\n",
    "        inner join ip_administrative ia\n",
    "            on rt.genc_id = ia.genc_id\n",
    "    where ia.del_present is not null and ia.del_present <> 3\n",
    "    group by rt.genc_id;\n",
    "\"\"\"\n",
    "\n",
    "room_transfer = pd.read_sql_query(room_transfer_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffa4d0",
   "metadata": {},
   "source": [
    "## 1.3 `ip_scu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_scu_query = \"\"\"\n",
    "    select\n",
    "        scu.genc_id, count(*) as icu_transfers\n",
    "    from ip_scu scu\n",
    "        inner join ip_administrative ia\n",
    "            on scu.genc_id = ia.genc_id\n",
    "    where ia.del_present is not null and ia.del_present <> 3\n",
    "    group by scu.genc_id;\n",
    "\"\"\"\n",
    "\n",
    "ip_scu = pd.read_sql_query(ip_scu_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01180db9",
   "metadata": {},
   "source": [
    "## 1.4 `er_administrative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c169ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "er_administrative_query = \"\"\"\n",
    "    select\n",
    "        ea.genc_id,\n",
    "        ea.admit_via_ambulance,\n",
    "        ea.triage_level,\n",
    "        date_part('day', ea.left_er_date_time::timestamp - ea.triage_date_time::timestamp) * 24 + date_part('hour', ea.left_er_date_time::timestamp - ea.triage_date_time::timestamp) as er_los_derived\n",
    "    from er_administrative ea\n",
    "        inner join ip_administrative ia\n",
    "            on ea.genc_id = ia.genc_id\n",
    "    where ia.del_present is not null and ia.del_present <> 3;\n",
    "\"\"\"\n",
    "\n",
    "er_administrative = pd.read_sql_query(er_administrative_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854c4ba",
   "metadata": {},
   "source": [
    "## 1.5 `blood_transfusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blood_transfusion_query = \"\"\"\n",
    "    select\n",
    "        bt.genc_id, \n",
    "        count(bt.*) as total_transfusions,\n",
    "        count(distinct bt.blood_product_raw) as unique_transfusions\n",
    "    from blood_transfusion bt\n",
    "        inner join ip_administrative ia\n",
    "            on bt.genc_id = ia.genc_id\n",
    "    where ia.del_present is not null and ia.del_present <> 3\n",
    "    group by bt.genc_id;\n",
    "\"\"\"\n",
    "\n",
    "blood_transfusion = pd.read_sql_query(blood_transfusion_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aacdcb0",
   "metadata": {},
   "source": [
    "## 1.6 `delirium_icd10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delirium_icd10_query = \"\"\"\n",
    "    select\n",
    "        a.genc_id, bool_or(a.icd_delirium) as icd_delirium\n",
    "    from (\n",
    "        select distinct\n",
    "            d.genc_id,\n",
    "            case when d.diagnosis_code like 'F05%' then TRUE else FALSE end as icd_delirium\n",
    "        from diagnosis d\n",
    "            inner join ip_administrative ia\n",
    "                on d.genc_id = ia.genc_id\n",
    "        where ia.del_present is not null and ia.del_present <> 3\n",
    "    ) a\n",
    "    group by a.genc_id;\n",
    "\"\"\"\n",
    "\n",
    "delirium_icd10 = pd.read_sql_query(delirium_icd10_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef3338",
   "metadata": {},
   "source": [
    "## 1.7 `diagnosis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d23e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_query = \"\"\"\n",
    "    select d.genc_id, d.ccsr_no, substring(lc.ccsr_desc, 1, 20) as ccsr_code\n",
    "    from (\n",
    "             (with ccsr as (\n",
    "                 select i.genc_id,\n",
    "                        i.ccsr_default,\n",
    "                        i.ccsr_1,\n",
    "                        i.ccsr_2,\n",
    "                        i.ccsr_3,\n",
    "                        i.ccsr_4,\n",
    "                        i.ccsr_5\n",
    "                 from diagnosis i\n",
    "                          inner join ip_administrative a\n",
    "                                     on i.genc_id = a.genc_id\n",
    "                 where a.del_present is not null and a.del_present <> 3\n",
    "             )\n",
    "             select t.genc_id, m.ccsr_no, m.ccsr_code\n",
    "             from ccsr t\n",
    "                      cross join lateral (\n",
    "                 values (1, ccsr_default),\n",
    "                        (1, ccsr_1),\n",
    "                        (1, ccsr_2),\n",
    "                        (1, ccsr_3),\n",
    "                        (1, ccsr_4),\n",
    "                        (1, ccsr_5)\n",
    "                 ) as m(ccsr_no, ccsr_code)\n",
    "             where ccsr_code is not null\n",
    "               and ccsr_code <> '')\n",
    "         ) d\n",
    "    inner join (\n",
    "        select\n",
    "               i.ccsr_default,\n",
    "               count(*)\n",
    "        from diagnosis i\n",
    "        group by i.ccsr_default\n",
    "        order by count(*) desc\n",
    "        limit 250\n",
    "        ) common\n",
    "    on d.ccsr_code = common.ccsr_default\n",
    "    inner join lookup_ccsr lc on d.ccsr_code = lc.ccsr;\n",
    "\"\"\"\n",
    "\n",
    "diagnosis = pd.read_sql_query(diagnosis_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352a8f5",
   "metadata": {},
   "source": [
    "96% of codes are mapped in `delirium_sbk_v2_2_0.ipdiagnosis` and 90% are mapped in `delirium_v3_1_0.diagnosis`. If we had `lookup_icd10_ca_to_ccsr` in `delirium_v3_1_0` we would probably have identical coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381dec1b",
   "metadata": {},
   "source": [
    "Doing a weighted one-hot encoding where the weight is given by the ccsr_no. This may or may not be necessary. Instead we could use the diagnosis \"type\" - but this might be hard to code as a \"weight\". `@TODO`: Check that this makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = (\n",
    "    diagnosis.pivot_table(\n",
    "        index=\"genc_id\", columns=\"ccsr_code\", values=\"ccsr_no\", aggfunc=\"max\"\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .add_prefix(\"diag_\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a85d73",
   "metadata": {},
   "source": [
    "## 1.8 `lab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_lab_concept_query = \"\"\"\n",
    "    select concept_id::int, concept_desc from lookup_lab_concept\n",
    "\"\"\"\n",
    "\n",
    "lookup_lab_concept = pd.read_sql_query(lookup_lab_concept_query, conn_sbk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_query = \"\"\"\n",
    "    select stats_long.*\n",
    "    from (\n",
    "             with lab_stats as (\n",
    "                 select l.genc_id,\n",
    "                        l.test_type_mapped_omop,\n",
    "                        min(l.result_value::double precision)                    as minimum,\n",
    "                        --percentile_disc(0.25)\n",
    "                        --within group (order by l.result_value::double precision) as p25,\n",
    "                        percentile_disc(0.5)\n",
    "                        within group (order by l.result_value::double precision) as median,\n",
    "                        --percentile_disc(0.75)\n",
    "                        --within group (order by l.result_value::double precision) as p75,\n",
    "                        max(l.result_value::double precision)                    as maximum\n",
    "                 from lab l\n",
    "                          inner join ip_administrative a\n",
    "                                     on l.genc_id = a.genc_id\n",
    "                 where l.result_value ~ '^[0-9]+(\\\\.[0-9]+)?$'\n",
    "                   and a.del_present is not null and a.del_present <> 3\n",
    "                 group by l.genc_id, l.test_type_mapped_omop\n",
    "             )\n",
    "             select t.genc_id, t.test_type_mapped_omop, m.stat, m.value\n",
    "             from lab_stats t\n",
    "                      cross join lateral (\n",
    "                 values ('min', minimum),\n",
    "                        --('p25', p25),\n",
    "                        ('median', median),\n",
    "                        --('p75', p75),\n",
    "                        ('max', maximum)--,\n",
    "                        --('n', n_tests)\n",
    "                 ) as m(stat, value)\n",
    "         ) stats_long\n",
    "    inner join (\n",
    "        select\n",
    "               l.test_type_mapped_omop,\n",
    "               count(l.*)\n",
    "        from lab l\n",
    "        where l.test_type_mapped_omop is not null\n",
    "        group by l.test_type_mapped_omop\n",
    "        order by count(l.*) desc\n",
    "        limit 250\n",
    "        ) common\n",
    "    on stats_long.test_type_mapped_omop = common.test_type_mapped_omop\n",
    "    \n",
    "union\n",
    "\n",
    "    select stats_long_n.*\n",
    "    from (\n",
    "             with lab_stats_n as (\n",
    "                 select l.genc_id,\n",
    "                        l.test_type_mapped_omop,\n",
    "                        count(*) as n_tests\n",
    "                 from lab l\n",
    "                          inner join ip_administrative a\n",
    "                                     on l.genc_id = a.genc_id\n",
    "                 where a.del_present is not null and a.del_present <> 3\n",
    "                 group by l.genc_id, l.test_type_mapped_omop\n",
    "             )\n",
    "             select t.genc_id, t.test_type_mapped_omop, m.stat, m.value\n",
    "             from lab_stats_n t\n",
    "                      cross join lateral (\n",
    "                 values ('n', n_tests)\n",
    "                 ) as m(stat, value)\n",
    "         ) stats_long_n\n",
    "    inner join (\n",
    "        select\n",
    "               l.test_type_mapped_omop,\n",
    "               count(l.*)\n",
    "        from lab l\n",
    "        where l.test_type_mapped_omop is not null\n",
    "        group by l.test_type_mapped_omop\n",
    "        order by count(l.*) desc\n",
    "        limit 250\n",
    "        ) common_n\n",
    "    on stats_long_n.test_type_mapped_omop = common_n.test_type_mapped_omop;\n",
    "\"\"\"\n",
    "\n",
    "lab = pd.read_sql_query(lab_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5502d5c",
   "metadata": {},
   "source": [
    "Give english names to the lab tests by joining with concept map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = pd.merge(\n",
    "    lab,\n",
    "    lookup_lab_concept,\n",
    "    left_on=\"test_type_mapped_omop\",\n",
    "    right_on=\"concept_id\",\n",
    "    how=\"left\",\n",
    ").drop([\"test_type_mapped_omop\", \"concept_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e126349",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.loc[:, \"concept_desc\"] = lab.loc[:, \"concept_desc\"].str.slice(\n",
    "    stop=20\n",
    ")  # shorten names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb758ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = lab.pivot_table(index=\"genc_id\", columns=[\"concept_desc\", \"stat\"], values=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.columns = [\"_\".join(map(str, col)).strip() for col in lab.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ef65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = lab.add_prefix(\"lab_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LAB_FREQUENCY_ONLY:\n",
    "    labs_to_drop = lab.columns[~lab.columns.str.endswith(\"_n\")].tolist()\n",
    "    lab.drop(labs_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f9ded",
   "metadata": {},
   "source": [
    "## 1.9 `intervention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b853f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_query = \"\"\"\n",
    "    select\n",
    "        i.genc_id, \n",
    "        substring(i.intervention_code, 1, 2) as cci, 1 as value \n",
    "    from intervention i \n",
    "        inner join ip_administrative ia \n",
    "            on i.genc_id = ia.genc_id \n",
    "    where ia.del_present is not null and ia.del_present <> 3\n",
    "        and i.intervention_code is not null;\n",
    "\"\"\"\n",
    "\n",
    "intervention = pd.read_sql_query(intervention_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention = intervention.pivot_table(\n",
    "    index=\"genc_id\", columns=\"cci\", values=\"value\", aggfunc=\"max\"\n",
    ").add_prefix(\"intervention_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention.drop(\"intervention_\", axis=1, inplace=True)  # NULL intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ce4f",
   "metadata": {},
   "source": [
    "Look for very specific interventions in the data.\n",
    "\n",
    "`@TODO`: Add rare interventions only looking at training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c941a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_interventions_query = \"\"\"\n",
    "    select\n",
    "        a.genc_id,\n",
    "        max(a.\"CT head without enha\") as \"CT head without enha\",\n",
    "        max(a.\"CT brain without enh\") as \"CT brain without enh\",\n",
    "        max(a.\"Drain bladder PO &tu\") as \"Drain bladder PO &tu\",\n",
    "        max(a.\"Xray thor cav withou\") as \"Xray thor cav withou\",\n",
    "        max(a.\"Specimen collect NEC\") as \"Specimen collect NEC\"\n",
    "    from (\n",
    "        select \n",
    "            i.genc_id, \n",
    "            case when i.intervention_code = '3ER20VA' then 1 else 0\n",
    "                end as \"CT head without enha\",\n",
    "            case when i.intervention_code = '3AN20WA' then 1 else 0\n",
    "                end as \"CT brain without enh\",\n",
    "            case when i.intervention_code = '1PM52CATS' then 1 else 0\n",
    "                end as \"Drain bladder PO &tu\",\n",
    "            case when i.intervention_code = '3GY10VA' then 1 else 0\n",
    "                end as \"Xray thor cav withou\",\n",
    "            case when i.intervention_code = '2ZZ13RK' then 1 else 0\n",
    "                end as \"Specimen collect NEC\"\n",
    "        from intervention i \n",
    "            inner join ip_administrative ia \n",
    "                on i.genc_id = ia.genc_id \n",
    "        where ia.del_present is not null and ia.del_present <> 3\n",
    "    ) a\n",
    "    group by a.genc_id;\n",
    "\"\"\"\n",
    "\n",
    "rare_interventions = pd.read_sql_query(rare_interventions_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_interventions = rare_interventions.set_index(\"genc_id\").add_prefix(\n",
    "    \"rare_intervention_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14134b7",
   "metadata": {},
   "source": [
    "## 1.10 `pharmacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy_query = \"\"\"\n",
    "    select\n",
    "        r.genc_id, \n",
    "        string_agg(r.med_id_generic_name_raw, ', ') as medications, \n",
    "        count(r.med_id_generic_name_raw) as n_meds\n",
    "    from pharmacy r \n",
    "        right join ip_administrative ia \n",
    "            on ia.genc_id = r.genc_id \n",
    "    where ia.del_present is not null and ia.del_present <> 3\n",
    "    group by r.genc_id\n",
    "\"\"\"\n",
    "\n",
    "pharmacy = pd.read_sql_query(pharmacy_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy.loc[:, \"medications\"] = pharmacy.loc[:, \"medications\"].str.replace(\n",
    "    \"[^\\w\\s]\", \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21aaa8",
   "metadata": {},
   "source": [
    "Find number of Beer's List medications per encounter. Cross check against Beer's List (csv) and create a count of Beer's List medications per encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93392e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_list = pd.read_csv(beers_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy_raw_query = \"\"\"\n",
    "    select\n",
    "        p.genc_id, \n",
    "        p.med_id_generic_name_raw \n",
    "        from pharmacy p \n",
    "            inner join ip_administrative ia \n",
    "                on p.genc_id = ia.genc_id \n",
    "        where ia.del_present is not null and ia.del_present <> 3;\n",
    "\"\"\"\n",
    "\n",
    "pharmacy_raw = pd.read_sql_query(pharmacy_raw_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2deb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_beers_list(x):\n",
    "    for drug in beers_list.Medication:\n",
    "        if re.search(r\"\" + drug, x, re.IGNORECASE):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_beers = pharmacy_raw.med_id_generic_name_raw.apply(in_beers_list)\n",
    "pharmacy_raw.loc[:, \"in_beers\"] = in_beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c00052",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy_beers = pharmacy_raw.groupby(\"genc_id\")[\"in_beers\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66f223",
   "metadata": {},
   "source": [
    "Some drug class can be mapped manually. We have CSV files with mapped names for drugs of interest. Cross check pharmacy table for medications in these groups (per encounter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_meds(path, name):\n",
    "    def in_class(x, rxnorm_output):\n",
    "        for name in rxnorm_output.med_id_generic_name_raw:\n",
    "            if isinstance(name, str):\n",
    "                if re.search(r\"\" + name, x, re.IGNORECASE):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    mappings = pd.read_csv(antipsychotics_path)\n",
    "\n",
    "    mask = pharmacy_raw.med_id_generic_name_raw.apply(in_class, rxnorm_output=mappings)\n",
    "    pharmacy_raw.loc[:, name] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf341b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_meds(antipsychotics_path, \"n_antipsychotics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea12b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_meds(nootropics_path, \"n_nootropics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_meds(benzodiazepine_path, \"n_benzodiazepine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_meds(antibiotics_path, \"n_antibiotics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aab6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_meds(sedatives_path, \"n_sedatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c7726",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy_meds_mapped = pharmacy_raw.groupby(\"genc_id\")[\n",
    "    [\n",
    "        \"n_nootropics\",\n",
    "        \"n_antipsychotics\",\n",
    "        \"n_benzodiazepine\",\n",
    "        \"n_antibiotics\",\n",
    "        \"n_sedatives\",\n",
    "    ]\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de991ac3",
   "metadata": {},
   "source": [
    "## 1.11 `imaging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc265e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_query = \"\"\"\n",
    "    select \n",
    "        i.genc_id, \n",
    "        string_agg(i.test_result, ', ') as test_result\n",
    "    from imaging i\n",
    "        right join ip_administrative ia \n",
    "            on ia.genc_id = i.genc_id \n",
    "    where ia.del_present is not null and ia.del_present <> 3\n",
    "    group by i.genc_id\n",
    "\"\"\"\n",
    "\n",
    "imaging = pd.read_sql_query(imaging_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35badc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.loc[:, \"test_result\"] = imaging.loc[:, \"test_result\"].str.replace(\"[^\\w\\s]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d26ae",
   "metadata": {},
   "source": [
    "Create hard-coded imaging keyword features based on recommended patterns as described in the chart abstracting manual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ce76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymous_diagnoses = {\n",
    "    \"imag_Acute brain syndrome\": \"acute brain syndrome\",\n",
    "    \"imag_Acute brain failure\": \"acute brain failure\",\n",
    "    \"imag_Acute cerebral insufficiency\": \"acute cerebral insufficiency\",\n",
    "    \"imag_Acute organic psychosis\": \"acute organic psychosis\",\n",
    "    \"imag_Acute organic brain syndrome\": \"acute organic brain syndrome\",\n",
    "    \"imag_ICU Psychosis\": \"icu psychosis\",\n",
    "    \"imag_Metabolic encephalopathy\": \"metabolic encephalopathy\",\n",
    "    \"imag_Pseudosenility\": \"pseudosenility\",\n",
    "    \"imag_Reversible dementia\": \"reversible dementia\",\n",
    "    \"imag_Toxic-metabolic encepalopathy\": \"toxic metabolic encepalopathy\",\n",
    "    \"imag_Toxic psychosis\": \"toxic psychosis\",\n",
    "}\n",
    "\n",
    "trigger_words = {\n",
    "    \"imag_Acute confusion\": \"acute confusion\",\n",
    "    #'imag_Acute': 'acute', # assume this will create lots of false positives\n",
    "    \"imag_Acute mental status change (MS)\": \"acute mental status change\",\n",
    "    \"imag_Altered mental status (AMS)\": \"altered mental status\",\n",
    "    \"imag_Alert and Disoriented <3\": \"alert and disoriented\",\n",
    "    \"imag_Confus*\": \"confus[A-Za-z]*\",\n",
    "    \"imag_Disorient*\": \"disorient[A-Za-z]*\",\n",
    "    \"imag_Deliri*\": \"deliri[A-Za-z]*\",\n",
    "    \"imag_Encephalopathy\": \"encephalopathy\",\n",
    "    \"imag_Hallucinati*\": \"hallucinati[A-Za-z]*\",\n",
    "    \"imag_Mental status change\": \"mental status change\",\n",
    "    \"imag_Reorient*\": \"reorient[A-Za-z]*\",\n",
    "}\n",
    "\n",
    "supporting_words = {\n",
    "    \"imag_Agitat*\": \"agitat[A-Za-z]\",\n",
    "    \"imag_Alarm\": \"alarm\",\n",
    "    \"imag_Anxi*\": \"anxi[A-Za-z]*\",\n",
    "    \"imag_Attent*\": \"attent[A-Za-z]*\",\n",
    "    \"imag_Combative\": \"combative\",\n",
    "    \"imag_Commands\": \"commands\",\n",
    "    \"imag_Delusion\": \"delusion\",\n",
    "    \"imag_Distract*\": \"distract[A-Za-z]*\",\n",
    "    \"imag_Fall\": \"fall\",\n",
    "    \"imag_Fluctu*\": \"fluctu[A-Za-z]*\",\n",
    "    \"imag_Forget*\": \"forget[A-Za-z]*\",\n",
    "    \"imag_Hypoactive\": \"hypoactive\",\n",
    "    \"imag_Illusion\": \"illusion\",\n",
    "    \"imag_Impuls*\": \"impuls[A-Za-z]*\",\n",
    "    \"imag_Letharg*\": \"letharg[A-Za-z]*\",\n",
    "    \"imag_Multifactorial\": \"multifactorial\",\n",
    "    \"imag_Not Cooperative\": \"not cooperative\",\n",
    "    \"imag_Non-responsiveness\": \"non-responsiveness\",\n",
    "    \"imag_Narcotic*\": \"narcotic[A-Za-z]*\",\n",
    "    \"imag_Out of Bed or OOB\": \"(out of bed)|(OOB)\",\n",
    "    \"imag_Redirected\": \"redirected\",\n",
    "    \"imag_Refus*\": \"refus[A-Za-z]*\",\n",
    "    \"imag_Restless\": \"restless\",\n",
    "    \"imag_Sedat*\": \"sedat[A-Za-z]*\",\n",
    "    \"imag_Sleepy\": \"sleepy\",\n",
    "    \"imag_Somnolen*\": \"somnolent[A-Za-z]*\",\n",
    "    \"imag_Unrespons*\": \"unrespons[A-Za-z]*\",\n",
    "    \"imag_Uncooperative\": \"uncooperative\",\n",
    "    \"imag_Resist*\": \"resist[A-Za-z]\",\n",
    "    \"imag_Inappropriate\": \"inappropriate\",\n",
    "    \"imag_Altered level of consciousness\": \"altered level of consciousness\",\n",
    "    \"imag_Diminished level of responsiveness\": \"diminished level of responsiveness\",\n",
    "    \"imag_Difficulty with arousal\": \"difficulty with arousal\",\n",
    "    \"imag_New onset coma\": \"new onset coma\",\n",
    "    \"imag_Stupor\": \"stupor\",\n",
    "    \"imag_Lethargy\": \"lethargy\",\n",
    "    \"imag_Obtunded\": \"obtunded\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_search_dict(x, d):\n",
    "    if isinstance(x, str):\n",
    "        for key, value in d.items():\n",
    "            if re.search(r\"\" + value, x, re.IGNORECASE):\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def re_search_value(x, v):\n",
    "    if isinstance(x, str):\n",
    "        if re.search(r\"\" + v, x, re.IGNORECASE):\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARDCODED_IMAGING_SYNONYMOUS_DIAGNOSES == \"aggregate\":\n",
    "    imaging.loc[:, \"imag_Synonymous diagnoses\"] = imaging.loc[:, \"test_result\"].apply(\n",
    "        re_search_dict, d=synonymous_diagnoses\n",
    "    )\n",
    "\n",
    "elif HARDCODED_IMAGING_SYNONYMOUS_DIAGNOSES == \"granular\":\n",
    "    for key, value in synonymous_diagnoses.items():\n",
    "        imaging.loc[:, \"imag_regex_\" + key] = imaging.loc[:, \"test_result\"].apply(\n",
    "            re_search_value, v=value\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab83e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARDCODED_IMAGING_TRIGGER_WORDS == \"aggregate\":\n",
    "    imaging.loc[:, \"imag_Trigger words\"] = imaging.loc[:, \"test_result\"].apply(\n",
    "        re_search_dict, d=trigger_words\n",
    "    )\n",
    "\n",
    "elif HARDCODED_IMAGING_TRIGGER_WORDS == \"granular\":\n",
    "    for key, value in trigger_words.items():\n",
    "        imaging.loc[:, \"imag_regex_\" + key] = imaging.loc[:, \"test_result\"].apply(\n",
    "            re_search_value, v=value\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARDCODED_IMAGING_SUPPORTING_WORDS == \"aggregate\":\n",
    "    imaging.loc[:, \"imag_Supporting words\"] = imaging.loc[:, \"test_result\"].apply(\n",
    "        re_search_dict, d=supporting_words\n",
    "    )\n",
    "\n",
    "elif HARDCODED_IMAGING_SUPPORTING_WORDS == \"granular\":\n",
    "    for key, value in supporting_words.items():\n",
    "        imaging.loc[:, \"imag_regex_\" + key] = imaging.loc[:, \"test_result\"].apply(\n",
    "            re_search_value, v=value\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0feebbb",
   "metadata": {},
   "source": [
    "Just diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_end = time.time()\n",
    "query_time = query_end - query_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b5754",
   "metadata": {},
   "source": [
    "# 2 Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c459a0f",
   "metadata": {},
   "source": [
    "## 2.1 Combine Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fb221",
   "metadata": {},
   "source": [
    "Check missing values in `ip_administrative`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_administrative.columns[ip_administrative.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260b78d",
   "metadata": {},
   "source": [
    "Use `institution_from` and `institution_to` to determine if patient was from or was sent to a nursing home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_list = [4, 9, 50004, 50009]\n",
    "institution_type_list = [\"CR\", \"HF\", \"LT\", \"NH\", \"TM\"]\n",
    "\n",
    "ip_administrative.loc[:, \"to_nursing_home\"] = np.where(\n",
    "    ip_administrative.institution_to.isin(institution_list)\n",
    "    | ip_administrative.institution_to_type.isin(institution_type_list),\n",
    "    1,\n",
    "    0,\n",
    ")\n",
    "ip_administrative.loc[:, \"from_nursing_home\"] = np.where(\n",
    "    ip_administrative.institution_from.isin(institution_list)\n",
    "    | ip_administrative.institution_from_type.isin(institution_type_list),\n",
    "    1,\n",
    "    0,\n",
    ")\n",
    "\n",
    "ip_administrative = ip_administrative.drop(\n",
    "    [\n",
    "        \"institution_to\",\n",
    "        \"institution_to_type\",\n",
    "        \"institution_from\",\n",
    "        \"institution_from_type\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a2fea",
   "metadata": {},
   "source": [
    "Merge room transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062804f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(ip_administrative, room_transfer, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687d169",
   "metadata": {},
   "source": [
    "Fill missing `room_transfers` with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e89a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"room_transfers\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5c0b1",
   "metadata": {},
   "source": [
    "Merge `ip_scu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda08d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, ip_scu, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f4597",
   "metadata": {},
   "source": [
    "Fill missing `icu_transfers` with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"icu_transfers\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f2eba",
   "metadata": {},
   "source": [
    "Merge `er_administrative`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, er_administrative, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.admit_via_ambulance.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35720c61",
   "metadata": {},
   "source": [
    "`@TODO`: Apply processing function to column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.triage_level.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef6f72",
   "metadata": {},
   "source": [
    "`@TODO`: Apply processing function to column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22866c3d",
   "metadata": {},
   "source": [
    "Fill missing `er_los_derived` with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24662c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"er_los_derived\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6a83f",
   "metadata": {},
   "source": [
    "Merge `blood_transfusion`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, blood_transfusion, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10f209",
   "metadata": {},
   "source": [
    "Fill missing `total_transfusions` and `unique_transfusions` with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e659f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"total_transfusions\": 0, \"unique_transfusions\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f77e03",
   "metadata": {},
   "source": [
    "Merge `delirium_icd10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebe861",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, delirium_icd10, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64047840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.icd_delirium.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4702f",
   "metadata": {},
   "source": [
    "Impute missing `ICD10` code for Delirium with `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd83da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"icd_delirium\": False}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758b76b",
   "metadata": {},
   "source": [
    "Merge `diagnosis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa39076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, diagnosis, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9e681",
   "metadata": {},
   "source": [
    "Impute `nan` diagnoses as 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_imputer_mask = np.where(diagnosis.columns.str.startswith(\"diag_\"), 0, np.nan)\n",
    "diagnosis_imputer = {k: v for k, v in zip(diagnosis.columns, diagnosis_imputer_mask)}\n",
    "\n",
    "data.fillna(diagnosis_imputer, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efaad9a",
   "metadata": {},
   "source": [
    "Merge `lab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853321b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, lab, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b13b3",
   "metadata": {},
   "source": [
    "We will want to try a couple of imputation strategies with lab stats, so this will be part of the pipeline and not done at this step. However, when frequency is `nan`, then it should be 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_imputer_mask = np.where(\n",
    "    lab.columns.str.endswith(\"_n\") & lab.columns.str.startswith(\"lab_\"), 0, np.nan\n",
    ")\n",
    "lab_imputer = {k: v for k, v in zip(lab.columns, lab_imputer_mask)}\n",
    "\n",
    "data.fillna(lab_imputer, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f84992",
   "metadata": {},
   "source": [
    "Merge `intervention`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, intervention, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae435415",
   "metadata": {},
   "source": [
    "Missing intervention codes should be imputed with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_imputer_mask = np.where(\n",
    "    intervention.columns.str.startswith(\"intervention_\"), 0, np.nan\n",
    ")\n",
    "intervention_imputer = {\n",
    "    k: v for k, v in zip(intervention.columns, intervention_imputer_mask)\n",
    "}\n",
    "\n",
    "data.fillna(intervention_imputer, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e243f3d",
   "metadata": {},
   "source": [
    "Merge `rare_interventions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd059011",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, rare_interventions, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ace66",
   "metadata": {},
   "source": [
    "Missing `rare_interventions` should be imputed with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b93757",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_interventions_imputer_mask = np.where(\n",
    "    rare_interventions.columns.str.startswith(\"rare_intervention_\"), 0, np.nan\n",
    ")\n",
    "rare_interventions_imputer = {\n",
    "    k: v for k, v in zip(rare_interventions.columns, rare_interventions_imputer_mask)\n",
    "}\n",
    "\n",
    "data.fillna(rare_interventions_imputer, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d6bd6",
   "metadata": {},
   "source": [
    "Merge `pharmacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24623a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, pharmacy, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19341880",
   "metadata": {},
   "source": [
    "Impute missing `medications` with an empty string and impute missing `n_meds` with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"medications\": \"\", \"n_meds\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5688060",
   "metadata": {},
   "source": [
    "Merge `pharmacy_beers` and `pharmacy_meds_mapped`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, pharmacy_beers, on=\"genc_id\", how=\"left\")\n",
    "data = pd.merge(data, pharmacy_meds_mapped, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604045d",
   "metadata": {},
   "source": [
    "Impute missing Beer's medications with False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\n",
    "    {\n",
    "        \"in_beers\": False,\n",
    "        \"n_antipsychotics\": 0,\n",
    "        \"n_antibiotics\": 0,\n",
    "        \"n_sedatives\": 0,\n",
    "        \"n_benzodiazepine\": 0,\n",
    "        \"n_nootropics\": 0,\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cfa50",
   "metadata": {},
   "source": [
    "Merge `imaging`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, imaging, on=\"genc_id\", how=\"left\")\n",
    "data.columns[data.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1cf167",
   "metadata": {},
   "source": [
    "Impute missing `test_results` with an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\n",
    "    {\n",
    "        \"test_result\": \"\",\n",
    "        \"imag_Synonymous diagnoses\": False,\n",
    "        \"imag_Trigger words\": False,\n",
    "        \"imag_Supporting words\": False,\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARDCODED_IMAGING_SUPPORTING_WORDS == \"granular\":\n",
    "    imag_regex_to_impute = data.columns[\n",
    "        self.data.columns.str.startswith(\"imag_regex_\")\n",
    "    ].tolist()\n",
    "\n",
    "    data.fillna(\n",
    "        imag_regex_impute_map=dict((x, 0) for x in imag_regex_to_impute), inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb865c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index(\"genc_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c29d5",
   "metadata": {},
   "source": [
    "## 2.2 Custom Column Cleanup (CCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970d81c",
   "metadata": {},
   "source": [
    "`triage_level` should not have an `L` prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_l(x):\n",
    "    if isinstance(x, str):\n",
    "        if x[0] == \"L\":\n",
    "            x = int(x[1])\n",
    "        else:\n",
    "            x = int(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ede6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, \"triage_level\"] = data.loc[:, \"triage_level\"].apply(trim_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d87c7",
   "metadata": {},
   "source": [
    "`admit_via_ambulance` sometimes uses `GROUND` which means `G`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b601a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GROUND_to_G(x):\n",
    "    if x == \"GROUND\":\n",
    "        return \"G\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, \"admit_via_ambulance\"] = data.loc[:, \"admit_via_ambulance\"].apply(\n",
    "    GROUND_to_G\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9e56e",
   "metadata": {},
   "source": [
    "Make `del_present` 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, \"del_present\"] = np.where(data.loc[:, \"del_present\"] == 2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f762fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.to_frame().to_csv(\n",
    "    \"H:\\\\repos\\\\delirium\\\\delirium_features.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618eab3",
   "metadata": {},
   "source": [
    "## 2.3 Splitting Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf02135",
   "metadata": {},
   "source": [
    "`@TODO` Move these as static methods in `DeliriumExperimenter` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749739f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out_one_quarter(data, datetime_colname, target_colname, year=2015, quarter=1):\n",
    "    quarter_mask = data.loc[:, datetime_colname].dt.quarter == quarter\n",
    "    year_mask = data.loc[:, datetime_colname].dt.year == year\n",
    "\n",
    "    X = data.drop([datetime_colname, target_colname], axis=1)\n",
    "    y = data.loc[:, target_colname]\n",
    "    mask = quarter_mask & year_mask\n",
    "\n",
    "    X_train = X[~mask]\n",
    "    X_test = X[mask]\n",
    "\n",
    "    y_train = y[~mask]\n",
    "    y_test = y[mask]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(data, datetime_colname, target_colname):\n",
    "    X = data.drop([datetime_colname, target_colname], axis=1)\n",
    "    y = data.loc[:, target_colname]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5077f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_split(data, datetime_colname, target_colname):\n",
    "    data = data.sort_values(datetime_colname)\n",
    "\n",
    "    X = data.drop([datetime_colname, target_colname], axis=1)\n",
    "    y = data.loc[:, target_colname]\n",
    "\n",
    "    X_train, X_test = np.split(data_w_nlp, [int(0.9 * len(data_w_nlp))])\n",
    "    y_train, y_test = np.split(y, [int(0.9 * len(y))])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfe173",
   "metadata": {},
   "source": [
    "# 3 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15fee78",
   "metadata": {},
   "source": [
    "## 3.1 Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba59dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_stemming(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(WordNetLemmatizer().lemmatize(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d5965",
   "metadata": {},
   "source": [
    "Create stemmed stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_stopwords = []\n",
    "\n",
    "for stopword in stopwords.words(\"english\"):\n",
    "    stemmed = wordnet_stemming(stopword)\n",
    "    for stemmed_stopword in stemmed:\n",
    "        stemmed_stopwords.append(stemmed_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0feaacb",
   "metadata": {},
   "source": [
    "## 3.2 Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33126050",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stopwords_stem = TfidfVectorizer(\n",
    "    stop_words=stemmed_stopwords,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000,\n",
    "    tokenizer=wordnet_stemming,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15cfd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_stopwords_stem = CountVectorizer(\n",
    "    stop_words=stemmed_stopwords,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000,\n",
    "    tokenizer=wordnet_stemming,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfaa272",
   "metadata": {},
   "source": [
    "# 4 Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f479d2c",
   "metadata": {},
   "source": [
    "Below is an class which will allow for faster experimentation with respect to the delirium classifier. The class will be initated with particular parameters for the pipeline, and then will output performance metrics for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliriumExperimenter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        lab_frequency_only=LAB_FREQUENCY_ONLY,\n",
    "        first_experiment=False,\n",
    "        split_method=SPLIT_METHOD,\n",
    "        scoring=SCORING,\n",
    "        vectorizer=VECTORIZER,\n",
    "        del_label_weight=DEL_LABEL_WEIGHT,\n",
    "        dimensionality_reduction=DIMENSIONALITY_REDUCTION,\n",
    "        scaler=SCALING,\n",
    "        metrics_filename=\"H:\\\\delirium_metrics_separate_keywords.csv\",\n",
    "    ):\n",
    "\n",
    "        if lab_frequency_only:\n",
    "            labs_to_drop = data.columns[\n",
    "                data.columns.str.startswith(\"lab_\") & ~(data.columns.str.endswith(\"_n\"))\n",
    "            ].tolist()\n",
    "            data.drop(labs_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        # set variables\n",
    "        self.data = data\n",
    "        self.scoring = scoring\n",
    "        self.vectorizer = vectorizer\n",
    "        self.del_label_weight = del_label_weight\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.scaler = scaler\n",
    "        self.params = {}\n",
    "        self.metrics = {  # @TODO: Not clean design\n",
    "            \"split_method\": split_method,\n",
    "            \"scoring\": self.scoring,\n",
    "            \"vectorizer\": self.vectorizer,\n",
    "            \"del_label_weight\": self.del_label_weight,\n",
    "            \"dimensionality_reduction\": self.dimensionality_reduction,\n",
    "            \"scaler\": self.scaler,\n",
    "            \"model\": \"GradientBoostingClassifier\",\n",
    "            \"hardcoded_imaging_synonymous_diagnoses\": HARDCODED_IMAGING_SYNONYMOUS_DIAGNOSES,\n",
    "            \"hardcoded_imaging_trigger_words\": HARDCODED_IMAGING_TRIGGER_WORDS,\n",
    "            \"hardcoded_imaging_supporting_words\": HARDCODED_IMAGING_SUPPORTING_WORDS,\n",
    "        }\n",
    "\n",
    "        # create training and test data\n",
    "        self.create_train_test(split_method)\n",
    "\n",
    "        # develop pipeline skeleton\n",
    "        self.initialize_preprocessor()\n",
    "        self.add_vectorizer()\n",
    "\n",
    "        self.column_transformer = ColumnTransformer(\n",
    "            self.transformer_list, remainder=\"passthrough\"\n",
    "        )\n",
    "\n",
    "        self.initialize_pipeline()\n",
    "        self.add_scaling()\n",
    "        self.add_dimensionality_reduction()\n",
    "\n",
    "        self.pipeline_list = self.pipeline_list + [\n",
    "            (\"classifier\", GradientBoostingClassifier())\n",
    "        ]\n",
    "\n",
    "        self.pipe = Pipeline(self.pipeline_list)\n",
    "\n",
    "        # tune the model\n",
    "        self.params = {\n",
    "            **self.params,\n",
    "            **{\n",
    "                \"preprocessor__impute_labs__strategy\": [\"mean\", \"constant\"],\n",
    "                \"preprocessor__medications_nlp__ngram_range\": [(1, 2), (2, 3)],\n",
    "                \"preprocessor__medications_nlp__max_features\": [1000, 5000, 10000],\n",
    "                \"preprocessor__imaging_nlp__ngram_range\": [(1, 1), (1, 2)],\n",
    "                \"preprocessor__imaging_nlp__max_features\": [1000, 5000, 10000],\n",
    "                \"classifier__n_estimators\": [100, 250, 500],\n",
    "                \"classifier__learning_rate\": [0.1, 0.01, 0.001],\n",
    "                \"classifier__max_depth\": [2, 5, 10],\n",
    "            },\n",
    "        }\n",
    "        self.create_tuner()\n",
    "        self.tune()\n",
    "\n",
    "        self.evaluate(self.X_test, self.y_test, \"test\")\n",
    "        self.save_experiment(metrics_filename, first_experiment)\n",
    "\n",
    "    def create_train_test(\n",
    "        self,\n",
    "        method,\n",
    "        datetime_colname=\"admit_date_time\",\n",
    "        target_colname=\"del_present\",\n",
    "        year=2015,\n",
    "        quarter=1,\n",
    "    ):\n",
    "        if method == \"quarter\":\n",
    "            X_train, X_test, y_train, y_test = hold_out_one_quarter(\n",
    "                data, datetime_colname, target_colname\n",
    "            )\n",
    "\n",
    "        elif method == \"random\":\n",
    "            X_train, X_test, y_train, y_test = random_split(\n",
    "                data, datetime_colname, target_colname\n",
    "            )\n",
    "\n",
    "        elif method == \"ordered\":\n",
    "            X_train, X_test, y_train, y_test = ordered_split(\n",
    "                data, datetime_colname, target_colname\n",
    "            )\n",
    "\n",
    "        self.X = data.drop([target_colname, datetime_colname], axis=1)\n",
    "        self.y = data.loc[:, target_colname]\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def initialize_preprocessor(self):\n",
    "        labs_to_impute = self.data.columns[\n",
    "            self.data.columns.str.startswith(\"lab_\")\n",
    "            & ~self.data.columns.str.endswith(\"_n\")\n",
    "        ].tolist()\n",
    "\n",
    "        transformer_list = [\n",
    "            (\n",
    "                \"onehot\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                [\n",
    "                    \"gender\",\n",
    "                    \"discharge_disposition\",\n",
    "                    \"admit_category\",\n",
    "                    \"admit_via_ambulance\",\n",
    "                    \"triage_level\",\n",
    "                ],\n",
    "            ),\n",
    "            (\n",
    "                \"impute_labs\",\n",
    "                SimpleImputer(strategy=\"constant\", fill_value=-999),\n",
    "                labs_to_impute,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.transformer_list = transformer_list\n",
    "\n",
    "    def add_vectorizer(self):\n",
    "        if self.vectorizer == \"tfidf\":\n",
    "            self.transformer_list = self.transformer_list + [\n",
    "                (\"medications_nlp\", tfidf_stopwords_stem, \"medications\"),\n",
    "                (\"imaging_nlp\", tfidf_stopwords_stem, \"test_result\"),\n",
    "            ]\n",
    "        elif self.vectorizer == \"count\":\n",
    "            self.transformer_list = self.transformer_list + [\n",
    "                (\"medications_nlp\", count_stopwords_stem, \"medications\"),\n",
    "                (\"imaging_nlp\", count_stopwords_stem, \"test_result\"),\n",
    "            ]\n",
    "\n",
    "    def initialize_pipeline(self):\n",
    "        self.pipeline_list = [(\"preprocessor\", self.column_transformer)]\n",
    "\n",
    "    def add_scaling(self):\n",
    "        if self.scaler == \"maxabs\":\n",
    "            self.pipeline_list = self.pipeline_list + [(\"scaling\", MaxAbsScaler())]\n",
    "        elif self.scaler == \"standard\":\n",
    "            self.pipeline_list = self.pipeline_list + [\n",
    "                (\n",
    "                    \"scaling\",\n",
    "                    StandardScaler(with_mean=self.dimensionality_reduction != \"pca\"),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "    def add_dimensionality_reduction(self):\n",
    "        columns = len(self.data.columns)\n",
    "\n",
    "        if self.dimensionality_reduction == \"pca\":\n",
    "            self.params = {\n",
    "                **self.params,\n",
    "                **{\n",
    "                    \"dimensionality_reduction__n_components\": [\n",
    "                        math.ceil(columns / 2),\n",
    "                        math.ceil(columns * 0.9),\n",
    "                        500,\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "            self.pipeline_list = self.pipeline_list + [\n",
    "                (\n",
    "                    \"dimensionality_reduction\",\n",
    "                    TruncatedSVD(n_components=math.ceil(columns * 0.9)),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "    def create_tuner(self):\n",
    "        self.tuner = RandomizedSearchCV(\n",
    "            self.pipe,\n",
    "            param_distributions=self.params,\n",
    "            cv=5,\n",
    "            n_iter=12,  # @TODO make higher\n",
    "            scoring=self.scoring,\n",
    "        )\n",
    "\n",
    "    def tune(self):\n",
    "        tuning_start = time.time()\n",
    "\n",
    "        self.tuner.fit(\n",
    "            self.X_train,\n",
    "            self.y_train,\n",
    "            classifier__sample_weight=np.array(\n",
    "                [self.del_label_weight if i == 1 else 1 for i in self.y_train]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        tuning_end = time.time()\n",
    "        self.tuning_time = tuning_end - tuning_start\n",
    "\n",
    "        self.metrics = {\n",
    "            **self.metrics,\n",
    "            **{\n",
    "                \"tuning_time\": self.tuning_time,\n",
    "                \"best_score\": self.tuner.best_score_,\n",
    "                **self.tuner.best_params_,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def evaluate(self, X, y, name):\n",
    "        y_pred = self.tuner.predict(X)\n",
    "\n",
    "        roc_auc = roc_auc_score(y, y_pred)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y, y_pred, average=\"binary\"\n",
    "        )\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "        self.metrics = {\n",
    "            **self.metrics,\n",
    "            **{\n",
    "                name + \"_\" + \"roc_auc\": roc_auc,\n",
    "                name + \"_\" + \"precision\": precision,\n",
    "                name + \"_\" + \"recall\": recall,\n",
    "                name + \"_\" + \"f1\": f1,\n",
    "                name + \"_\" + \"support\": support,\n",
    "                name + \"_\" + \"npv\": tn / (tn + fn),\n",
    "                name + \"_\" + \"specificity\": tn / (tn + fp),\n",
    "                name + \"_\" + \"fpr\": fp / (fp + tn),\n",
    "                name + \"_\" + \"fnr\": fn / (tp + fn),\n",
    "                name + \"_\" + \"fdr\": fp / (tp + fp),\n",
    "                name + \"_\" + \"accuracy\": (tp + tn) / (tp + fp + fn + tn),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def save_experiment(self, metrics_filename, first_experiment):\n",
    "\n",
    "        with open(metrics_filename, \"w\" if first_experiment else \"a\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=list(self.metrics.keys()))\n",
    "            if first_experiment:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(self.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb19a55",
   "metadata": {},
   "source": [
    "## 4.1 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09de7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeliriumExperimenter(\n",
    "    data,\n",
    "    first_experiment=False,\n",
    "    lab_frequency_only=False,\n",
    "    scaler=\"standard\",\n",
    "    dimensionality_reduction=\"pca\",\n",
    "    vectorizer=\"tfidf\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclops",
   "language": "python",
   "name": "cyclops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
