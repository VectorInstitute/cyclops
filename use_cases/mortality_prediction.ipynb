{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortality Prediction using Tabular Data\n",
    "\n",
    "This notebooks presents the use-case of predicting the risk of mortality in patients on Mimic-IV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from datasets import load_dataset\n",
    "from datasets.splits import Split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from cyclops.datasets.slicing import SlicingConfig as SliceSpec\n",
    "\n",
    "# from cyclops.datasets.slice import SliceSpec XXX: Add when the branch is merged\n",
    "from cyclops.evaluate.metrics import MetricCollection, create_metric\n",
    "from cyclops.models.catalog import create_model\n",
    "from cyclops.models.constants import CONFIG_ROOT\n",
    "from cyclops.process.column_names import AGE, SEX\n",
    "from cyclops.tasks.mortality_prediction import MortalityPrediction\n",
    "from cyclops.utils.file import join, process_dir_save_path\n",
    "from use_cases.util import get_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"mimiciv\"\n",
    "CONST_NAME = \"mortality_decompensation\"\n",
    "\n",
    "USECASE_ROOT_DIR = join(\n",
    "    \"/mnt/data\",\n",
    "    \"cyclops\",\n",
    "    \"use_cases\",\n",
    "    DATASET,\n",
    "    CONST_NAME,\n",
    ")\n",
    "DATA_DIR = process_dir_save_path(join(USECASE_ROOT_DIR, \"./data\"))\n",
    "ENCOUNTERS_FILE = join(DATA_DIR, \"encounters.parquet\")\n",
    "\n",
    "OUTCOME_DEATH = \"outcome_death\"\n",
    "TARGET = [OUTCOME_DEATH]\n",
    "FEATURES = [\n",
    "    AGE,\n",
    "    SEX,\n",
    "    \"admission_type\",\n",
    "    \"admission_location\",\n",
    "]\n",
    "\n",
    "SPLIT_FRACTIONS = [0.8, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Constructing a Hugging Face Dataset from the encounters data queried form Mimic-IV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_ds = load_dataset(\n",
    "    \"parquet\", data_files=ENCOUNTERS_FILE, split=Split.ALL, keep_in_memory=True\n",
    ")\n",
    "encounters_ds.cleanup_cache_files()\n",
    "encounters_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split to train, validation, and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_ds = encounters_ds.train_test_split(train_size=SPLIT_FRACTIONS[0], seed=42)\n",
    "encounters_ds_ = encounters_ds[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "encounters_ds[\"validation\"] = encounters_ds_.pop(\"train\")\n",
    "encounters_ds[\"test\"] = encounters_ds_.pop(\"test\")\n",
    "encounters_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step, Scikit-learn transformations are appliedto the training dataset after converting it to a Pandas DataFrame. However, it's important to note that the conversion process should only be attempted if the dataset can fit into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_train_df = get_pandas_df(\n",
    "    encounters_ds[\"train\"], feature_cols=FEATURES, label_cols=TARGET\n",
    ")\n",
    "encounters_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric and categorical features are specified as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = (\n",
    "    encounters_train_df[0]\n",
    "    .loc[:, FEATURES]\n",
    "    .select_dtypes(include=[\"int\", \"float\"])\n",
    "    .columns.tolist()\n",
    ")\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = (\n",
    "    encounters_train_df[0]\n",
    "    .loc[:, FEATURES]\n",
    "    .select_dtypes(include=[\"object\"])\n",
    "    .columns.tolist()\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing pipeline\n",
    "numeric_features = [\n",
    "    encounters_train_df[0].columns.get_loc(col) for col in numeric_features\n",
    "]\n",
    "categorical_features = [\n",
    "    encounters_train_df[0].columns.get_loc(col) for col in categorical_features\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit and transform\n",
    "X_train = preprocessor.fit_transform(encounters_train_df[0]).toarray()\n",
    "y_train = encounters_train_df[1].to_numpy() * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "The CyclOps Model API is used to create models using estimators from the Scikit-learn package. The configuration of the model is based on the corresponding config files, which include the necessary parameters for instantiating the Scikit-learn estimators, as well as optional parameters for hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_name = \"mlp\"\n",
    "config_path = join(CONFIG_ROOT, mlp_name + \".yaml\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    mlp_config = yaml.safe_load(f)\n",
    "\n",
    "best_mlp_params = mlp_config[\"best_model_params\"]\n",
    "mlp_model = create_model(mlp_name, **mlp_config[\"model_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_name = \"xgb_classifier\"\n",
    "config_path = join(CONFIG_ROOT, xgb_name + \".yaml\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    xgb_config = yaml.safe_load(f)\n",
    "\n",
    "best_xgb_params = xgb_config[\"best_model_params\"]\n",
    "xgb_model = create_model(xgb_name, **xgb_config[\"model_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mortality Prediction Task\n",
    "\n",
    "The CyclOps Task API is used to create a Mortality Prediction Task based on the available models and dataset. The task can contain multiple models that can be trained and used for prediction individually. This is particularly useful when comparing the performance of multiple models during the evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_task = MortalityPrediction({mlp_name: mlp_model}, FEATURES, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_task.add_model(xgb_model, model_name=xgb_name)\n",
    "mortality_task.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_task.list_models_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "There are two methods to train models for mortality prediction: `train` and `train_on_hf_dataset`.\n",
    "\n",
    "The `train` method is used when the training features and labels are provided separately either as numpy arrays or dataframes (containing only the relevant columns). This method is suitable when the entire data can fit into the memory, partial fitting is not required, and hyperparameter search is desired. To use train, you can provide best_model_params to perform hyperparameter search.\n",
    "\n",
    "On the other hand, the `train_on_hf_dataset` method is used when the data is in the Hugging Face dataset format, especially when the data is too large to fit into memory. In this method, you can use the training dataset that includes both the features and labels.\n",
    "\n",
    "If the data is not preprocessed, you can use `ColumnTransformer` to preprocess the data before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mortality_task.train(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     model_name=xgb_name,\n",
    "#     best_model_params=best_xgb_params,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_task.train_on_hf_dataset(\n",
    "    encounters_ds[\"train\"], model_name=mlp_name, preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "In the prediction phase, the task object allows for a variety of data inputs, including numpy arrays, pandas dataframes, and Hugging Face Datasets.\n",
    "\n",
    "When using a Hugging Face dataset as the input, you have the option to obtain the entire dataset with the added prediction column as the output of the predict method. This is particularly useful when dealing with large datasets that cannot fit into memory or when batched prediction is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_ds_test = encounters_ds[\"test\"]\n",
    "encounters_df_test = get_pandas_df(\n",
    "    encounters_ds_test, feature_cols=FEATURES, label_cols=TARGET\n",
    ")\n",
    "X_test = preprocessor.transform(encounters_df_test[0].to_numpy()).toarray()\n",
    "Y_test = encounters_df_test[1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_task.predict(\n",
    "    X_test,\n",
    "    model_name=xgb_name,\n",
    "    proba=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_with_mlp_preds = mortality_task.predict(\n",
    "    encounters_ds_test,\n",
    "    model_name=mlp_name,\n",
    "    prediction_column_prefix=\"preds\",\n",
    "    preprocessor=preprocessor,\n",
    "    batch_size=5000,\n",
    "    only_predictions=False,\n",
    ")\n",
    "ds_with_mlp_preds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluation is typically performed on a Hugging Face dataset. To evaluate the models, you can also provide a slice specification to see how well they perform for different slices of data based on the feature values.\n",
    "\n",
    "In addition to the dataset and slice specification, you need to specify the desired evaluation metrics. This can be done by providing a MetricCollection object, a list of metrics, or metric names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = [\n",
    "    \"dod\",  # non-null/non-missing values in column\n",
    "    [\n",
    "        \"admission_type\",\n",
    "        \"admission_location\",\n",
    "    ],  # non-null/non-missing values in all columns in the list\n",
    "]\n",
    "\n",
    "feature_values = [\n",
    "    {\"sex\": {\"value\": \"M\"}},  # feature value is M\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"min_value\": 18,\n",
    "            \"max_value\": 65,\n",
    "            \"min_inclusive\": True,\n",
    "            \"max_inclusive\": False,\n",
    "        }\n",
    "    },  # feature value is between 18 and 65, inclusive of 18, exclusive of 65\n",
    "    {\n",
    "        \"admission_type\": {\"value\": [\"EW EMER.\", \"DIRECT EMER.\", \"URGENT\"]}\n",
    "    },  # feature value is in the list\n",
    "    {\n",
    "        \"admission_location\": {\n",
    "            \"value\": [\"PHYSICIAN REFERRAL\", \"CLINIC REFERRAL\", \"WALK-IN/SELF REFERRAL\"],\n",
    "            \"negate\": True,\n",
    "        }\n",
    "    },  # feature value is NOT in the list\n",
    "    {\n",
    "        \"dod\": {\"max_value\": \"2019-12-01\", \"keep_nulls\": False}\n",
    "    },  # possibly before COVID-19\n",
    "    {\n",
    "        \"dod\": {\"max_value\": \"2019-12-01\", \"negate\": True, \"keep_nulls\": False}\n",
    "    },  # possibly during COVID-19\n",
    "    {\"admit_timestamp\": {\"month\": [6, 7, 8, 9], \"keep_nulls\": False}},\n",
    "    {\n",
    "        \"sex\": {\"value\": \"F\"},\n",
    "        \"race\": {\n",
    "            \"value\": [\n",
    "                \"BLACK/AFRICAN AMERICAN\",\n",
    "                \"BLACK/CARIBBEAN ISLAND\",\n",
    "                \"BLACK/CAPE VERDEAN\",\n",
    "                \"BLACK/AFRICAN\",\n",
    "            ]\n",
    "        },\n",
    "        \"age\": {\"min_value\": 25, \"max_value\": 40},\n",
    "    },  # compound slice\n",
    "]\n",
    "\n",
    "\n",
    "# create the slice functions\n",
    "slice_spec = SliceSpec()\n",
    "\n",
    "for key in feature_keys:\n",
    "    slice_spec.add_feature_keys(key)\n",
    "\n",
    "for feature_value in feature_values:\n",
    "    slice_spec.add_feature_values(feature_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auroc\"]\n",
    "metrics = [create_metric(metric_name, task=\"binary\") for metric_name in metric_names]\n",
    "metric_collection = MetricCollection(metrics)\n",
    "\n",
    "\n",
    "results = mortality_task.evaluate(\n",
    "    encounters_ds_test,\n",
    "    metric_collection,\n",
    "    preprocessor=preprocessor,\n",
    "    prediction_column_prefix=\"preds\",\n",
    "    slice_spec=slice_spec,\n",
    "    batch_size=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[mlp_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[xgb_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
