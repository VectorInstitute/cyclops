{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Extensibility of the ðŸ¤— Datasets Library for Medical Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from datasets import Dataset, Image, load_dataset\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from cyclops.utils.file import join\n",
    "from use_cases.params.mimiciv.mortality_decompensation.constants_v1 import QUERIED_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring existing functionalities that are relevant to CyclOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_PROC = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a ðŸ¤— Dataset from MIMICIV-v2.0 PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cfg = OmegaConf.load(join(\"..\", \"cyclops\", \"query\", \"configs\", \"config.yaml\"))\n",
    "\n",
    "con_str = (\n",
    "    db_cfg.dbms\n",
    "    + \"://\"\n",
    "    + db_cfg.user\n",
    "    + \":\"\n",
    "    + db_cfg.password\n",
    "    + \"@\"\n",
    "    + db_cfg.host\n",
    "    + \"/\"\n",
    "    + db_cfg.database\n",
    ")\n",
    "\n",
    "ds = Dataset.from_sql(\n",
    "    sql=\"SELECT * FROM mimiciv_hosp.patients LIMIT 10\",\n",
    "    con=con_str,\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a ðŸ¤— Dataset from local parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = list(glob.glob(join(QUERIED_DIR, \"*.parquet\")))\n",
    "len(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 100 files\n",
    "parquet_files = parquet_files[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_md_ds = load_dataset(\"parquet\", data_files=parquet_files, num_proc=4)\n",
    "\n",
    "# clear all other cache files, except for the current cache file\n",
    "mimic_md_ds.cleanup_cache_files()\n",
    "\n",
    "size_gb = mimic_md_ds[\"train\"].dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_md_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Benchmarking Filtering operations: ðŸ¤— Dataset vs. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler=\"processes\", num_workers=NUM_PROC)\n",
    "\n",
    "ddf = dd.read_parquet(parquet_files)\n",
    "len(ddf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Filtering on 1 column**\n",
    "\n",
    "Get all rows where the values in column `event_cateogry` is in a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_filter = [\n",
    "    \"Cadiovascular\",\n",
    "    \"Dialysis\",\n",
    "    \"Hemodynamics\",\n",
    "    \"Neurological\",\n",
    "    \"Toxicology\",\n",
    "    \"General\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "events_ddf = ddf[ddf[\"event_category\"].isin(event_filter)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "events_ds = mimic_md_ds[\"train\"].filter(\n",
    "    lambda examples: [\n",
    "        example in event_filter for example in examples[\"event_category\"]\n",
    "    ],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Filtering on multiple columns**\n",
    "\n",
    "Get all items where the values in two columns are in a list of values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_location_filter = [\"HOME\", \"HOME HEALTH CARE\"]\n",
    "admission_location_filter = [\n",
    "    \"TRANSFER FROM HOSPITAL\",\n",
    "    \"PHYSICIAN REFERRAL\",\n",
    "    \"CLINIC REFERRAL\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "location_ddf = ddf[\n",
    "    (ddf[\"discharge_location\"].isin(discharge_location_filter))\n",
    "    & (ddf[\"admission_location\"].isin(admission_location_filter))\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "location_ds = mimic_md_ds[\"train\"].filter(\n",
    "    lambda examples: [\n",
    "        example[0] in discharge_location_filter\n",
    "        and example[1] in admission_location_filter\n",
    "        for example in zip(\n",
    "            examples[\"discharge_location\"], examples[\"admission_location\"]\n",
    "        )\n",
    "    ],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Filtering on a datetime condition**\n",
    "\n",
    "Get all rows where `death of death` occurred after January 1, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "dod_ddf = ddf[ddf[\"dod\"] > datetime.datetime(2020, 1, 1)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "dod_ds = mimic_md_ds[\"train\"].filter(\n",
    "    lambda examples: [\n",
    "        example is not None and example > datetime.datetime(2020, 1, 1)\n",
    "        for example in examples[\"dod\"]\n",
    "    ],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Filter on a condition on a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "millenials_ddf = ddf[(ddf.age <= 40) & (ddf.age >= 25)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "millenials_ds = mimic_md_ds[\"train\"].filter(\n",
    "    lambda examples: [25 <= example <= 40 for example in examples[\"age\"]],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data - Constructing a ðŸ¤— Dataset from image folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ðŸ¤— Datasets documentation, there are 3 ways to load local image data into a ðŸ¤— Dataset:\n",
    "1. **Load images from a folder with the following structure:**\n",
    "    ```bash\n",
    "    root_folder/train/class1/img1.png\n",
    "    root_folder/train/class1/img2.png\n",
    "    root_folder/train/class2/img1.png\n",
    "    root_folder/train/class2/img2.png\n",
    "    root_folder/test/class1/img1.png\n",
    "    root_folder/test/class1/img2.png\n",
    "    root_folder/test/class2/img1.png\n",
    "    root_folder/test/class2/img2.png\n",
    "    ...\n",
    "    ```\n",
    "    The folder names are the class names and the dataset splits (train/test) will automatically be recognized.\n",
    "    The dataset can be loaded using the following code:\n",
    "    ```python\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"imagefolder\", data_dir=\"root_folder\")\n",
    "    ```\n",
    "    (This method also supports loading remote image folders from URLs.)\n",
    "    \n",
    "    The downside of this approach is that it uses PIL to load the images, which does not support many medical image formats like DICOM and NIfTI.\n",
    "\n",
    "2. **Load images using a list of image paths**\n",
    "    ```python\n",
    "    from datasets import Dataset\n",
    "    from datasets.features import Image\n",
    "    dataset = Dataset.from_dict({\"image\": [\"path/to/img1.png\", \"path/to/img2.png\", ...]}).cast_column(\"image\", Image())\n",
    "    ```\n",
    "    This approach is more flexible than the previous one, but it still has the same limitation of not supporting many medical image formats.\n",
    "\n",
    "3. **Create a dataset loading script**\n",
    "\n",
    "    This is the most flexible way to load and share different types of datasets that are not natively supported by ðŸ¤— Datasets library.\n",
    "    In fact, the `imagefolder` dataset is an example of a dataset loading script. In essence, we can extend that script to support more image formats like DICOM and NIfTI. That solves half the problem. The other half is that we need to create a new feature to extend the `Image` class to support decoding medical image formats."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study: MIMIC-CXR-JPG v2.0.0\n",
    "For this case study, we will combine CSV metadata and the `Image` feature to create a ðŸ¤— Dataset from the MIMIC-CXR-JPG v2.0.0 dataset. The dataset is available on [PhysioNet](https://physionet.org/content/mimic-cxr-jpg/2.0.0/).\n",
    "\n",
    "The dataset comes with 4 compressed CSV metadata files. The metadata files are `mimic-cxr-2.0.0-split.csv.gz`, `mimic-cxr-2.0.0-chexpert.csv.gz`, `mimic-cxr-2.0.0-negbio.csv.gz`, and `mimic-cxr-2.0.0-metadata.csv.gz`. The `mimic-cxr-2.0.0-split.csv.gz` file contains the train/val/test split for each image. The `mimic-cxr-2.0.0-chexpert.csv.gz` file contains the CheXpert labels for each image. The `mimic-cxr-2.0.0-negbio.csv.gz` file contains the NegBio labels for each image. The `mimic-cxr-2.0.0-metadata.csv.gz` file contains other metadata for each image. All the metadata files can be joined on the `subject_id` and `study_id` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_jpg_dir = \"/mnt/data/clinical_datasets/mimic-cxr-jpg-2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\n",
    "    os.path.join(mimic_cxr_jpg_dir, \"mimic-cxr-2.0.0-metadata.csv.gz\")\n",
    ")\n",
    "negbio_df = pd.read_csv(\n",
    "    os.path.join(mimic_cxr_jpg_dir, \"mimic-cxr-2.0.0-negbio.csv.gz\")\n",
    ")\n",
    "split_df = pd.read_csv(os.path.join(mimic_cxr_jpg_dir, \"mimic-cxr-2.0.0-split.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the 3 dataframes on subject_id and study_id\n",
    "metadata_df = metadata_df.merge(\n",
    "    split_df, on=[\"subject_id\", \"study_id\", \"dicom_id\"]\n",
    ").merge(negbio_df, on=[\"subject_id\", \"study_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows with images in folder 'p10' i.e. subject_id starts with 10\n",
    "metadata_df = metadata_df[metadata_df[\"subject_id\"].astype(str).str.startswith(\"10\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HuggingFace Dataset from pandas DataFrame\n",
    "mimic_cxr_ds = Dataset.from_pandas(\n",
    "    metadata_df[metadata_df.split == \"train\"], split=\"train\", preserve_index=False\n",
    ")\n",
    "mimic_cxr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the full path to the image:\n",
    "# mimic_cxr_jpg_dir + \"p10\" + \"p\" + subject_id + study_id + dicom_id + \".jpg\"\n",
    "def get_filename(examples):\n",
    "    subject_ids = examples[\"subject_id\"]\n",
    "    study_ids = examples[\"study_id\"]\n",
    "    dicom_ids = examples[\"dicom_id\"]\n",
    "    examples[\"image\"] = [\n",
    "        os.path.join(\n",
    "            mimic_cxr_jpg_dir,\n",
    "            \"files\",\n",
    "            \"p10\",\n",
    "            \"p\" + str(subject_id),\n",
    "            \"s\" + str(study_id),\n",
    "            dicom_id + \".jpg\",\n",
    "        )\n",
    "        for subject_id, study_id, dicom_id in zip(subject_ids, study_ids, dicom_ids)\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "\n",
    "mimic_cxr_ds = mimic_cxr_ds.map(\n",
    "    get_filename,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    remove_columns=[\"dicom_id\", \"split\", \"Rows\", \"Columns\"],\n",
    ")\n",
    "mimic_cxr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_ds = mimic_cxr_ds.cast_column(\"image\", Image())\n",
    "mimic_cxr_ds.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending ðŸ¤— Dataset to Load DICOM (and NIfTI) images\n",
    "1. Create a new feature class that extends the `Image` class to support decoding medical image formats. Let's call it `MedicalImage`. This will use MONAI to decode the medical image formats.\n",
    "2. Create a new dataset loading script that extends the `imagefolder` dataset loading script to support the `MedicalImage` feature class. We can call it `medical_imagefolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.datasets import medicalimagefolder  # noqa: E402\n",
    "from cyclops.datasets.features import MedicalImage  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_files = glob.glob(\n",
    "    \"/mnt/data/clinical_datasets/pseudo_phi_dataset/Pseudo-PHI-DICOM-Data/**/*.dcm\",\n",
    "    recursive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_ds = Dataset.from_dict({\"image\": dcm_files}).cast_column(\"image\", MedicalImage())\n",
    "dicom_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_ds.set_format(\"torch\")\n",
    "type(dicom_ds[0][\"image\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ds = load_dataset(medicalimagefolder, data_files=dcm_files)\n",
    "med_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ds[\"train\"].features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Challenges\n",
    "\n",
    "1. Handling metadata. What to do with it?\n",
    "2. Encoding and decoding image bytes in the formats that are supported by the `MedicalImage` feature class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb98aa873a47e8d8e5d25ce17cc1dc6ed916c9ffff198232d6a1309fece50006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
