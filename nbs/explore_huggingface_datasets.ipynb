{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Extensibility of the ðŸ¤— Datasets Library for Medical Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from functools import partial\n",
    "from typing import Dict, List, Mapping, Tuple, Union\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import psutil\n",
    "import torch\n",
    "import torchxrayvision as xrv\n",
    "import yaml\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from datasets.features import Image\n",
    "from datasets.splits import Split\n",
    "from monai.transforms import AddChanneld, Compose, Lambdad, Resized, ToDeviced\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "from cyclops.datasets.slicing import SlicingConfig\n",
    "from cyclops.evaluate.metrics import MetricCollection, create_metric\n",
    "from cyclops.models.catalog import create_model, list_models\n",
    "from cyclops.models.constants import CONFIG_ROOT\n",
    "from cyclops.utils.file import join\n",
    "from use_cases.params.mimiciv.mortality_decompensation.constants_v1 import (\n",
    "    ENCOUNTERS_FILE,\n",
    "    QUERIED_DIR,\n",
    "    TAB_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/mnt/data/.cache/huggingface/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_PROC = 4\n",
    "TORCH_BATCH_SIZE = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring existing functionalities that are relevant to CyclOps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a ðŸ¤— Dataset from MIMICIV-v2.0 PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cfg = OmegaConf.load(join(\"..\", \"cyclops\", \"query\", \"configs\", \"config.yaml\"))\n",
    "\n",
    "con_str = (\n",
    "    db_cfg.dbms\n",
    "    + \"://\"\n",
    "    + db_cfg.user\n",
    "    + \":\"\n",
    "    + db_cfg.password\n",
    "    + \"@\"\n",
    "    + db_cfg.host\n",
    "    + \"/\"\n",
    "    + db_cfg.database\n",
    ")\n",
    "\n",
    "ds = Dataset.from_sql(\n",
    "    sql=\"SELECT * FROM mimiciv_hosp.patients LIMIT 10\",\n",
    "    con=con_str,\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a ðŸ¤— Dataset from local parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = list(glob.glob(join(QUERIED_DIR, \"*.parquet\")))\n",
    "len(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 100 files\n",
    "parquet_files = parquet_files[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_md_ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=parquet_files,\n",
    "    split=Split.ALL,\n",
    "    num_proc=4,\n",
    "    cache_dir=os.environ[\"HF_DATASETS_CACHE\"],\n",
    ")\n",
    "\n",
    "# clear all other cache files, except for the current cache file\n",
    "mimic_md_ds.cleanup_cache_files()\n",
    "\n",
    "size_gb = mimic_md_ds.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "mimic_md_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_md_ds.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Benchmarking Filtering operations: ðŸ¤— Dataset vs. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler=\"processes\", num_workers=NUM_PROC)\n",
    "\n",
    "ddf = dd.read_parquet(parquet_files)\n",
    "len(ddf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Filtering on 1 column**\n",
    "\n",
    "Get all rows where the values in column `event_cateogry` is in a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_filter = [\n",
    "    \"Cadiovascular\",\n",
    "    \"Dialysis\",\n",
    "    \"Hemodynamics\",\n",
    "    \"Neurological\",\n",
    "    \"Toxicology\",\n",
    "    \"General\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "events_ddf = ddf[ddf[\"event_category\"].isin(event_filter)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "events_ds = mimic_md_ds.filter(\n",
    "    lambda examples: [\n",
    "        example in event_filter for example in examples[\"event_category\"]\n",
    "    ],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Filtering on multiple columns**\n",
    "\n",
    "Get all items where the values in two columns are in a list of values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_location_filter = [\"HOME\", \"HOME HEALTH CARE\"]\n",
    "admission_location_filter = [\n",
    "    \"TRANSFER FROM HOSPITAL\",\n",
    "    \"PHYSICIAN REFERRAL\",\n",
    "    \"CLINIC REFERRAL\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "location_ddf = ddf[\n",
    "    (ddf[\"discharge_location\"].isin(discharge_location_filter))\n",
    "    & (ddf[\"admission_location\"].isin(admission_location_filter))\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "location_ds = mimic_md_ds.filter(\n",
    "    lambda examples: [\n",
    "        example[0] in discharge_location_filter\n",
    "        and example[1] in admission_location_filter\n",
    "        for example in zip(\n",
    "            examples[\"discharge_location\"], examples[\"admission_location\"]\n",
    "        )\n",
    "    ],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Filtering on a datetime condition**\n",
    "\n",
    "Get all rows where `date of death` occurred after January 1, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "dod_ddf = ddf[ddf[\"dod\"] > datetime.datetime(2020, 1, 1)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "dod_ds = mimic_md_ds.filter(\n",
    "    lambda examples: [\n",
    "        example is not None and example > datetime.datetime(2020, 1, 1)\n",
    "        for example in examples[\"dod\"]\n",
    "    ],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Filter on a condition on a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "millenials_ddf = ddf[(ddf.age <= 40) & (ddf.age >= 25)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "millenials_ds = mimic_md_ds.filter(\n",
    "    lambda examples: [25 <= example <= 40 for example in examples[\"age\"]],\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    load_from_cache_file=False,  # timeit will run multiple times\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data - Constructing a ðŸ¤— Dataset from image folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ðŸ¤— Datasets documentation, there are 3 ways to load local image data into a ðŸ¤— Dataset:\n",
    "1. **Load images from a folder with the following structure:**\n",
    "    ```bash\n",
    "    root_folder/train/class1/img1.png\n",
    "    root_folder/train/class1/img2.png\n",
    "    root_folder/train/class2/img1.png\n",
    "    root_folder/train/class2/img2.png\n",
    "    root_folder/test/class1/img1.png\n",
    "    root_folder/test/class1/img2.png\n",
    "    root_folder/test/class2/img1.png\n",
    "    root_folder/test/class2/img2.png\n",
    "    ...\n",
    "    ```\n",
    "    The folder names are the class names and the dataset splits (train/test) will automatically be recognized.\n",
    "    The dataset can be loaded using the following code:\n",
    "    ```python\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"imagefolder\", data_dir=\"root_folder\")\n",
    "    ```\n",
    "    (This method also supports loading remote image folders from URLs.)\n",
    "    \n",
    "    The downside of this approach is that it uses PIL to load the images, which does not support many medical image formats like DICOM and NIfTI.\n",
    "\n",
    "2. **Load images using a list of image paths**\n",
    "    ```python\n",
    "    from datasets import Dataset\n",
    "    from datasets.features import Image\n",
    "    dataset = Dataset.from_dict({\"image\": [\"path/to/img1.png\", \"path/to/img2.png\", ...]}).cast_column(\"image\", Image())\n",
    "    ```\n",
    "    This approach is more flexible than the previous one, but it still has the same limitation of not supporting many medical image formats.\n",
    "\n",
    "3. **Create a dataset loading script**\n",
    "\n",
    "    This is the most flexible way to load and share different types of datasets that are not natively supported by ðŸ¤— Datasets library.\n",
    "    In fact, the `imagefolder` dataset is an example of a dataset loading script. In essence, we can extend that script to support more image formats like DICOM and NIfTI. That solves half the problem. The other half is that we need to create a new feature to extend the `Image` class to support decoding medical image formats."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study: MIMIC-CXR-JPG v2.0.0\n",
    "For this case study, we will combine CSV metadata and the `Image` feature to create a ðŸ¤— Dataset from the MIMIC-CXR-JPG v2.0.0 dataset. The dataset is available on [PhysioNet](https://physionet.org/content/mimic-cxr-jpg/2.0.0/).\n",
    "\n",
    "The dataset comes with 4 compressed CSV metadata files. The metadata files are `mimic-cxr-2.0.0-split.csv.gz`, `mimic-cxr-2.0.0-chexpert.csv.gz`, `mimic-cxr-2.0.0-negbio.csv.gz`, and `mimic-cxr-2.0.0-metadata.csv.gz`. The `mimic-cxr-2.0.0-split.csv.gz` file contains the train/val/test split for each image. The `mimic-cxr-2.0.0-chexpert.csv.gz` file contains the CheXpert labels for each image. The `mimic-cxr-2.0.0-negbio.csv.gz` file contains the NegBio labels for each image. The `mimic-cxr-2.0.0-metadata.csv.gz` file contains other metadata for each image. All the metadata files can be joined on the `subject_id` and `study_id` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_jpg_dir = \"/mnt/data/clinical_datasets/mimic-cxr-jpg-2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\n",
    "    os.path.join(mimic_cxr_jpg_dir, \"mimic-cxr-2.0.0-metadata.csv.gz\")\n",
    ")\n",
    "negbio_df = pd.read_csv(\n",
    "    os.path.join(mimic_cxr_jpg_dir, \"mimic-cxr-2.0.0-negbio.csv.gz\")\n",
    ")\n",
    "split_df = pd.read_csv(os.path.join(mimic_cxr_jpg_dir, \"mimic-cxr-2.0.0-split.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the 3 dataframes on subject_id and study_id\n",
    "metadata_df = metadata_df.merge(\n",
    "    split_df, on=[\"subject_id\", \"study_id\", \"dicom_id\"]\n",
    ").merge(negbio_df, on=[\"subject_id\", \"study_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows with images in folder 'p10' i.e. subject_id starts with 10\n",
    "metadata_df = metadata_df[metadata_df[\"subject_id\"].astype(str).str.startswith(\"10\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HuggingFace Dataset from pandas DataFrame\n",
    "mimic_cxr_ds = Dataset.from_pandas(\n",
    "    metadata_df[metadata_df.split == \"train\"], split=\"train\", preserve_index=False\n",
    ")\n",
    "mimic_cxr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the full path to the image:\n",
    "# mimic_cxr_jpg_dir + \"p10\" + \"p\" + subject_id + study_id + dicom_id + \".jpg\"\n",
    "def get_filename(examples):\n",
    "    subject_ids = examples[\"subject_id\"]\n",
    "    study_ids = examples[\"study_id\"]\n",
    "    dicom_ids = examples[\"dicom_id\"]\n",
    "    examples[\"image\"] = [\n",
    "        os.path.join(\n",
    "            mimic_cxr_jpg_dir,\n",
    "            \"files\",\n",
    "            \"p10\",\n",
    "            \"p\" + str(subject_id),\n",
    "            \"s\" + str(study_id),\n",
    "            dicom_id + \".jpg\",\n",
    "        )\n",
    "        for subject_id, study_id, dicom_id in zip(subject_ids, study_ids, dicom_ids)\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "\n",
    "mimic_cxr_ds = mimic_cxr_ds.map(\n",
    "    get_filename,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    remove_columns=[\"dicom_id\", \"split\", \"Rows\", \"Columns\"],\n",
    ")\n",
    "mimic_cxr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_ds = mimic_cxr_ds.cast_column(\"image\", Image())\n",
    "mimic_cxr_ds.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending ðŸ¤— Dataset to Load DICOM (and NIfTI) images\n",
    "1. Create a new feature class that extends the `Image` class to support decoding medical image formats. Let's call it `MedicalImage`. This will use MONAI to decode the medical image formats.\n",
    "2. Create a new dataset loading script that extends the `imagefolder` dataset loading script to support the `MedicalImage` feature class. We can call it `medical_imagefolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.datasets import medicalimagefolder  # noqa: E402\n",
    "from cyclops.datasets.features import MedicalImage  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_files = glob.glob(\n",
    "    \"/mnt/data/clinical_datasets/pseudo_phi_dataset/Pseudo-PHI-DICOM-Data/**/*.dcm\",\n",
    "    recursive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_ds = Dataset.from_dict({\"image\": dcm_files}).cast_column(\"image\", MedicalImage())\n",
    "dicom_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_ds.set_format(\"torch\")\n",
    "type(dicom_ds[0][\"image\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ds = load_dataset(medicalimagefolder, data_files=dcm_files)\n",
    "med_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ds[\"train\"].features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Challenges\n",
    "\n",
    "1. Handling metadata. What to do with it?\n",
    "2. Encoding and decoding image bytes in the formats that are supported by the `MedicalImage` feature class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Training and Evaluation of Scikit-Learn and PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def is_out_of_core(dataset_size) -> bool:\n",
    "    \"\"\"Check if dataset is too large to fit in memory.\"\"\"\n",
    "    return dataset_size > psutil.virtual_memory().available\n",
    "\n",
    "\n",
    "def get_pandas_df(\n",
    "    dataset: Union[Dataset, DatasetDict, Mapping],\n",
    "    feature_cols: List[str] = None,\n",
    "    label_col: str = None,\n",
    ") -> Union[Tuple[pd.DataFrame, pd.Series], Dict[str, Tuple[pd.DataFrame, pd.Series]]]:\n",
    "    \"\"\"Convert dataset to pandas dataframe.\n",
    "\n",
    "    NOTE: converting to pandas does not work with IterableDataset/IterableDatasetDict\n",
    "    (i.e. when dataset is loaded with stream=True). So, this function should only be\n",
    "    used with datasets that are loaded with stream=False and are small enough to fit\n",
    "    in memory. Use :func:`is_out_of_core` to check if dataset is too large to fit in\n",
    "    memory.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Union[Dataset, DatasetDict, Mapping]\n",
    "        Dataset to convert to pandas dataframe.\n",
    "    feature_cols : List[str], optional\n",
    "        List of feature columns to include in the dataframe, by default None\n",
    "    label_col : str, optional\n",
    "        Label column to include in the dataframe, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[Tuple[pd.DataFrame, pd.Series], Dict[str, Tuple[pd.DataFrame, pd.Series]]]\n",
    "        Pandas dataframe or dictionary of pandas dataframes.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        If dataset is not a Dataset, DatasetDict, or Mapping.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, (DatasetDict, Mapping)):\n",
    "        return {\n",
    "            k: get_pandas_df(v, feature_cols=feature_cols, label_col=label_col)\n",
    "            for k, v in dataset.items()\n",
    "        }\n",
    "    if isinstance(dataset, Dataset) and not is_out_of_core(dataset.dataset_size):\n",
    "        # validate feature_cols and label_col\n",
    "        if feature_cols is not None and not set(feature_cols).issubset(\n",
    "            dataset.column_names\n",
    "        ):\n",
    "            raise ValueError(\"feature_cols must be a subset of dataset column names.\")\n",
    "        if label_col is not None and label_col not in dataset.column_names:\n",
    "            raise ValueError(\"label_col must be a column name of dataset.\")\n",
    "\n",
    "        df = dataset.to_pandas(batched=False)  # set batched=True for large datasets\n",
    "\n",
    "        if feature_cols is not None and label_col is not None:\n",
    "            pd_dataset = (df[feature_cols], df[label_col])\n",
    "        elif label_col is not None:\n",
    "            pd_dataset = (df.drop(label_col, axis=1), df[label_col])\n",
    "        elif feature_cols is not None:\n",
    "            pd_dataset = (df[feature_cols], None)\n",
    "        else:\n",
    "            pd_dataset = (df, None)\n",
    "        return pd_dataset\n",
    "\n",
    "    raise TypeError(\n",
    "        f\"Expected dataset to be a Dataset or DatasetDict. Got: {type(dataset)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_slices(\n",
    "    ds: Dataset,\n",
    "    metrics: MetricCollection,\n",
    "    slice_config: SlicingConfig,\n",
    "    target_cols: Union[str, List[str]],\n",
    "    batch_size: int = 5000,\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate slices of a dataset.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): Dataset to evaluate.\n",
    "        slice_config (SlicingConfig): SlicingConfig object.\n",
    "        metric_collection (MetricCollection): MetricCollection object.\n",
    "        target_cols (str): Name of the label column.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of slice names and metrics.\n",
    "    \"\"\"\n",
    "    if isinstance(target_cols, str):\n",
    "        target_cols = [target_cols]\n",
    "\n",
    "    assert isinstance(ds, Dataset), \"`ds` must be a Hugging Face Dataset object.\"\n",
    "\n",
    "    # if present, drop the `image` column; it is not needed for evaluation\n",
    "    if \"image\" in ds.column_names:\n",
    "        ds = ds.remove_columns(\"image\")\n",
    "\n",
    "    ds.set_format(\"numpy\")\n",
    "\n",
    "    slice_metrics = {}\n",
    "    for slice_name, slice_func in slice_config.get_slices().items():\n",
    "        slice_ds = ds.filter(slice_func, batched=True, batch_size=batch_size)\n",
    "        print(slice_name)\n",
    "        print(\"NUM_ROWS (sliced): \", slice_ds.num_rows)\n",
    "\n",
    "        y_true = np.stack(\n",
    "            [slice_ds[feature] for feature in target_cols], axis=1\n",
    "        ).squeeze()\n",
    "        y_pred = slice_ds[\"predictions\"]\n",
    "        slice_metrics[slice_name] = metrics(y_true, y_pred)\n",
    "        metrics.reset_state()\n",
    "\n",
    "    return slice_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters_ds = load_dataset(\n",
    "    \"parquet\", data_files=ENCOUNTERS_FILE, split=Split.ALL, keep_in_memory=True\n",
    ")\n",
    "encounters_ds.cleanup_cache_files()\n",
    "encounters_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Casting string columns to categorical/numerical columns**\n",
    "\n",
    "```python\n",
    "# cast string columns to some numerical type\n",
    "# this could be categorical or one-hot encoded, depending on the model\n",
    "# XXX: this might be easier to do/generalize after converting to a DataFrame.\n",
    "\n",
    "features_copy = encounters_ds.features.copy()\n",
    "\n",
    "for col in TAB_FEATURES:\n",
    "    if features_copy[col].dtype in [\"string\", \"bool\"]:\n",
    "        features_copy[col] = ClassLabel(names=encounters_ds.unique(col))\n",
    "\n",
    "encounters_ds = encounters_ds.cast(features_copy)\n",
    "encounters_ds.features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, validation, test - 0.8, 0.1, 0.1\n",
    "# NOTE: train_test_split does not work with IterableDataset objects\n",
    "encounters_ds = encounters_ds.train_test_split(test_size=0.2, seed=42)\n",
    "encounters_ds_ = encounters_ds[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "encounters_ds[\"validation\"] = encounters_ds_.pop(\"train\")\n",
    "encounters_ds[\"test\"] = encounters_ds_.pop(\"test\")\n",
    "encounters_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dod_col = np.asanyarray(encounters_ds[\"test\"][\"dod\"], dtype=\"datetime64\")\n",
    "dod_col = dod_col[pd.notnull(dod_col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dod_col.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_out_of_core(dataset_size=encounters_ds[\"train\"].dataset_size):\n",
    "    raise ValueError(\"Dataset is too large to fit in memory.\")\n",
    "\n",
    "encounters_df = get_pandas_df(\n",
    "    encounters_ds[\"train\"], feature_cols=TAB_FEATURES[:-1], label_col=TAB_FEATURES[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing pipeline\n",
    "numeric_features = [0]  # ['age']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [1, 2, 3]  # ['sex', 'admission_type', 'admission_location']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit and transform\n",
    "X_train = preprocessor.fit_transform(encounters_df[0].to_numpy())\n",
    "y_train = encounters_df[1].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Other Ideas\n",
    "\n",
    "**Normalize numerical columns**\n",
    "\n",
    "```python\n",
    "_pa_table = encounters_ds.data  # pyarrow table\n",
    "for feature in TAB_FEATURES[:-1]:\n",
    "    if not isinstance(features_copy[feature], ClassLabel) and features_copy[\n",
    "        feature\n",
    "    ].dtype in [\"int64\", \"float64\"]:\n",
    "        mean = pa.compute.mean(_pa_table[feature])\n",
    "        std = pa.compute.stddev(_pa_table[feature])\n",
    "        feature_norm = pa.compute.divide_checked(\n",
    "            pa.compute.subtract_checked(_pa_table[feature], mean), std\n",
    "        )\n",
    "        _pa_table = _pa_table.append_column(feature, feature_norm)\n",
    "\n",
    "encounters_ds._data = _pa_table\n",
    "```\n",
    "\n",
    "**Split out features and target columns**\n",
    "\n",
    "```python\n",
    "def split_out(examples, features, targets):\n",
    "    \"\"\"Split out features and targets from the dataset\"\"\"\n",
    "\n",
    "    if not isinstance(features, list):\n",
    "        features = [features]\n",
    "    if not isinstance(targets, list):\n",
    "        targets = [targets]\n",
    "\n",
    "    # split out features\n",
    "    # example: 'attributes': [{'feature_1': 1, 'feature_2': 2}, ...]\n",
    "    examples[\"attributes\"] = [\n",
    "        {feature: examples[feature][i] for feature in features}\n",
    "        for i in range(len(examples[features[0]]))\n",
    "    ]\n",
    "\n",
    "    examples[\"targets\"] = [\n",
    "        {target: examples[target][i] for target in targets}\n",
    "        for i in range(len(examples[targets[0]]))\n",
    "    ]\n",
    "\n",
    "    return examples\n",
    "\n",
    "# XXX: Check that features and targets are in the dataset before mapping\n",
    "# NOTE: Applying map on a `DatasetDict` (splits) is much slower than on a `Dataset`\n",
    "\n",
    "encounters_ds_mapped = encounters_ds.map(\n",
    "    partial(split_out, features=TAB_FEATURES[:-1], targets=TAB_FEATURES[-1]),\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROC,\n",
    "    remove_columns=TAB_FEATURES,\n",
    ")\n",
    "encounters_ds_mapped\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models(\"sklearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xgb_classifier\"\n",
    "config_path = join(CONFIG_ROOT, model_name + \".yaml\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model = create_model(model_name, **config[\"model_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other Ideas\n",
    "\n",
    "```python\n",
    "def train_model(examples):\n",
    "    X = np.stack([examples[feature] for feature in TAB_FEATURES[:-1]], axis=1)\n",
    "    y = examples[\"outcome_death\"]\n",
    "    model.partial_fit(X, y, classes=np.unique(y))\n",
    "    return examples\n",
    "\n",
    "if hasattr(model.model, \"partial_fit\"):\n",
    "    encounters_ds[\"train\"].with_format(\"numpy\", columns=TAB_FEATURES).map(\n",
    "        train_model,\n",
    "        batched=True,\n",
    "        batch_size=5000,\n",
    "        num_proc=NUM_PROC,\n",
    "    )\n",
    "else:\n",
    "    ds = encounters_ds[\"train\"].with_format(\"numpy\", columns=TAB_FEATURES)\n",
    "    X_train = np.stack([ds[feature] for feature in TAB_FEATURES[:-1]], axis=1)\n",
    "    y_train = ds[TAB_FEATURES[-1]]\n",
    "    model.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(examples: Dict[str, Union[List, np.ndarray]]) -> dict:\n",
    "    X = np.stack([examples[feature] for feature in TAB_FEATURES[:-1]], axis=1)\n",
    "    X = preprocessor.transform(X)\n",
    "    try:\n",
    "        examples[\"predictions\"] = model.predict_proba(X)\n",
    "    except AttributeError:  # some models don't have `predict_proba`\n",
    "        examples[\"predictions\"] = model.predict(X)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_with_preds = (\n",
    "    encounters_ds[\"test\"]\n",
    "    .with_format(\"numpy\", columns=TAB_FEATURES, output_all_columns=True)\n",
    "    .map(\n",
    "        get_predictions,\n",
    "        batched=True,\n",
    "        batch_size=5000,\n",
    "    )\n",
    ")\n",
    "ds_with_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the slices\n",
    "feature_keys = [\n",
    "    \"dod\",  # non-null/non-missing values in column\n",
    "    [\n",
    "        \"admission_type\",\n",
    "        \"admission_location\",\n",
    "    ],  # non-null/non-missing values in all columns in the list\n",
    "]\n",
    "\n",
    "feature_values = [\n",
    "    {\"sex\": {\"value\": \"M\"}},  # feature value is M\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"min_value\": 18,\n",
    "            \"max_value\": 65,\n",
    "            \"min_inclusive\": True,\n",
    "            \"max_inclusive\": False,\n",
    "        }\n",
    "    },  # feature value is between 18 and 65, inclusive of 18, exclusive of 65\n",
    "    {\n",
    "        \"admission_type\": {\"value\": [\"EW EMER.\", \"DIRECT EMER.\", \"URGENT\"]}\n",
    "    },  # feature value is in the list\n",
    "    {\n",
    "        \"admission_location\": {\n",
    "            \"value\": [\"PHYSICIAN REFERRAL\", \"CLINIC REFERRAL\", \"WALK-IN/SELF REFERRAL\"],\n",
    "            \"negate\": True,\n",
    "        }\n",
    "    },  # feature value is NOT in the list\n",
    "    {\n",
    "        \"dod\": {\"max_value\": \"2019-12-01\", \"keep_nulls\": False}\n",
    "    },  # possibly before COVID-19\n",
    "    {\n",
    "        \"dod\": {\"max_value\": \"2019-12-01\", \"negate\": True, \"keep_nulls\": False}\n",
    "    },  # possibly during COVID-19\n",
    "    {\"admit_timestamp\": {\"month\": [6, 7, 8, 9], \"keep_nulls\": False}},\n",
    "    {\n",
    "        \"sex\": {\"value\": \"F\"},\n",
    "        \"race\": {\n",
    "            \"value\": [\n",
    "                \"BLACK/AFRICAN AMERICAN\",\n",
    "                \"BLACK/CARIBBEAN ISLAND\",\n",
    "                \"BLACK/CAPE VERDEAN\",\n",
    "                \"BLACK/AFRICAN\",\n",
    "            ]\n",
    "        },\n",
    "        \"age\": {\"min_value\": 25, \"max_value\": 40},\n",
    "    },  # compound slice\n",
    "]\n",
    "\n",
    "# create the slice functions\n",
    "slice_config = SlicingConfig()\n",
    "\n",
    "for key in feature_keys:\n",
    "    slice_config.add_feature_keys(key)\n",
    "\n",
    "for feature_value in feature_values:\n",
    "    slice_config.add_feature_values(feature_value)\n",
    "\n",
    "# or\n",
    "# slice_config = SlicingConfig(\n",
    "#     feature_keys=feature_keys, feature_values=feature_values\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auroc\"]\n",
    "metrics = [create_metric(metric_name, task=\"binary\") for metric_name in metric_names]\n",
    "metric_collection = MetricCollection(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_slices(\n",
    "    ds=ds_with_preds,\n",
    "    metrics=metric_collection,\n",
    "    slice_config=slice_config,\n",
    "    target_cols=TAB_FEATURES[-1],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nihcxr_preprocess(df: pd.DataFrame, nihcxr_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess NIHCXR dataframe.\n",
    "\n",
    "    Add a column with the path to the image and create one-hot encoded pathogies\n",
    "    from Finding Labels column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): NIHCXR dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: pre-processed NIHCXR dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add path column\n",
    "    df[\"image\"] = df[\"Image Index\"].apply(\n",
    "        lambda x: os.path.join(nihcxr_dir, \"images\", x)\n",
    "    )\n",
    "\n",
    "    # Create one-hot encoded pathologies\n",
    "    pathologies = df[\"Finding Labels\"].str.get_dummies(sep=\"|\")\n",
    "\n",
    "    # Add one-hot encoded pathologies to dataframe\n",
    "    df = pd.concat([df, pathologies], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "nihcxr_dir = \"/mnt/data/clinical_datasets/NIHCXR\"\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    join(nihcxr_dir, \"test_list.txt\"), header=None, names=[\"Image Index\"]\n",
    ")\n",
    "\n",
    "# select only the images in the test list\n",
    "df = pd.read_csv(join(nihcxr_dir, \"Data_Entry_2017.csv\"))\n",
    "df.dropna(how=\"all\", axis=\"columns\", inplace=True)  # drop empty columns\n",
    "df = df[df[\"Image Index\"].isin(test_df[\"Image Index\"])]\n",
    "\n",
    "df = nihcxr_preprocess(df, nihcxr_dir)\n",
    "\n",
    "# create a Dataset object\n",
    "nih_ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "nih_ds = nih_ds.cast_column(\"image\", Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathologies = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Effusion\",\n",
    "    \"Emphysema\",\n",
    "    \"Fibrosis\",\n",
    "    \"Hernia\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"No Finding\",\n",
    "    \"Nodule\",\n",
    "    \"Pleural_Thickening\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_ds.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose(\n",
    "    [\n",
    "        # TorchVisiond(keys=(\"image\",), name=\"PILToTensor\"), doesn't work\n",
    "        AddChanneld(keys=(\"image\",)),\n",
    "        Resized(keys=(\"image\",), spatial_size=(1, 224, 224)),\n",
    "        Lambdad(keys=(\"image\"), func=lambda x: ((2 * (x / 255.0)) - 1.0) * 1024),\n",
    "        ToDeviced(keys=(\"image\",), device=device),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def apply_transforms(examples: Dict[str, List], transforms: callable) -> dict:\n",
    "    \"\"\"Apply transforms to examples.\"\"\"\n",
    "\n",
    "    # examples is a dict of lists; convert to list of dicts.\n",
    "    # doing a conversion from PIL to tensor is necessary here when working\n",
    "    # with the Image feature type.\n",
    "    value_len = len(list(examples.values())[0])\n",
    "    examples = [\n",
    "        {\n",
    "            k: PILToTensor()(v[i]) if isinstance(v[i], PIL.Image.Image) else v[i]\n",
    "            for k, v in examples.items()\n",
    "        }\n",
    "        for i in range(value_len)\n",
    "    ]\n",
    "\n",
    "    # apply the transforms to each example\n",
    "    examples = [transforms(example) for example in examples]\n",
    "\n",
    "    # convert back to a dict of lists\n",
    "    examples = {k: [d[k] for d in examples] for k in examples[0]}\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data.sampler import BatchSampler, RandomSampler\n",
    "\n",
    "# batch_sampler = BatchSampler(\n",
    "#     RandomSampler(nih_ds), batch_size=TORCH_BATCH_SIZE, drop_last=False\n",
    "# )\n",
    "# nih_dl = DataLoader(nih_ds, batch_sampler=batch_sampler)\n",
    "\n",
    "# for batch in nih_dl:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xrv.models.DenseNet(weights=\"densenet121-res224-nih\")\n",
    "model.classifier = torch.nn.Linear(1024, len(pathologies))\n",
    "model.op_threshs = None\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_torch(examples):\n",
    "    images = torch.stack(examples[\"image\"]).squeeze(1)\n",
    "    examples[\"predictions\"] = model(images)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nih_ds = nih_ds.with_transform(\n",
    "    partial(apply_transforms, transforms=transforms),\n",
    "    columns=[\"image\"],\n",
    "    output_all_columns=True,\n",
    ").map(get_predictions_torch, batched=True, batch_size=TORCH_BATCH_SIZE)\n",
    "nih_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slice-wise Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the slices\n",
    "feature_values = [\n",
    "    {\"Patient Gender\": {\"value\": \"M\"}},\n",
    "    {\"Patient Gender\": {\"value\": \"F\"}},\n",
    "    {\"Patient Age\": {\"min_value\": 25, \"max_value\": 40}},\n",
    "    {\"Patient Age\": {\"min_value\": 65}},\n",
    "    {\"View Position\": {\"value\": \"PA\"}},\n",
    "]\n",
    "\n",
    "# create the slice functions\n",
    "slice_config = SlicingConfig(feature_values=feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auroc\"]\n",
    "metrics = [\n",
    "    create_metric(metric_name, task=\"multilabel\", num_labels=len(pathologies))\n",
    "    for metric_name in metric_names\n",
    "]\n",
    "metric_collection = MetricCollection(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_slices(\n",
    "    ds=nih_ds,\n",
    "    metrics=metric_collection,\n",
    "    slice_config=slice_config,\n",
    "    target_cols=pathologies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycyclops-py3.9",
   "language": "python",
   "name": "pycyclops-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "80bb9ab4ae869aa3df1775d82da11beafa064f9b0592ec408880679b9273a716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
